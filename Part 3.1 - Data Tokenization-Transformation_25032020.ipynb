{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.1 - Data Tokenization-Transformation (latest changes on 09.03.2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tabulate import tabulate\n",
    "import re\n",
    "import os\n",
    "\n",
    "import random\n",
    "\n",
    "# Module to serialize the content produced from the execution of the code\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Module to monitor the progress of a python for loop\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# Module to manipulate text in python - NLTK package\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "# Module to compute word vectorizers and compute the cosine distance\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "import string\n",
    "import itertools\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:70% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from time import time\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "\n",
    "import pydot\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "from IPython.display import SVG\n",
    "from tensorflow.keras.utils import model_to_dot\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import json\n",
    "\n",
    "# Import ML FLow\n",
    "import mlflow.tensorflow\n",
    "import mlflow.pyfunc\n",
    "from tensorflow.keras import regularizers\n",
    "import datetime\n",
    "\n",
    "# Import TensorBoard\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots as tfplots\n",
    "import tensorflow_docs.modeling as tfmodel\n",
    "from tensorflow.keras import regularizers\n",
    "# from tensorboard import default\n",
    "# from tensorboard import program\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "from bert import tokenization\n",
    "from bert.tokenization import FullTokenizer\n",
    "\n",
    "#Visualize Model\n",
    "\n",
    "def visualize_model(model):\n",
    "    return SVG(model_to_dot(model, show_shapes= True, show_layer_names=True, dpi=65).create(prog='dot', format='svg'))\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from packaging import version\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the dataset (this demonstrates how the genres have been cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= pd.read_pickle(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\dataset_part_2_16022020.pkl'))\n",
    "\n",
    "print(\"\\nThe shape of the dataset that will be used in Keras classifier is: {}\".format(dataset.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the correlation between user ratings and IMDB ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k2, p = stats.normaltest(dataset.rating)\n",
    "print(\"p = {:g}\".format(p))\n",
    "\n",
    "alpha = 0.05\n",
    "if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
    "    print(\"The null hypothesis can be rejected\")\n",
    "else:\n",
    "    print(\"The null hypothesis cannot be rejected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "dataset['imdb_rating'] = dataset['imdb_rating'].astype(float)\n",
    "\n",
    "k2, p = stats.normaltest(dataset.imdb_rating)\n",
    "print(\"p = {:g}\".format(p))\n",
    "\n",
    "alpha = 0.05\n",
    "if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
    "    print(\"The null hypothesis can be rejected\")\n",
    "else:\n",
    "    print(\"The null hypothesis cannot be rejected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "rho, pval = spearmanr(dataset.rating,dataset.imdb_rating)\n",
    "rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.scatterplot('rating','imdb_rating',data=dataset)\n",
    "plt.title('user rating vs IMDB rating', fontsize=18)\n",
    "plt.ylabel('IMDB rating', fontsize=16)\n",
    "plt.xlabel('user rating', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understand the dependent variable: Genres of each movie\n",
    "\n",
    "Check their frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['genres'].explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(dataset['genres'].explode().value_counts(normalize=True) * 100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Remove genres less than 1% frequency\n",
    "\n",
    "dataset['reduced_genres'] = dataset['genres'].apply(\n",
    "    lambda row: [val for val in row if val not in ['IMAX', 'Sport', 'Adult', 'News', 'Reality-TV',\n",
    "                                                   'Film-Noir', 'Short', 'Family', 'Biography', 'Music', 'History']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['reduced_genres'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Find indexes with EMPTY LISTST\n",
    "\n",
    "dataset_empty_lists = dataset[dataset.reduced_genres.apply(lambda c: c==[])]\n",
    "\n",
    "remove_indices = dataset_empty_lists.index.to_list()\n",
    "\n",
    "dataset_empty_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Remove the indexes with EMPTY LISTS\n",
    "\n",
    "dataset_frequent_genres =  dataset[~dataset.index.isin(remove_indices)]\n",
    "\n",
    "dataset_frequent_genres.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_frequent_genres = dataset_frequent_genres.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Having cut the most scarse occurences of genres it is still obvious that genres \"Drama\" & \"Comedy\" belong to 40% of the movies.\n",
    "A good approach is either to up-sample the dataset or down-sample it.\n",
    "What we chose was to down-sample the two dominant genres \"Drama\" & \"Comedy\". However, in the sub-part 3.2 \n",
    "we use the imbalanced dataset to train and test the keras text classification models.\n",
    "\"\"\"\n",
    "round(dataset_frequent_genres['reduced_genres'].explode().value_counts(normalize=True) * 100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The dataset below contains 17 out of 27 genres. The 11 genres cut were not frequent enough compared to the rest of the genres.\n",
    "\"\"\"\n",
    "dataset_frequent_genres.to_pickle(\"dataset_part_2_cleaned_of_redundant_genres_16022020.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #1 Import cleaned of redundant genres dataset and genres_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "dataset_frequent_genres = pd.read_pickle(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\dataset_part_2_cleaned_of_redundant_genres_16022020.pkl'))\n",
    "\n",
    "print(\"\\nThe shape of the dataset that will be used in Keras classifier is: {}\".format(dataset_frequent_genres.shape))\n",
    "# Comment: From now on, \"reduced_genres\" column will be used for model classification and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multi-hot encoding is a good practice to transform the value y into a data structure appropriate for multi-label text calssification.\n",
    "\"\"\"\n",
    "# Multy hot encoding since a Movie can have more than 1 genres assigned!\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "dataset_frequent_genres = dataset_frequent_genres.join(pd.DataFrame(mlb.fit_transform(dataset_frequent_genres['reduced_genres']),\n",
    "                                                                    columns=mlb.classes_,\n",
    "                                                                    index=dataset_frequent_genres.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import genres\n",
    "with open(os.path.join(os.getcwd(), \"pickled_data_per_part\\\\genres_list_06032020.pkl\"), 'rb') as handle:\n",
    "    genres_list = pickle.load(handle)\n",
    "genres_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #2 Prune the movie reviews (keep only the first review for each movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_frequent_genres['reviews_length'] = dataset_frequent_genres.reviews.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_frequent_genres['reviews_length'][dataset_frequent_genres['reviews_length']==1])\n",
    "\n",
    "# Since I don't want to loose 3326 movies, I will keep only the first review for each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_frequent_genres.loc[:, 'reviews_pruned'] = dataset_frequent_genres.reviews.apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We observed that a plain text of a reviews as such of a plot summary, contain a lot of stop-words, punctuations and \"noisy\" words\n",
    "that could spoil the results of a text classification model.\n",
    "\"\"\"\n",
    "print(\"Raw text of a movie review:\", dataset_frequent_genres.reviews_pruned.iloc[7])\n",
    "print('\\n')\n",
    "print(\"Raw text of a plot summary: \", dataset_frequent_genres['plot'].iloc[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "# dataset_frequent_genres.loc[:, 'reviews_pruned'] = dataset_frequent_genres.loc[:, 'reviews_pruned'].apply(lambda x: x.translate(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #3 Unify (join) the columns of Actors and Reviews in order to achive a dataframe cell with a unique TEXT (corpus) and not a LIST of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 1: Actors\n",
    "def unify_actors(row):\n",
    "    return ','.join(row['actors']).strip()\n",
    "\n",
    "# Function 2: Reviews\n",
    "def unify_reviews(row):\n",
    "    return ', '.join(row['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_frequent_genres['actors_unified'] = dataset_frequent_genres.apply(unify_actors, axis=1)\n",
    "dataset_frequent_genres['reviews_unified'] = dataset_frequent_genres.apply(unify_reviews, axis=1)\n",
    "\n",
    "print(\"Actors before: {}\".format(dataset_frequent_genres.actors.iloc[0]))\n",
    "print(\"Actors after: {}\\n\".format(dataset_frequent_genres.actors_unified.iloc[0]))\n",
    "\n",
    "print(\"Reviews before: {}\".format(dataset_frequent_genres.reviews.iloc[0]))\n",
    "print(\"Reviews after: {}\".format(dataset_frequent_genres.reviews_unified.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #4 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions used across the whole notebook.\n",
    "Those functions are explisetely used to pre-process the raw data input of texts\n",
    "\"\"\"\n",
    "\n",
    "# Function 1\n",
    "\n",
    "def inference_function(indx, model, x_test_seq, x_test, genres_list):\n",
    "    \n",
    "    test_sequence = x_test_seq[indx:indx+1]\n",
    "    \n",
    "    text_prediction = model.predict(test_sequence)\n",
    "    \n",
    "    [float(i) for i in text_prediction[0]]\n",
    "    \n",
    "    tag_probabilities = text_prediction[0][np.argsort(text_prediction[0])[-3:]]\n",
    "    \n",
    "    indexes = np.argsort(text_prediction[0])[::-1][:3]\n",
    "\n",
    "    predicted_tags = []\n",
    "    \n",
    "    for i, tag in enumerate(genres_list):\n",
    "        if i in indexes:\n",
    "            predicted_tags.append(genres_list[i])\n",
    "    \n",
    "    return print('\\n\\nMovie Title: {}'.format(x_test['Movie Title'].iloc[indx]), '\\n\\nPredicted Genre labels: {}'.format(predicted_tags), '\\n\\nWith predicted probabilities: {}'.format(tag_probabilities), '\\n\\nThe actual Genre labels: {}'.format(x_test['Genres'].iloc[indx]), \"\\n\\n\", \"---------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "# Function 2\n",
    "\n",
    "# version 2.1 (this version was used until 21.02.2020)\n",
    "# def preprocess_text(text):\n",
    "    \n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "#     no_stopword_text = [word for word in text.split(' ') if not word in stop_words]\n",
    "    \n",
    "#     lemmatized_text = [lemmatizer.lemmatize(word, pos='v') for word in no_stopword_text]\n",
    "    \n",
    "#     lowercase_text = [word.lower() for word in lemmatized_text]\n",
    "    \n",
    "#     return ' '.join(lowercase_text)\n",
    "\n",
    "# version 2.2 (this version is an alternative approach of version 2.1, created on 22.02.2020)\n",
    "\n",
    "def preprocess_text(raw_text):\n",
    "    \n",
    "    re_punc=re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    \n",
    "    stripped=[re_punc.sub('', w) for w in raw_text.split(' ')]\n",
    "    \n",
    "    stripped=[token for token in stripped if token.isalpha()]\n",
    "    \n",
    "    #------------------------------------------------\n",
    "    \n",
    "    stop_words=text.ENGLISH_STOP_WORDS.union([\"book\"])\n",
    "    \n",
    "    no_stopword_text=[word for word in stripped if not word.lower() in stop_words]\n",
    "    \n",
    "    no_stopword_text = ' '.join(no_stopword_text) #i joined the text once more because a new lemmatizing approach is implemented below\n",
    "    \n",
    "    #------------------------------------------------\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    #approach 1: lemmatized_text = [lemmatizer.lemmatize(word, pos='v') for word in stripped]\n",
    "    #approach 1 was used until 21.02.2020, although we observed that only some of the tokens were lemmatized while others not.\n",
    "    #Thus, we developed an alternative approach like below to lemmatize as many tokens/words as possible\n",
    "    \n",
    "    #approach 2 developed on 22.02.2020:\n",
    "    lemmatized_text = [lemmatizer.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else lemmatizer.lemmatize(i) for i,j in pos_tag(word_tokenize(no_stopword_text))]\n",
    "    \n",
    "    #------------------------------------------------\n",
    "    \n",
    "    lowercase_text = [word.lower() for word in lemmatized_text]\n",
    "    \n",
    "    return ' '.join(lowercase_text)\n",
    "\n",
    "def transform_actors(column_name, dataset):\n",
    "\n",
    "    dataset.loc[:, 'clean_actors'] = dataset.loc[:, column_name].apply(lambda x: x.lower()) #if column \"actors_unified\" is used\n",
    "\n",
    "def transform_plot(column_name, dataset):\n",
    "    \n",
    "    dataset.loc[:, 'clean_plot_summary'] = dataset.loc[:, column_name].apply(lambda x: preprocess_text(x))\n",
    "\n",
    "def transform_features(column_name, dataset):\n",
    "    \n",
    "    dataset.loc[:, 'clean_combined_features'] = dataset.loc[:, column_name].apply(lambda x: preprocess_text(x))\n",
    "    \n",
    "def transform_reviews(column_name, dataset):\n",
    "    \n",
    "    dataset.loc[:, 'clean_reviews'] = dataset.loc[:, column_name].apply(lambda x: preprocess_text(x))\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "#Function 3.1\n",
    "\n",
    "def split_dataset(method, labels, dataset, split_ratio):\n",
    "    \n",
    "    \"\"\"\n",
    "    Random shuffle split, with an option to split it into a stratified manner.\n",
    "    However, when the stratified method was tested it didn't work out.\n",
    "    \n",
    "    Thus, we created a second function using the StratifiedShuffleSplit of the sklearn module.\n",
    "    \"\"\"\n",
    "    \n",
    "    #As mentioned earler \"reduced genres\" are now used and NOT the column \"genres\"\n",
    "    X = dataset[['title', 'clean_actors', 'clean_plot_summary', 'clean_combined_features', 'clean_reviews', 'reduced_genres']]\n",
    "    \n",
    "    y = labels\n",
    "    \n",
    "    if method==\"stratified\":\n",
    "    \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_ratio, random_state=552, shuffle= True, stratify=y)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_ratio, random_state=552, shuffle= True)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "#Function 3.2\n",
    "\n",
    "def stratify_split_train_test(number_of_splits, split_ratio, dataset, labels):\n",
    "    \n",
    "    \"\"\"\n",
    "    parameters: number_of_splits: Number of re-shuffling & splitting iterations. (based on module documentation)\n",
    "                split_ratio: If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. \n",
    "                             If int, represents the absolute number of test samples.\n",
    "                dataset: The dataset with the transformed (\"cleaned\") inputs, title and genres.\n",
    "                labels: The genre tags for each movie.\n",
    "    \n",
    "    output: The X_train, X_test, y_train, y_test splitted in stratified manner.\n",
    "    \"\"\"\n",
    "    \n",
    "    test_sss = StratifiedShuffleSplit(n_splits=number_of_splits,\n",
    "                                      test_size=split_ratio,\n",
    "                                      random_state=123)\n",
    "    \n",
    "    X = dataset[['title', 'clean_actors', 'clean_plot_summary', 'clean_combined_features', 'clean_reviews', 'reduced_genres']]\n",
    "    \n",
    "    y = labels\n",
    "    \n",
    "    # splitting in train-val and test\n",
    "\n",
    "    X_train_val, X_test, y_train_val, y_test = None, None, None, None\n",
    "\n",
    "    # getting the indexes for each dataset\n",
    "    for train_index, test_index in test_sss.split(X, y):\n",
    "    \n",
    "        print(\"TRAIN-VAL data (indexes selected):\", train_index[:10], \"TEST data (indexes selected):\", test_index[:10])\n",
    "        \n",
    "        X_train_val, X_test = X[train_index], X[test_index]\n",
    "        y_train_val, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    return X_train_val, X_test, y_train_val, y_test\n",
    "\n",
    "#Function 3.3\n",
    "\n",
    "def stratify_split_train_validation(number_of_splits, split_ratio, X_train_val, y_train_val):\n",
    "    \n",
    "    \"\"\"\n",
    "    parameters: number_of_splits: Number of re-shuffling & splitting iterations. (based on module documentation)\n",
    "                split_ratio: If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. \n",
    "                             If int, represents the absolute number of test samples.\n",
    "                dataset: The dataset with the transformed (\"cleaned\") inputs, title and genres.\n",
    "                labels: The genre tags for each movie.\n",
    "    \n",
    "    output: The X_train, X_test, y_train, y_test splitted in stratified manner.\n",
    "    \"\"\"\n",
    "    \n",
    "    val_sss  = StratifiedShuffleSplit(n_splits=number_of_splits,\n",
    "                                      test_size=split_ratio,\n",
    "                                      random_state=123)\n",
    "    \n",
    "    # We reset the indexes for bot the X-train-val and y-train-val in order to break them again into two subsets.\n",
    "    X_train_val = X_train_val.reset_index(drop=True)\n",
    "    y_train_val = y_train_val.reset_index(drop=True)\n",
    "    \n",
    "    # splitting in train-val and test\n",
    "\n",
    "    X_train, X_val, y_train, y_val = None, None, None, None\n",
    "\n",
    "    # getting the indexes for each dataset\n",
    "    for train_index, val_index in val_sss.split(X_train_val, y_train_val):\n",
    "    \n",
    "        print(\"TRAIN data (indexes selected):\", train_index[:10], \"VALIDATION data (indexes selected):\", val_index[:10])\n",
    "\n",
    "        X_train, X_val = X_train_val[train_index], X_train_val[val_index]\n",
    "        y_train, y_val = y_train_val[train_index], y_train_val[val_index]\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "#Function 3.4\n",
    "\n",
    "def iterative_split_dataset(dataset, labels, split_ratio):\n",
    "    \n",
    "    \"\"\"\n",
    "    Iterative shuffle split, with an option to split it into a stratified manner.\n",
    "    The method used is part of the skmultilearn module\n",
    "    \"\"\"\n",
    "    \n",
    "    #As mentioned earler \"reduced genres\" are now used and NOT the column \"genres\"\n",
    "    #X = dataset[['title', 'clean_actors', 'clean_plot_summary', 'clean_combined_features', 'clean_reviews', 'reduced_genres']]\n",
    "    X = dataset.loc[:, ['title', 'clean_actors', 'clean_plot_summary', 'clean_combined_features', 'clean_reviews', 'reduced_genres']].values\n",
    "    y = labels\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = iterative_train_test_split(X, y, test_size=split_ratio)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "# Function 4.1\n",
    "\n",
    "def keras_tokenization(variable, maximum_words, x_train, x_test):\n",
    "    \n",
    "    if variable == \"actors\":  #old maximum_words=20000\n",
    "\n",
    "        actors_tokenizer = Tokenizer(num_words=maximum_words, lower=True, split=', ', oov_token = '<OOV>')\n",
    "\n",
    "        actors_tokenizer.fit_on_texts(list(x_train.loc[:, 'clean_actors']))\n",
    "\n",
    "        actors_tokenizer.word_index = {e:i for e,i in actors_tokenizer.word_index.items() if i <= maximum_words}\n",
    "\n",
    "        actors_tokenizer.word_index[actors_tokenizer.oov_token] = maximum_words + 1\n",
    "\n",
    "        x_train.loc[:, 'actors_seqs'] = actors_tokenizer.texts_to_sequences(x_train.loc[:, 'clean_actors'])\n",
    "    \n",
    "        x_test.loc[:, 'actors_seqs'] = actors_tokenizer.texts_to_sequences(x_test.loc[:, 'clean_actors'])\n",
    "\n",
    "        vocabulary_size_frequent_words = len(actors_tokenizer.word_index) + 1\n",
    "        \n",
    "        tokenizer = actors_tokenizer\n",
    "            \n",
    "    elif variable == \"plot\": #old maximum_words=17500\n",
    "        \n",
    "        plot_tokenizer = Tokenizer(num_words=maximum_words, lower=True, split=' ', oov_token = '<OOV>')\n",
    "        \n",
    "        plot_tokenizer.fit_on_texts(list(x_train.loc[:, 'clean_plot_summary']))\n",
    "\n",
    "        plot_tokenizer.word_index = {e:i for e,i in plot_tokenizer.word_index.items() if i <= maximum_words}\n",
    "        \n",
    "        plot_tokenizer.word_index[plot_tokenizer.oov_token] = maximum_words + 1\n",
    "\n",
    "        x_train.loc[:, 'plot_summary_seqs'] = plot_tokenizer.texts_to_sequences(x_train.loc[:, 'clean_plot_summary'])\n",
    "        \n",
    "        x_test.loc[:, 'plot_summary_seqs'] = plot_tokenizer.texts_to_sequences(x_test.loc[:, 'clean_plot_summary'])\n",
    "\n",
    "        vocabulary_size_frequent_words = len(plot_tokenizer.word_index) + 1\n",
    "        \n",
    "        tokenizer = plot_tokenizer\n",
    "        \n",
    "    elif variable == \"features\": #old maximum_words=20000\n",
    "        \n",
    "        combined_features_tokenizer = Tokenizer(num_words=maximum_words, lower=True, split=' ', oov_token = '<OOV>')\n",
    "        \n",
    "        combined_features_tokenizer.fit_on_texts(list(x_train.loc[:, 'clean_combined_features']))\n",
    "\n",
    "        combined_features_tokenizer.word_index = {e:i for e,i in combined_features_tokenizer.word_index.items() if i <= maximum_words}\n",
    "        \n",
    "        combined_features_tokenizer.word_index[combined_features_tokenizer.oov_token] = maximum_words + 1\n",
    "\n",
    "        x_train.loc[:, 'combined_features_seqs'] = combined_features_tokenizer.texts_to_sequences(x_train.loc[:, 'clean_combined_features'])\n",
    "        \n",
    "        x_test.loc[:, 'combined_features_seqs'] = combined_features_tokenizer.texts_to_sequences(x_test.loc[:, 'clean_combined_features'])\n",
    "\n",
    "        vocabulary_size_frequent_words = len(combined_features_tokenizer.word_index) + 1\n",
    "        \n",
    "        tokenizer = combined_features_tokenizer\n",
    "        \n",
    "    elif variable == \"reviews\": #old maximum_words=20000\n",
    "        \n",
    "        reviews_tokenizer = Tokenizer(num_words=maximum_words, lower=True, split=' ', oov_token = '<OOV>')\n",
    "        \n",
    "        reviews_tokenizer.fit_on_texts(x_train.loc[:, 'clean_reviews'])\n",
    "\n",
    "        reviews_tokenizer.word_index = {e:i for e,i in reviews_tokenizer.word_index.items() if i <= maximum_words}\n",
    "        \n",
    "        reviews_tokenizer.word_index[reviews_tokenizer.oov_token] = maximum_words + 1\n",
    "\n",
    "        x_train.loc[:, 'reviews_seqs'] = reviews_tokenizer.texts_to_sequences(x_train.loc[:, 'clean_reviews'])\n",
    "        \n",
    "        x_test.loc[:, 'reviews_seqs'] = reviews_tokenizer.texts_to_sequences(x_test.loc[:, 'clean_reviews'])\n",
    "\n",
    "        vocabulary_size_frequent_words = len(reviews_tokenizer.word_index) + 1\n",
    "        \n",
    "        tokenizer = reviews_tokenizer\n",
    "        \n",
    "    return vocabulary_size_frequent_words, tokenizer\n",
    "\n",
    "# Function 4.2: For Kfold cross validation\n",
    "def keras_tokenization_cv(variable, maximum_words, x_data):\n",
    "    \n",
    "    if variable == \"actors\":  #old maximum_words=20000\n",
    "\n",
    "        actors_tokenizer = Tokenizer(num_words=maximum_words, lower=True, split=',', oov_token = '<OOV>')\n",
    "\n",
    "        actors_tokenizer.fit_on_texts(list(x_data.loc[:, 'clean_actors']))\n",
    "\n",
    "        actors_tokenizer.word_index = {e:i for e,i in actors_tokenizer.word_index.items() if i <= maximum_words}\n",
    "\n",
    "        actors_tokenizer.word_index[actors_tokenizer.oov_token] = maximum_words + 1\n",
    "\n",
    "        x_data.loc[:, 'actors_seqs'] = actors_tokenizer.texts_to_sequences(x_data.loc[:, 'clean_actors'])\n",
    "\n",
    "        vocabulary_size_frequent_words = len(actors_tokenizer.word_index) + 1\n",
    "        \n",
    "        tokenizer = actors_tokenizer\n",
    "            \n",
    "    elif variable == \"plot\": #old maximum_words=17500\n",
    "        \n",
    "        plot_tokenizer = Tokenizer(num_words=maximum_words, lower=True, split=' ', oov_token = '<OOV>')\n",
    "        \n",
    "        plot_tokenizer.fit_on_texts(list(x_data.loc[:, 'clean_plot_summary']))\n",
    "\n",
    "        plot_tokenizer.word_index = {e:i for e,i in plot_tokenizer.word_index.items() if i <= maximum_words}\n",
    "        \n",
    "        plot_tokenizer.word_index[plot_tokenizer.oov_token] = maximum_words + 1\n",
    "\n",
    "        x_data.loc[:, 'plot_summary_seqs'] = plot_tokenizer.texts_to_sequences(x_data.loc[:, 'clean_plot_summary'])\n",
    "\n",
    "        vocabulary_size_frequent_words = len(plot_tokenizer.word_index) + 1\n",
    "        \n",
    "        tokenizer = plot_tokenizer\n",
    "        \n",
    "    elif variable == \"features\": #old maximum_words=20000\n",
    "        \n",
    "        combined_features_tokenizer = Tokenizer(num_words=maximum_words, lower=True, split=' ', oov_token = '<OOV>')\n",
    "        \n",
    "        combined_features_tokenizer.fit_on_texts(list(x_data.loc[:, 'clean_combined_features']))\n",
    "\n",
    "        combined_features_tokenizer.word_index = {e:i for e,i in combined_features_tokenizer.word_index.items() if i <= maximum_words}\n",
    "        \n",
    "        combined_features_tokenizer.word_index[combined_features_tokenizer.oov_token] = maximum_words + 1\n",
    "\n",
    "        x_data.loc[:, 'combined_features_seqs'] = combined_features_tokenizer.texts_to_sequences(x_data.loc[:, 'clean_combined_features'])\n",
    "        \n",
    "        vocabulary_size_frequent_words = len(combined_features_tokenizer.word_index) + 1\n",
    "        \n",
    "        tokenizer = combined_features_tokenizer\n",
    "        \n",
    "    elif variable == \"reviews\": #old maximum_words=20000\n",
    "        \n",
    "        reviews_tokenizer = Tokenizer(num_words=maximum_words, lower=True, split=' ', oov_token = '<OOV>')\n",
    "        \n",
    "        reviews_tokenizer.fit_on_texts(x_data.loc[:, 'clean_reviews'])\n",
    "\n",
    "        reviews_tokenizer.word_index = {e:i for e,i in reviews_tokenizer.word_index.items() if i <= maximum_words}\n",
    "        \n",
    "        reviews_tokenizer.word_index[reviews_tokenizer.oov_token] = maximum_words + 1\n",
    "        \n",
    "        x_data.loc[:, 'reviews_seqs'] = reviews_tokenizer.texts_to_sequences(x_data.loc[:, 'clean_reviews'])\n",
    "        \n",
    "        vocabulary_size_frequent_words = len(reviews_tokenizer.word_index) + 1\n",
    "        \n",
    "        tokenizer = reviews_tokenizer\n",
    "        \n",
    "    return vocabulary_size_frequent_words, tokenizer\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "# Function 5.1\n",
    "\n",
    "def padding_sequnce_length(variable, x_train):\n",
    "    \n",
    "    if variable == \"actors\":\n",
    "    \n",
    "        all_train_lengths =  list(x_train.actors_seqs.apply(len))\n",
    "\n",
    "        maxlen = int(np.percentile(all_train_lengths, q=90))\n",
    "\n",
    "        print('Max Length of the pad sequence for Actors: {}\\n'.format(maxlen))\n",
    "        \n",
    "    elif variable == \"plot\":\n",
    "        \n",
    "        all_train_lengths = list(x_train.plot_summary_seqs.apply(len))\n",
    "\n",
    "        maxlen = int(np.percentile(all_train_lengths, q=90))\n",
    "\n",
    "        print('Max Length of each padding sequence for Plot summary text: {}\\n'.format(maxlen))\n",
    "        \n",
    "    elif variable == \"features\":\n",
    "        \n",
    "        all_train_lengths =  list(x_train.combined_features_seqs.apply(len))\n",
    "\n",
    "        maxlen = int(np.percentile(all_train_lengths, q=90))\n",
    "\n",
    "        print('Max Length of each padding sequence for Movie features text: {}\\n'.format(maxlen))\n",
    "        \n",
    "    elif variable == \"reviews\":\n",
    "        \n",
    "        all_train_lengths =  list(x_train.reviews_seqs.apply(len))\n",
    "\n",
    "        maxlen = int(np.percentile(all_train_lengths, q=90))\n",
    "\n",
    "        print('Max Length of each padding sequence for Reviews text: {}\\n'.format(maxlen))\n",
    "        \n",
    "    return maxlen\n",
    "\n",
    "# Function 5.2\n",
    "\n",
    "def padding_sequnce_length_cv(variable, x_data):\n",
    "    \n",
    "    if variable == \"actors\":\n",
    "    \n",
    "        all_train_lengths =  list(x_data.actors_seqs.apply(len))\n",
    "\n",
    "        maxlen = int(np.percentile(all_train_lengths, q=90))\n",
    "\n",
    "        print('Max Length of the pad sequence for Actors: {}\\n'.format(maxlen))\n",
    "        \n",
    "    elif variable == \"plot\":\n",
    "        \n",
    "        all_train_lengths = list(x_data.plot_summary_seqs.apply(len))\n",
    "\n",
    "        maxlen = int(np.percentile(all_train_lengths, q=90))\n",
    "\n",
    "        print('Max Length of each padding sequence for Plot summary text: {}\\n'.format(maxlen))\n",
    "        \n",
    "    elif variable == \"features\":\n",
    "        \n",
    "        all_train_lengths =  list(x_data.combined_features_seqs.apply(len))\n",
    "\n",
    "        maxlen = int(np.percentile(all_train_lengths, q=90))\n",
    "\n",
    "        print('Max Length of each padding sequence for Movie features text: {}\\n'.format(maxlen))\n",
    "        \n",
    "    elif variable == \"reviews\":\n",
    "        \n",
    "        all_train_lengths =  list(x_data.reviews_seqs.apply(len))\n",
    "\n",
    "        maxlen = int(np.percentile(all_train_lengths, q=90))\n",
    "\n",
    "        print('Max Length of each padding sequence for Reviews text: {}\\n'.format(maxlen))\n",
    "        \n",
    "    return maxlen\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "# Function 6.1\n",
    "\n",
    "# the input data for a deep learning model must be a single tensor (of shape e.g. (batch_size, 6, vocab_size) in this case), \n",
    "# samples that are shorter than the longest item need to be padded with some placeholder value.\n",
    "\n",
    "#url https://www.tensorflow.org/guide/keras/masking_and_padding\n",
    "def padding_sequence(variable, x_train, x_test, y_train, y_test, maxlen):\n",
    "    \n",
    "    if variable == \"actors\":\n",
    "        \n",
    "        x_train_seq = pad_sequences(x_train.loc[:, 'actors_seqs'], padding='post', maxlen=maxlen)\n",
    "    \n",
    "        x_test_seq = pad_sequences(x_test.loc[:, 'actors_seqs'], padding='post', maxlen=maxlen)\n",
    "        \n",
    "        #--------------------------------------------------------------------\n",
    "        \n",
    "        # Task 1: Discussed with Mr. Louridas\n",
    "        \n",
    "        #padded_shapes = ([100], [None]) # ([None],())\n",
    "        \n",
    "        #x_train_seq = x_train.values.shuffle(1000).padded_batch(32, padded_shapes = padded_shapes) # shuffle is the length of the longest string\n",
    "        #x_test_seq = x_test.values.shuffle(1000).padded_batch(32, padded_shapes = padded_shapes)\n",
    "\n",
    "        #--------------------------------------------------------------------\n",
    "        \n",
    "        assert len(x_train_seq) == len(y_train) # x_train_seq\n",
    "\n",
    "        assert len(x_test_seq) == len(y_test) # x_test_seq\n",
    "        \n",
    "    elif variable == \"plot\":\n",
    "        \n",
    "        x_train_seq = pad_sequences(x_train.loc[:, 'plot_summary_seqs'], padding='post', maxlen=maxlen)\n",
    "    \n",
    "        x_test_seq = pad_sequences(x_test.loc[:, 'plot_summary_seqs'], padding='post', maxlen=maxlen)\n",
    "\n",
    "        assert len(x_train_seq) == len(y_train)\n",
    "\n",
    "        assert len(x_test_seq) == len(y_test)\n",
    "        \n",
    "    elif variable == \"features\":\n",
    "        \n",
    "        x_train_seq = pad_sequences(x_train.loc[:, 'combined_features_seqs'], padding='post', maxlen=maxlen)\n",
    "    \n",
    "        x_test_seq = pad_sequences(x_test.loc[:, 'combined_features_seqs'], padding='post', maxlen=maxlen)\n",
    "\n",
    "        assert len(x_train_seq) == len(y_train)\n",
    "\n",
    "        assert len(x_test_seq) == len(y_test)\n",
    "        \n",
    "    elif variable == \"reviews\":\n",
    "        \n",
    "        x_train_seq = pad_sequences(x_train.loc[:, 'reviews_seqs'], padding='post', maxlen=maxlen)\n",
    "    \n",
    "        x_test_seq = pad_sequences(x_test.loc[:, 'reviews_seqs'], padding='post', maxlen=maxlen)\n",
    "\n",
    "        assert len(x_train_seq) == len(y_train)\n",
    "\n",
    "        assert len(x_test_seq) == len(y_test)\n",
    "        \n",
    "    return x_train_seq, x_test_seq\n",
    "\n",
    "# Function 6.2\n",
    "\n",
    "def padding_sequence_cv(variable, x_data, y_data, maxlen):\n",
    "    \n",
    "    if variable == \"actors\":\n",
    "        \n",
    "        x_data_seq = pad_sequences(x_data.loc[:, 'actors_seqs'], padding='post', maxlen=maxlen)\n",
    "    \n",
    "        #--------------------------------------------------------------------\n",
    "        \n",
    "        # Task 1: Discussed with Mr. Louridas\n",
    "        \n",
    "        #padded_shapes = ([100], [None]) # ([None],())\n",
    "        \n",
    "        #x_train_seq = x_train.values.shuffle(1000).padded_batch(32, padded_shapes = padded_shapes) # shuffle is the length of the longest string\n",
    "        #x_test_seq = x_test.values.shuffle(1000).padded_batch(32, padded_shapes = padded_shapes)\n",
    "\n",
    "        #--------------------------------------------------------------------\n",
    "        \n",
    "        assert len(x_data_seq) == len(y_data)\n",
    "        \n",
    "    elif variable == \"plot\":\n",
    "        \n",
    "        x_data_seq = pad_sequences(x_data.loc[:, 'plot_summary_seqs'], padding='post', maxlen=maxlen)\n",
    "\n",
    "        assert len(x_data_seq) == len(y_data)\n",
    "        \n",
    "    elif variable == \"features\":\n",
    "        \n",
    "        x_data_seq = pad_sequences(x_data.loc[:, 'combined_features_seqs'], padding='post', maxlen=maxlen)\n",
    "\n",
    "        assert len(x_data_seq) == len(y_data)\n",
    "        \n",
    "    elif variable == \"reviews\":\n",
    "        \n",
    "        x_data_seq = pad_sequences(x_data.loc[:, 'reviews_seqs'], padding='post', maxlen=maxlen)\n",
    "\n",
    "        assert len(x_data_seq) == len(y_data)\n",
    "        \n",
    "    return x_data_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Previously we experinced an error using the stratified sampling. Below we printed the number of genre sequences that are assigned to only one movie.\n",
    "For those 131 movies the stratified sampling is failing to complete.\n",
    "Thus, we should find their indexes and remove them. The final dataset should contain 49123-131=48992\n",
    "\"\"\"\n",
    "print(\"Number of movies that are assigned to only 1 sequence of genres: \", len(dataset_frequent_genres.reduced_genres.value_counts()[dataset_frequent_genres.reduced_genres.value_counts()==1]), '\\n')\n",
    "list_of_movies_to_remove=dataset_frequent_genres.reduced_genres.value_counts()[dataset_frequent_genres.reduced_genres.value_counts()==1].index.tolist()\n",
    "print(\"The sequences of genres assigned to only 1 movie: \", list_of_movies_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Below are the indexes of rows that should be removed from the dataset. In total 131 indexes.\n",
    "With those final 48992 rows of the dataset, the stratified sampling will be successfully completed.\n",
    "\"\"\"\n",
    "indexes_to_remove=dataset_frequent_genres['reduced_genres'].map(lambda x: 1 if x in list_of_movies_to_remove else 0)[dataset_frequent_genres['reduced_genres'].map(lambda x: 1 if x in list_of_movies_to_remove else 0)==1].index.tolist()\n",
    "dataset_frequent_genres=dataset_frequent_genres[~dataset_frequent_genres.index.isin(indexes_to_remove)]\n",
    "dataset_frequent_genres=dataset_frequent_genres.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "This code shell may not be executed since it's already pickled after the transformation functions have been applied to each\n",
    "column, which will be later used as model input.\n",
    "\"\"\"\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "# Transfrom the columns:\n",
    "# -> Actors\n",
    "# -> Plot summary\n",
    "# -> Movie Features\n",
    "# -> Reviews\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\\n\")\n",
    "print(\"Transfrom the column of the actors\\n\")\n",
    "transform_actors(\"actors_unified\", dataset_frequent_genres) # function 3: transform_actors\n",
    "\n",
    "print(\"Transfrom the column of the plot summary\\n\")\n",
    "transform_plot(\"plot\", dataset_frequent_genres) # function 3: transform_plot\n",
    "\n",
    "print(\"Transfrom the column of the movie features\\n\")\n",
    "transform_features(\"movie_features\", dataset_frequent_genres) # function 3: transform_features\n",
    "\n",
    "print(\"Transfrom the column of the movie reviews\\n\")\n",
    "transform_reviews(\"reviews_pruned\", dataset_frequent_genres) # function 3: transform_reviews\n",
    "\n",
    "# I could pickle the dataset that contains the transformed columns of actors, plot, features and reviews.\n",
    "# The serialization of the dataset could save me time from transforming each time the data.\n",
    "# total time to transform the columns: 10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code cell was executed once, to serialize the dataset with the transformed \"cleaned\" columns of Actors, Plot, Features, Reviews\n",
    "The cell will be executed to import the latest version of the dataset.\n",
    "\"\"\"\n",
    "#dataset_frequent_genres.to_pickle(os.path.join(os.getcwd(), \"pickled_data_per_part\\\\dataset_frequent_genres_transformed_inputs_25032020.pkl\"))\n",
    "\n",
    "dataset_frequent_genres=pd.read_pickle(os.path.join(os.getcwd(), \"pickled_data_per_part\\\\dataset_frequent_genres_transformed_inputs_25032020.pkl\"))\n",
    "dataset_frequent_genres.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Before pre-processing the raw text of the first review about Toy Story\n",
    "\"\"\"\n",
    "dataset_frequent_genres.actors.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "After pre-processing the raw text of the first review about Toy Story\n",
    "\"\"\"\n",
    "dataset_frequent_genres.clean_actors.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: Random shuffle split (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "This is the first way to split the dataset, by using the random shuffle split of the Train_test_split function offered by sklearn module\n",
    "This version was the first to be developed and followed, however we decided to try a second more robust option.\n",
    "The second option refers to the data separation into train, validation and test set using the StratifiedShuffleSPlit function developed and mainted by sklearn module.\n",
    "\n",
    "In cases of imbalanced datasets and specifically for classification models, the stratification comes in handy because it ensures that the data will be splitted uniformly and both the train and test sets, will enclude all the categorical variables.\n",
    "\"\"\"\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "# Split the dataset into train & set sets\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(\"\\nSplit the dataset into train & test sets (stratified shuffle split)\\n\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_dataset(\"stratified\", dataset_frequent_genres.iloc[:, 13:30], dataset_frequent_genres, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_frequent_genres.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The shape of the X_train, X_test, y_train, y_test splitted and shuffled randomly\n",
    "\"\"\"\n",
    "print(\"X_train shape:{}\".format(X_train.shape))\n",
    "print(\"X_test shape:{}\".format(X_test.shape))\n",
    "print(\"y_train shape:{}\".format(y_train.shape))\n",
    "print(\"y_test shape:{}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The stratification worked!\n",
    "\"\"\"\n",
    "round(X_train.reduced_genres.explode().value_counts(normalize=True)*100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The stratification worked!\n",
    "\"\"\"\n",
    "round(X_test.reduced_genres.explode().value_counts(normalize=True)*100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The below cell serialises the X_train, X_test, y_train, y_test inputs created by the stratified split.\n",
    "\"\"\"\n",
    "# X_train.to_pickle(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\X_train_all_inputs_25032020.pkl'))\n",
    "# X_test.to_pickle(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\X_test_all_inputs_25032020.pkl'))\n",
    "# y_train.to_pickle(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\y_train_all_inputs_25032020.pkl'))\n",
    "# y_test.to_pickle(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\y_test_all_inputs_25032020.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: Stratified shuffle split (train, validation, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# \"\"\"\n",
    "# Stratified split of the dataset, using the function 3.2, 3.3 built in the beggining of the notebook.\n",
    "\n",
    "# Although promising, we experience the same error:\n",
    "\n",
    "# \"ValueError: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\"\n",
    "# \"\"\"\n",
    "# # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "# # Split the dataset into train, validation & test setsOption 1: Random shuffle split\n",
    "# print(\"\\n---------------------------------------------------------------------------------\")\n",
    "# print(\"\\nSplit the dataset into train, validation & test sets\\n\")\n",
    "\n",
    "# X_train_val, X_test, y_train_val, y_test = stratify_split_train_test(5, 0.2, dataset_frequent_genres, dataset_frequent_genres.iloc[:, 13:30].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 3: Stratified shuffle split (train, test) using the \"iterative_train_test_split\" method of skmultilearn module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train, X_test, y_test = iterative_split_dataset(dataset_frequent_genres,\n",
    "#                                                            dataset_frequent_genres.iloc[:, 13:30].values, \n",
    "#                                                            0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# The data split does not seem to be stratified.\n",
    "# Thus, we end up to choose the train_test split approach of random shuffle split. Although, the data is splitted on train, test sets we will later used the validation_split parameters of tensorflow to use a proportion of the train dataset for validation\n",
    "# \"\"\"\n",
    "# from collections import Counter\n",
    "# from skmultilearn.model_selection.measures import get_combination_wise_output_matrix\n",
    "# pd.DataFrame({\n",
    "#     'train': Counter(str(combination) for row in get_combination_wise_output_matrix(y_train, order=2) for combination in row),\n",
    "#     'test' : Counter(str(combination) for row in get_combination_wise_output_matrix(y_test, order=2) for combination in row)\n",
    "# }).T.fillna(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prune the most frequent genres (drama, comedy) - this is for the balancing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the frequency of each genre tag in TRAIN, TEST datasets and prune the high frequent genres to re-balance both train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(dataset_frequent_genres['reduced_genres'].explode().value_counts(normalize=True) * 100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-balance genre Drama\n",
    "\n",
    "dataset_frequent_genres_drama = dataset_frequent_genres[dataset_frequent_genres[\"reduced_genres\"].apply(lambda x: \"Drama\" in x)]\n",
    "dataset_frequent_genres_drama_out = dataset_frequent_genres_drama.sample(frac=.85, random_state=1)\n",
    "remove_indexes = dataset_frequent_genres_drama_out.index\n",
    "dataset_frequent_genres_updated_version1 = dataset_frequent_genres[~dataset_frequent_genres.index.isin(remove_indexes)]\n",
    "print(\"Dataset with drama pruned shape: {}\".format(dataset_frequent_genres_updated_version1.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(X_train_updated_version1['reduced_genres'].explode().value_counts(normalize=True) * 100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-balance genre Comedy\n",
    "\n",
    "dataset_frequent_genres_comedy = dataset_frequent_genres_updated_version1[dataset_frequent_genres_drama_updated_version1[\"reduced_genres\"].apply(lambda x: \"Comedy\" in x)]\n",
    "dataset_frequent_genres_comedy_out = dataset_frequent_genres_comedy.sample(frac=.75, random_state=1)\n",
    "remove_indexes=dataset_frequent_genres_comedy_out.index\n",
    "dataset_frequent_genres_updated_version2 = dataset_frequent_genres_drama_updated_version1[~dataset_frequent_genres_drama_updated_version1.index.isin(remove_indexes)]\n",
    "dataset_frequent_genres_pruned=dataset_frequent_genres_updated_version2\n",
    "dataset_frequent_genres_pruned=dataset_frequent_genres_pruned.reset_index(drop=True)\n",
    "print(\"Dataset with comedy pruned shape: {}\".format(dataset_frequent_genres_pruned.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(dataset_frequent_genres_pruned['reduced_genres'].explode().value_counts(normalize=True) * 100,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_dataset(dataset_frequent_genres_pruned.iloc[:, 13:30], dataset_frequent_genres_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before dropping the frequent genre tags (Drama & Comedy)\n",
    "\n",
    "print(\"X_train shape:{}\".format(X_train.shape))\n",
    "print(\"X_test shape:{}\".format(X_test.shape))\n",
    "print(\"y_train shape:{}\".format(y_train.shape))\n",
    "print(\"y_test shape:{}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of data re-balancing (This is an approach that may be followed or may not!)\n",
    "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Now that the data is splitted, we separated each column of interest to a different X_train and X_test\n",
    "Those train and text X sets will be later used for tokenization and padding\n",
    "\"\"\"\n",
    "# Separate each different input column (actors, plot, features, reviews)\n",
    "\n",
    "X_train_actors = X_train[[\"title\", \"clean_actors\", \"reduced_genres\"]]\n",
    "X_train_plot = X_train[[\"title\", \"clean_plot_summary\", \"reduced_genres\"]]\n",
    "X_train_features = X_train[[\"title\", \"clean_combined_features\", \"reduced_genres\"]]\n",
    "X_train_reviews = X_train[[\"title\", \"clean_reviews\", \"reduced_genres\"]]\n",
    "# In X_train and X_test I also use columns \"title\" and \"genres\" since they will be both used later for making inference with predictions\n",
    "\n",
    "assert X_train_actors.shape==X_train_plot.shape==X_train_features.shape==X_train_reviews.shape\n",
    "\n",
    "X_test_actors = X_test[[\"title\", \"clean_actors\", \"reduced_genres\"]]\n",
    "X_test_plot = X_test[[\"title\", \"clean_plot_summary\", \"reduced_genres\"]]\n",
    "X_test_features = X_test[[\"title\", \"clean_combined_features\", \"reduced_genres\"]]\n",
    "X_test_reviews = X_test[[\"title\", \"clean_reviews\", \"reduced_genres\"]]\n",
    "\n",
    "assert X_test_actors.shape==X_test_plot.shape==X_test_features.shape==X_test_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# \"\"\"\n",
    "# Prepare X and y for KFold cross validation of phase 3.2\n",
    "# \"\"\"\n",
    "# # Separate each different input column (actors, plot, features, reviews)\n",
    "# X=dataset_frequent_genres[['title', 'clean_actors', 'clean_plot_summary', 'clean_combined_features', 'clean_reviews', 'reduced_genres']]\n",
    "# y=dataset_frequent_genres.iloc[:, 13:30].values\n",
    "\n",
    "# X_actors = X[[\"title\", \"clean_actors\", \"reduced_genres\"]]\n",
    "# X_plot = X[[\"title\", \"clean_plot_summary\", \"reduced_genres\"]]\n",
    "# X_features = X[[\"title\", \"clean_combined_features\", \"reduced_genres\"]]\n",
    "# X_reviews = X[[\"title\", \"clean_reviews\", \"reduced_genres\"]]\n",
    "# # In X_train and X_test I also use columns \"title\" and \"genres\" since they will be both used later for making inference with predictions\n",
    "\n",
    "# assert X_actors.shape==X_plot.shape==X_features.shape==X_reviews.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "Token Frequency to determine best value for MAX_FREQUENCY_WORDS used later in word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Find the most frequent words among the actor names\n",
    "In the end the number of rows will be equal to the number of maximum features tokenized by the Tokenizer.\n",
    "MAX_FEATURES=12526 (non-balanced) old\n",
    "MAX_FEATURES=16333 (non-balanced) 06.03.2020 (old)\n",
    "MAX_FEATURES=35961 09.03.2020\n",
    "\n",
    "MAX_FEATURES=11065 (balanced)\n",
    "\n",
    "dataset_frequent_genres if no re-balnaced is implemented!\n",
    "dataset_frequent_genres_pruned if the dataset is re-balanced!\n",
    "\n",
    "Question: Shall I choose the tokens that are more frequent or infrequent?\n",
    "-> Probably the infrequent tokens will make a better classification\n",
    "\"\"\"\n",
    "def actors_split(s):\n",
    "    return s.split(',')\n",
    "\n",
    "corpus_actors=dataset_frequent_genres['clean_actors'].values.tolist() #dataset_frequent_genres, dataset_frequent_genres_pruned\n",
    "c_vectorizer=CountVectorizer(tokenizer=actors_split, min_df=2)\n",
    "\n",
    "X=c_vectorizer.fit_transform(corpus_actors)\n",
    "X_words=c_vectorizer.inverse_transform(X)\n",
    "\n",
    "tokens_list=c_vectorizer.get_feature_names()\n",
    "count_list = np.asarray(X.sum(axis=0)).ravel().tolist()\n",
    "\n",
    "token_frequency_df=pd.DataFrame({'term': c_vectorizer.get_feature_names(), 'token_frequency': count_list})\n",
    "token_frequency_df=token_frequency_df.sort_values(by='token_frequency', ascending=False)\n",
    "print(token_frequency_df.shape)\n",
    "#token_frequency_df_pruned_actors=token_frequency_df[token_frequency_df['token_frequency']>=3]\n",
    "#print(token_frequency_df_pruned_actors.shape)\n",
    "\n",
    "# The below code sample creates a dictionart that was only the n-frequent actors\n",
    "actors_frequency_dictionary=dict(zip(tokens_list, count_list))\n",
    "d = dict((k, v) for k, v in actors_frequency_dictionary.items() if v >= 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of actors that exist in the dataset is: {}\".format(str(262794)))\n",
    "print(\"The total number of actors that are present in more than 2 movies: {}\".format(str(84250)))\n",
    "print(\"Thus, the number of actors that have starred in only 1 movie is: {}\".format(str(262794-84250)))\n",
    "actors_tokenized=20000 #178544"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Find the most frequent words among the movie plots\n",
    "In the end the number of rows will be equal to the number of maximum features tokenized by the Tokenizer.\n",
    "MAX_FEATURES=13294 (non-balanced) old\n",
    "MAX_FEATURES=10083 (non-balanced) 06.03.2020 (old)\n",
    "MAX_FEATURES=37553\n",
    "\n",
    "MAX_FEATURES=10909 (balanced)\n",
    "\"\"\"\n",
    "def plot_split(s):\n",
    "    return s.split(' ')\n",
    "\n",
    "corpus_plot=dataset_frequent_genres['clean_plot_summary'].values.tolist() #dataset_frequent_genres, dataset_frequent_genres_pruned\n",
    "c_vectorizer=CountVectorizer(tokenizer=plot_split, min_df=2)\n",
    "\n",
    "X=c_vectorizer.fit_transform(corpus_plot)\n",
    "X_words=c_vectorizer.inverse_transform(X)\n",
    "\n",
    "tokens_list=c_vectorizer.get_feature_names()\n",
    "count_list = np.asarray(X.sum(axis=0)).ravel().tolist()\n",
    "\n",
    "token_frequency_df=pd.DataFrame({'term': c_vectorizer.get_feature_names(), 'token_frequency': count_list})\n",
    "token_frequency_df=token_frequency_df.sort_values(by='token_frequency', ascending=False)\n",
    "print(token_frequency_df.shape)\n",
    "#token_frequency_df_pruned_plot=token_frequency_df[token_frequency_df['token_frequency']>=3]\n",
    "#print(token_frequency_df_pruned_plot.shape)\n",
    "\n",
    "# The below code sample creates a dictionart that was only the n-frequent actors\n",
    "plot_frequency_dictionary=dict(zip(tokens_list, count_list))\n",
    "d = dict((k, v) for k, v in plot_frequency_dictionary.items() if v >= 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of plot tokens that exist in the dataset is: {}\".format(str(51568)))\n",
    "print(\"The total number of plot tokens that are present in more than 2 movies: {}\".format(str(23325)))\n",
    "print(\"Thus, the number of plot tokens that are present in only 1 movie is: {}\".format(str(51568-23325)))\n",
    "plot_words_tokenized=20000 #28243"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Find the most frequent words among the movie features\n",
    "In the end the number of rows will be equal to the number of maximum features tokenized by the Tokenizer.\n",
    "MAX_FEATURES=17665 (non-balanced) old\n",
    "MAX_FEATURES=14439 (non-balanced) 06.03.2020 (old)\n",
    "MAX_FEATURES=122703\n",
    "\n",
    "MAX_FEATURES=14964 (balanced)\n",
    "\"\"\"\n",
    "def movie_features_split(s):\n",
    "    return s.split(' ')\n",
    "\n",
    "corpus_features=dataset_frequent_genres['clean_combined_features'].values.tolist() #dataset_frequent_genres, dataset_frequent_genres_pruned\n",
    "c_vectorizer=CountVectorizer(tokenizer=movie_features_split, min_df=2)\n",
    "\n",
    "X=c_vectorizer.fit_transform(corpus_features)\n",
    "X_words=c_vectorizer.inverse_transform(X)\n",
    "\n",
    "tokens_list=c_vectorizer.get_feature_names()\n",
    "count_list = np.asarray(X.sum(axis=0)).ravel().tolist()\n",
    "\n",
    "token_frequency_df=pd.DataFrame({'term': c_vectorizer.get_feature_names(), 'token_frequency': count_list})\n",
    "token_frequency_df=token_frequency_df.sort_values(by='token_frequency', ascending=False)\n",
    "print(token_frequency_df.shape)\n",
    "#token_frequency_df_pruned_features=token_frequency_df[token_frequency_df['token_frequency']>=3]\n",
    "#print(token_frequency_df_pruned_features.shape)\n",
    "\n",
    "# The below code sample creates a dictionart that was only the n-frequent actors\n",
    "features_frequency_dictionary=dict(zip(tokens_list, count_list))\n",
    "d = dict((k, v) for k, v in features_frequency_dictionary.items() if v >= 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of feature tokens that exist in the dataset is: {}\".format(str(186231)))\n",
    "print(\"The total number of feature tokens that are present in more than 2 movies: {}\".format(str(88366)))\n",
    "print(\"Thus, the number of feature tokens that are present in only 1 movie is: {}\".format(str(186231-88366)))\n",
    "features_words_tokenized=20000 #97865\n",
    "# I might choose the 2/3 of those 122703 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Find the most frequent words among the movie reviews\n",
    "In the end the number of rows will be equal to the number of maximum features tokenized by the Tokenizer.\n",
    "MAX_FEATURES=16099 (non-balanced) old\n",
    "MAX_FEATURES=15250 (non-balanced) 06.03.2020 (old)\n",
    "MAX_FEATURES=194144\n",
    "\n",
    "MAX_FEATURES=15404 (balanced)\n",
    "\n",
    "\"\"\"\n",
    "def reviews_split(s):\n",
    "    return s.split(' ')\n",
    "\n",
    "corpus_reviews=dataset_frequent_genres['clean_reviews'].values.tolist() #dataset_frequent_genres, dataset_frequent_genres_pruned\n",
    "c_vectorizer=CountVectorizer(tokenizer=reviews_split, min_df=2)\n",
    "\n",
    "X=c_vectorizer.fit_transform(corpus_reviews)\n",
    "X_words=c_vectorizer.inverse_transform(X)\n",
    "\n",
    "tokens_list=c_vectorizer.get_feature_names()\n",
    "count_list = np.asarray(X.sum(axis=0)).ravel().tolist()\n",
    "\n",
    "token_frequency_df=pd.DataFrame({'term': c_vectorizer.get_feature_names(), 'token_frequency': count_list})\n",
    "token_frequency_df=token_frequency_df.sort_values(by='token_frequency', ascending=False)\n",
    "print(token_frequency_df.shape)\n",
    "#token_frequency_df_pruned_reviews=token_frequency_df[token_frequency_df['token_frequency']>=3]\n",
    "#print(token_frequency_df_pruned_reviews.shape)\n",
    "\n",
    "# The below code sample creates a dictionart that was only the n-frequent actors\n",
    "reviews_frequency_dictionary=dict(zip(tokens_list, count_list))\n",
    "d = dict((k, v) for k, v in reviews_frequency_dictionary.items() if v >= 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of review tokens that exist in the dataset is: {}\".format(str(252788)))\n",
    "print(\"The total number of review tokens that are present in more than 2 movies: {}\".format(str(83317)))\n",
    "print(\"Thus, the number of review tokens that have starred that are present in only 1 movie is: {}\".format(str(252788-83317)))\n",
    "reviews_words_tokenized=20000 #169471\n",
    "\n",
    "# I might choose the 2/3 of those 194144 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Data tokenization is one of the most important parts when dealing with text data.\n",
    "Since I am going to deploy keras models, I use the python api of Keras Tokenizer,\n",
    "more details about its use on: https://keras.io/preprocessing/text/\n",
    "\"\"\"\n",
    "#(This block of code should be executed each time the split range changes (80-20-> 50-50))\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "# Tokenize the dataset (using the keras tokenizer)\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(\"\\nTokenize the dataset (using the keras tokenizer)\\n\")\n",
    "\n",
    "vocabulary_size_frequent_words_actors, tokenizer_actors = keras_tokenization(\"actors\", actors_tokenized, X_train_actors, X_test_actors) # function 5: keras_tokenization\n",
    "print(\"\\nActors tokenized with maximum number of words: {}\\n\".format(vocabulary_size_frequent_words_actors))\n",
    "\n",
    "# Pickle the Actors Tokenizer\n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\actors_tokenizer_20000_25032020.pkl'), 'wb') as handle:\n",
    "    pickle.dump(tokenizer_actors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "vocabulary_size_frequent_words_plot, tokenizer_plot = keras_tokenization(\"plot\", plot_words_tokenized, X_train_plot, X_test_plot) # function 5: keras_tokenization\n",
    "print(\"\\nPlot Summary tokenized with maximum number of words: {}\\n\".format(vocabulary_size_frequent_words_plot))\n",
    "\n",
    "# Pickle the Plot Tokenizer\n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\plot_tokenizer_20000_25032020.pkl'), 'wb') as handle:\n",
    "    pickle.dump(tokenizer_plot, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "vocabulary_size_frequent_words_features, tokenizer_features = keras_tokenization(\"features\", features_words_tokenized, X_train_features, X_test_features) # function 5: keras_tokenization\n",
    "print(\"\\nMovie Features tokenized with maximum number of words: {}\\n\".format(vocabulary_size_frequent_words_features))\n",
    "\n",
    "# Pickle the Movie Features Tokenizer\n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\features_tokenizer_20000_25032020.pkl'), 'wb') as handle:\n",
    "    pickle.dump(tokenizer_features, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "vocabulary_size_frequent_words_reviews, tokenizer_reviews = keras_tokenization(\"reviews\", reviews_words_tokenized, X_train_reviews, X_test_reviews) # function 5: keras_tokenization\n",
    "print(\"\\nMovie Reviews tokenized with maximum number of words: {}\\n\".format(vocabulary_size_frequent_words_reviews))\n",
    "\n",
    "# Pickle the Reviews Tokenizer\n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\reviews_tokenizer_20000_25032020.pkl'), 'wb') as handle:\n",
    "    pickle.dump(tokenizer_reviews, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# \"\"\"\n",
    "# Data tokenization is one of the most important parts when dealing with text data.\n",
    "# Since I am going to deploy keras models, I use the python api of Keras Tokenizer\n",
    "# more details about its use on: https://keras.io/preprocessing/text/\n",
    "\n",
    "# Prepared for Kfold cross validation\n",
    "# \"\"\"\n",
    "# #(This block of code should be executed each time the split range changes (80-20-> 50-50))\n",
    "# # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "# # Tokenize the dataset (using the keras tokenizer)\n",
    "# print(\"\\n---------------------------------------------------------------------------------\")\n",
    "# print(\"\\nTokenize the dataset (using the keras tokenizer)\\n\")\n",
    "\n",
    "# vocabulary_size_frequent_words_actors, tokenizer_actors = keras_tokenization(\"actors\", weights_df_pruned_actors.shape[0], X_actors) # function 5: keras_tokenization\n",
    "# print(\"\\nActors tokenized with maximum number of words: {}\\n\".format(vocabulary_size_frequent_words_actors))\n",
    "\n",
    "# # Pickle the Actors Tokenizer\n",
    "# with open('C:\\\\Users\\\\spano\\\\Desktop\\\\GitHub-Thesis\\\\models_text_classification\\\\80-20 split_non-balanced\\\\actors_tokenizer_28022020.pkl', 'wb') as handle:\n",
    "#     pickle.dump(tokenizer_actors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "# vocabulary_size_frequent_words_plot, tokenizer_plot = keras_tokenization(\"plot\", weights_df_pruned_plot.shape[0], X_plot) # function 5: keras_tokenization\n",
    "# print(\"\\nPlot Summary tokenized with maximum number of words: {}\\n\".format(vocabulary_size_frequent_words_plot))\n",
    "\n",
    "# # Pickle the Plot Tokenizer\n",
    "# with open('C:\\\\Users\\\\spano\\\\Desktop\\\\GitHub-Thesis\\\\models_text_classification\\\\80-20 split_non-balanced\\\\plot_tokenizer_28022020.pkl', 'wb') as handle:\n",
    "#     pickle.dump(tokenizer_plot, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "# vocabulary_size_frequent_words_features, tokenizer_features = keras_tokenization(\"features\", weights_df_pruned_features.shape[0], X_features) # function 5: keras_tokenization\n",
    "# print(\"\\nMovie Features tokenized with maximum number of words: {}\\n\".format(vocabulary_size_frequent_words_features))\n",
    "\n",
    "# # Pickle the Movie Features Tokenizer\n",
    "# with open('C:\\\\Users\\\\spano\\\\Desktop\\\\GitHub-Thesis\\\\models_text_classification\\\\80-20 split_non-balanced\\\\features_tokenizer_28022020.pkl', 'wb') as handle:\n",
    "#     pickle.dump(tokenizer_features, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "# vocabulary_size_frequent_words_reviews, tokenizer_reviews = keras_tokenization(\"reviews\", weights_df_pruned_reviews.shape[0], X_reviews) # function 5: keras_tokenization\n",
    "# print(\"\\nMovie Reviews tokenized with maximum number of words: {}\\n\".format(vocabulary_size_frequent_words_reviews))\n",
    "\n",
    "# # Pickle the Reviews Tokenizer\n",
    "# with open('C:\\\\Users\\\\spano\\\\Desktop\\\\GitHub-Thesis\\\\models_text_classification\\\\80-20 split_non-balanced\\\\reviews_tokenizer_28022020.pkl', 'wb') as handle:\n",
    "#     pickle.dump(tokenizer_reviews, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment: The three below blocks of code where executed once and then were pickled!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_reviews.loc[:, 'reviews_seqs'] = X_train_reviews.loc[:, 'reviews_seqs'].apply(lambda x: [[weights_df_pruned_reviews.shape[0]+1] if len(sublist)==0 else sublist for sublist in x])\n",
    "# X_test_reviews.loc[:, 'reviews_seqs'] = X_test_reviews.loc[:, 'reviews_seqs'].apply(lambda x: [[weights_df_pruned_reviews.shape[0]+1] if len(sublist)==0 else sublist for sublist in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_reviews.loc[:, 'reviews_seqs'] = X_train_reviews.loc[:, 'reviews_seqs'].apply(lambda x: list(itertools.chain.from_iterable(x)))\n",
    "# X_test_reviews.loc[:, 'reviews_seqs'] = X_test_reviews.loc[:, 'reviews_seqs'].apply(lambda x: list(itertools.chain.from_iterable(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# BECAREFUL: y_train/y_test: BEFORE re-balancing the dataset, y_train_updated_version2/y_test_updated_version2: AFTER re-balancing the dataset\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "# Specify the length of the maxlen variable\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(\"\\nSpecify the length of the maxlen variable (length is a parameter for the optimal padding execution)\\n\")\n",
    "\n",
    "maxlen_actors = padding_sequnce_length(\"actors\", X_train_actors) # function 6: padding_sequnce_length\n",
    "maxlen_plot = padding_sequnce_length(\"plot\", X_train_plot) # function 6: padding_sequnce_length\n",
    "maxlen_features = padding_sequnce_length(\"features\", X_train_features) # function 6: padding_sequnce_length\n",
    "maxlen_reviews = padding_sequnce_length(\"reviews\", X_train_reviews) # function 6: padding_sequnce_length\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "# Create the padding sequence of texts\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(\"\\nCreate the padding sequence of texts\\n\")\n",
    "\n",
    "X_train_seq_actors, X_test_seq_actors = padding_sequence(\"actors\", X_train_actors, X_test_actors, y_train, y_test, maxlen_actors) # function 7: padding_sequence\n",
    "print(\"\\nActors padded sequences created\\n\")\n",
    "\n",
    "X_train_seq_plot, X_test_seq_plot = padding_sequence(\"plot\", X_train_plot, X_test_plot, y_train, y_test, maxlen_plot) # function 7: padding_sequence\n",
    "print(\"Plot padded sequences created\\n\")\n",
    "\n",
    "X_train_seq_features, X_test_seq_features = padding_sequence(\"features\", X_train_features, X_test_features, y_train, y_test, maxlen_features) # function 7: padding_sequence\n",
    "print(\"Movie Features padded sequences created\\n\")\n",
    "\n",
    "X_train_seq_reviews, X_test_seq_reviews = padding_sequence(\"reviews\", X_train_reviews, X_test_reviews, y_train, y_test, maxlen_reviews) # function 7: padding_sequence\n",
    "print(\"Movie Reviews padded sequences created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# \"\"\"\n",
    "# For cross validation!\n",
    "# \"\"\"\n",
    "# # BECAREFUL: y_train/y_test: BEFORE re-balancing the dataset, y_train_updated_version2/y_test_updated_version2: AFTER re-balancing the dataset\n",
    "# # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "# # Specify the length of the maxlen variable\n",
    "# print(\"\\n---------------------------------------------------------------------------------\")\n",
    "# print(\"\\nSpecify the length of the maxlen variable (length is a parameter for the optimal padding execution)\\n\")\n",
    "\n",
    "# maxlen_actors = padding_sequnce_length(\"actors\", X_actors) # function 6: padding_sequnce_length\n",
    "# maxlen_plot = padding_sequnce_length(\"plot\", X_plot) # function 6: padding_sequnce_length\n",
    "# maxlen_features = padding_sequnce_length(\"features\", X_features) # function 6: padding_sequnce_length\n",
    "# maxlen_reviews = padding_sequnce_length(\"reviews\", X_reviews) # function 6: padding_sequnce_length\n",
    "\n",
    "# # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "# # Create the padding sequence of texts\n",
    "# print(\"\\n---------------------------------------------------------------------------------\")\n",
    "# print(\"\\nCreate the padding sequence of texts\\n\")\n",
    "\n",
    "# X_seq_actors = padding_sequence(\"actors\", X_actors, y, maxlen_actors) # function 7: padding_sequence\n",
    "# print(\"\\nActors padded sequences created\\n\")\n",
    "\n",
    "# X_seq_plot = padding_sequence(\"plot\", X_plot, y, maxlen_plot) # function 7: padding_sequence\n",
    "# print(\"Plot padded sequences created\\n\")\n",
    "\n",
    "# X_seq_features = padding_sequence(\"features\", X_features, y, maxlen_features) # function 7: padding_sequence\n",
    "# print(\"Movie Features padded sequences created\\n\")\n",
    "\n",
    "# X_seq_reviews = padding_sequence(\"reviews\", X_reviews, y, maxlen_reviews) # function 7: padding_sequence\n",
    "# print(\"Movie Reviews padded sequences created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #1st case of data: 80-20 split and non-balanced dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* X_train & X_test with <b>80-20</b> split and <b>non-balanced genre</b> tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train_seq_actors shape:{}\".format(X_train_seq_actors.shape)) #80-20 split\n",
    "print(\"X_train_seq_plot shape:{}\".format(X_train_seq_plot.shape)) #80-20 split\n",
    "print(\"X_train_seq_features shape:{}\".format(X_train_seq_features.shape)) #80-20 split\n",
    "print(\"X_train_seq_reviews shape:{}\\n\".format(X_train_seq_reviews.shape)) #80-20 split\n",
    "\n",
    "print(\"X_test_seq_actors shape:{}\".format(X_test_seq_actors.shape)) #80-20 split\n",
    "print(\"X_test_seq_plot shape:{}\".format(X_test_seq_plot.shape)) #80-20 split\n",
    "print(\"X_test_seq_features shape:{}\".format(X_test_seq_features.shape)) #80-20 split\n",
    "print(\"X_test_seq_reviews shape:{}\".format(X_test_seq_reviews.shape)) #80-20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Data for Kfold cross validation\n",
    "# \"\"\"\n",
    "# print(\"X_train_seq_actors shape:{}\".format(X_seq_actors.shape)) #80-20 split non balanced\n",
    "# print(\"X_train_seq_plot shape:{}\".format(X_seq_plot.shape)) #80-20 split non balanced\n",
    "# print(\"X_train_seq_features shape:{}\".format(X_seq_features.shape)) #80-20 split non balanced\n",
    "# print(\"X_train_seq_reviews shape:{}\\n\".format(X_seq_reviews.shape)) #80-20 split non balanced\n",
    "\n",
    "# print(\"y_train shape:{}\".format(y.shape)) #80-20 split non balanced\n",
    "# print(\"y_test shape:{}\".format(y.shape)) #80-20 split non balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* y_train & y_test with <b>80-20</b> split and <b>non-balanced genre</b> tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"y_train shape:{}\".format(y_train.shape)) #80-20 split\n",
    "print(\"y_test shape:{}\".format(y_test.shape)) #80-20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\x_train_seq_actors_80-20_non-balanced_20000_25032020\"), X_train_seq_actors)\n",
    "np.save(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\x_train_seq_plot_80-20_non-balanced_20000_25032020\"), X_train_seq_plot)\n",
    "np.save(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\x_train_seq_features_80-20_non-balanced_20000_25032020\"), X_train_seq_features)\n",
    "np.save(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\x_train_seq_reviews_80-20_non-balanced_20000_25032020\"), X_train_seq_reviews)\n",
    "\n",
    "np.save(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\x_test_seq_actors_80-20_non-balanced_20000_25032020\"), X_test_seq_actors)\n",
    "np.save(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\x_test_seq_plot_80-20_non-balanced_20000_25032020\"), X_test_seq_plot)\n",
    "np.save(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\x_test_seq_features_80-20_non-balanced_20000_25032020\"), X_test_seq_features)\n",
    "np.save(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\x_test_seq_reviews_80-20_non-balanced_20000_25032020\"), X_test_seq_reviews)\n",
    "\n",
    "np.save(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\y_train_80-20_non-balanced_20000_25032020\"), y_train) #np.save: saves a multi-hot encoded dataframe as array!\n",
    "np.save(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\y_test_80-20_non-balanced_20000_25032020\"), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Data for Kfold cross validation\n",
    "# \"\"\"\n",
    "# np.save(\"x_seq_actors_80-20_non-balanced_28022020\", X_seq_actors)\n",
    "# np.save(\"x_seq_plot_80-20_non-balanced_28022020\", X_seq_plot)\n",
    "# np.save(\"x_seq_features_80-20_non-balanced_28022020\", X_seq_features)\n",
    "# np.save(\"x_seq_reviews_80-20_non-balanced_28022020\", X_seq_reviews)\n",
    "\n",
    "# np.save(\"y_80-20_non-balanced_28022020\", y) #np.save: saves a multi-hot encoded dataframe as array!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #2nd case of data: 50-50 split and non-balanced dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* X_train & X_test with <b>50-50</b> split and <b>non-balanced genre</b> tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"X_train_seq_actors shape:{}\".format(X_train_seq_actors.shape)) #50-50 split\n",
    "print(\"X_train_seq_plot shape:{}\".format(X_train_seq_plot.shape)) #50-50 split\n",
    "print(\"X_train_seq_features shape:{}\".format(X_train_seq_features.shape)) #50-50 split\n",
    "print(\"X_train_seq_reviews shape:{}\\n\".format(X_train_seq_reviews.shape)) #50-50 split\n",
    "\n",
    "print(\"X_test_seq_actors shape:{}\".format(X_test_seq_actors.shape)) #50-50 split\n",
    "print(\"X_test_seq_plot shape:{}\".format(X_test_seq_plot.shape)) #50-50 split\n",
    "print(\"X_test_seq_features shape:{}\".format(X_test_seq_features.shape)) #50-50 split\n",
    "print(\"X_test_seq_reviews shape:{}\".format(X_test_seq_reviews.shape)) #50-50 split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* y_train & y_test with <b>50-50</b> split and <b>non-balanced genre</b> tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"y_train shape:{}\".format(y_train.shape)) #50-50 split\n",
    "print(\"y_test shape:{}\".format(y_test.shape)) #50-50 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"x_train_seq_actors_50-50_non-balanced_07022020\", X_train_seq_actors)\n",
    "np.save(\"x_train_seq_plot_50-50_non-balanced_07022020\", X_train_seq_plot)\n",
    "np.save(\"x_train_seq_features_50-50_non-balanced_07022020\", X_train_seq_features)\n",
    "np.save(\"x_train_seq_reviews_50-50_non-balanced_07022020\", X_train_seq_reviews)\n",
    "\n",
    "np.save(\"x_test_seq_actors_50-50_non-balanced_07022020\", X_test_seq_actors)\n",
    "np.save(\"x_test_seq_plot_50-50_non-balanced_07022020\", X_test_seq_plot)\n",
    "np.save(\"x_test_seq_features_50-50_non-balanced_07022020\", X_test_seq_features)\n",
    "np.save(\"x_test_seq_reviews_50-50_non-balanced_07022020\", X_test_seq_reviews)\n",
    "\n",
    "np.save(\"y_train_50-50_non-balanced_07022020\", y_train)\n",
    "np.save(\"y_test_50-50_non-balanced_07022020\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #3rd case of data: 50-50 split and balanced dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* X_train & X_test with <b>50-50</b> split and <b>balanced genre</b> tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train_seq_actors shape:{}\".format(X_train_seq_actors.shape)) #50-50 split, balanced genres\n",
    "print(\"X_train_seq_plot shape:{}\".format(X_train_seq_plot.shape)) #50-50 split, balanced genres\n",
    "print(\"X_train_seq_features shape:{}\".format(X_train_seq_features.shape)) #50-50 split, balanced genres\n",
    "print(\"X_train_seq_reviews shape:{}\\n\".format(X_train_seq_reviews.shape)) #50-50 split, balanced genres\n",
    "\n",
    "print(\"X_test_seq_actors shape:{}\".format(X_test_seq_actors.shape)) #50-50 split, balanced genres\n",
    "print(\"X_test_seq_plot shape:{}\".format(X_test_seq_plot.shape)) #50-50 split, balanced genres\n",
    "print(\"X_test_seq_features shape:{}\".format(X_test_seq_features.shape)) #50-50 split, balanced genres\n",
    "print(\"X_test_seq_reviews shape:{}\".format(X_test_seq_reviews.shape)) #50-50 split, balanced genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* y_train & y_test with <b>50-50</b> split and <b>balanced genre</b> tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"y_train shape:{}\".format(y_train_updated_version2.shape)) #50-50 split, balanced genres\n",
    "print(\"y_test shape:{}\".format(y_test_updated_version2.shape)) #50-50 split, balanced genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"x_train_seq_actors_50-50_balanced_07022020\", X_train_seq_actors)\n",
    "np.save(\"x_train_seq_plot_50-50_balanced_07022020\", X_train_seq_plot)\n",
    "np.save(\"x_train_seq_features_50-50_balanced_07022020\", X_train_seq_features)\n",
    "np.save(\"x_train_seq_reviews_50-50_balanced_07022020\", X_train_seq_reviews)\n",
    "\n",
    "np.save(\"x_test_seq_actors_50-50_balanced_07022020\", X_test_seq_actors)\n",
    "np.save(\"x_test_seq_plot_50-50_balanced_07022020\", X_test_seq_plot)\n",
    "np.save(\"x_test_seq_features_50-50_balanced_07022020\", X_test_seq_features)\n",
    "np.save(\"x_test_seq_reviews_50-50_balanced_07022020\", X_test_seq_reviews)\n",
    "\n",
    "np.save(\"y_train_50-50_balanced_07022020\", y_train_updated_version2)\n",
    "np.save(\"y_test_50-50_balanced_07022020\", y_test_updated_version2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #4th case of data: 80-20 split and balanced dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* X_train & X_test with <b>80-20</b> split and <b>balanced genre</b> tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train_seq_actors shape:{}\".format(X_train_seq_actors.shape)) #80-20 split, balanced genres\n",
    "print(\"X_train_seq_plot shape:{}\".format(X_train_seq_plot.shape)) #80-20 split, balanced genres\n",
    "print(\"X_train_seq_features shape:{}\".format(X_train_seq_features.shape)) #80-20 split, balanced genres\n",
    "print(\"X_train_seq_reviews shape:{}\\n\".format(X_train_seq_reviews.shape)) #80-20 split, balanced genres\n",
    "\n",
    "print(\"X_test_seq_actors shape:{}\".format(X_test_seq_actors.shape)) #80-20 split, balanced genres\n",
    "print(\"X_test_seq_plot shape:{}\".format(X_test_seq_plot.shape)) #80-20 split, balanced genres\n",
    "print(\"X_test_seq_features shape:{}\".format(X_test_seq_features.shape)) #80-20 split, balanced genres\n",
    "print(\"X_test_seq_reviews shape:{}\".format(X_test_seq_reviews.shape)) #80-20 split, balanced genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* y_train & y_test with <b>80-0</b> split and <b>balanced genre</b> tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"y_train shape:{}\".format(y_train.shape)) #80-20 split, balanced genres\n",
    "print(\"y_test shape:{}\".format(y_test.shape)) #80-20 split, balanced genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"x_train_seq_actors_80-20_balanced_23022020\", X_train_seq_actors)\n",
    "np.save(\"x_train_seq_plot_80-20_balanced_23022020\", X_train_seq_plot)\n",
    "np.save(\"x_train_seq_features_80-20_balanced_23022020\", X_train_seq_features)\n",
    "np.save(\"x_train_seq_reviews_80-20_balanced_23022020\", X_train_seq_reviews)\n",
    "\n",
    "np.save(\"x_test_seq_actors_80-20_balanced_23022020\", X_test_seq_actors)\n",
    "np.save(\"x_test_seq_plot_80-20_balanced_23022020\", X_test_seq_plot)\n",
    "np.save(\"x_test_seq_features_80-20_balanced_23022020\", X_test_seq_features)\n",
    "np.save(\"x_test_seq_reviews_80-20_balanced_23022020\", X_test_seq_reviews)\n",
    "\n",
    "np.save(\"y_train_80-20_balanced_23022020\", y_train)\n",
    "np.save(\"y_test_80-20_balanced_23022020\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle the \"dataset_frequent_genres\" with the added cleaned columns of actors, plot, features and reviews\n",
    "\n",
    "#### Pickle the X_test dataset for use in part 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_frequent_genres.to_pickle(os.path.join(os.getcwd(), \"pickled_data_per_part\\\\dataset_part_3.1_25032020.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_pickle(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\20000_max_features\\\\x_test_20000_25032020.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### THIS IS THE END OF PART 3.1 - Where tokenization, cleaning and balancing of the data took place.\n",
    "#### The next PART 3.2, focuses on training & validating different models neural models based on the data prepaired on PART 3.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
