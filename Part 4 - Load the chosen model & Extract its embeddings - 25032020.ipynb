{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 - Load the chosen model & Extract its embeddings (latest changes on 25.03.2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cleaning and preparing the dataset\n",
    "# -> dataframe manipulation\n",
    "# -> text manipulation\n",
    "# -> Web Scrapping\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tabulate import tabulate\n",
    "import re\n",
    "import os\n",
    "\n",
    "import random\n",
    "\n",
    "# Module to serialize the content produced from the execution of the code\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Module to monitor the progress of a python for loop\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# Module to manipulate text in python - NLTK package\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Module to compute word vectorizers and compute the cosine distance\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "import string\n",
    "import itertools\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:70% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.1.0\n",
      "Version:  2.1.0\n",
      "Eager mode:  True\n",
      "Hub version:  0.7.0\n",
      "GPU is NOT AVAILABLE\n",
      "Wall time: 3.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from time import time\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "\n",
    "import pydot\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "from IPython.display import SVG\n",
    "from tensorflow.keras.utils import model_to_dot\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import json\n",
    "\n",
    "# Import ML FLow\n",
    "import mlflow.tensorflow\n",
    "import mlflow.pyfunc\n",
    "from tensorflow.keras import regularizers\n",
    "import datetime\n",
    "\n",
    "# Import TensorBoard\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots as tfplots\n",
    "import tensorflow_docs.modeling as tfmodel\n",
    "from tensorflow.keras import regularizers\n",
    "# from tensorboard import default\n",
    "# from tensorboard import program\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "from bert import tokenization\n",
    "from bert.tokenization import FullTokenizer\n",
    "\n",
    "#Visualize Model\n",
    "\n",
    "def visualize_model(model):\n",
    "    return SVG(model_to_dot(model, show_shapes= True, show_layer_names=True, dpi=65).create(prog='dot', format='svg'))\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from packaging import version\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the dataset from part 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48992, 38)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_pickle(os.path.join(os.getcwd(), \"pickled_data_per_part\\\\dataset_part_3.1_25032020.pkl\"))\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the already trained chosen model\n",
    "This is the model that perfomed better than those trained on part 3. <br>\n",
    "<i> The chosen model is the \"Multi-input keras model\". <i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.keras.engine.training.Model'>\n",
      "\n",
      "Model is loaded successfully\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the weights of the model saved with EarlyStopping\n",
    "\"\"\"\n",
    "with open(os.path.join(os.getcwd(), 'model_one\\\\multi_input_keras_model_{0}dim_{1}batchsize.json'.format(str(100), str(16))),'r') as f:\n",
    "    model_json = json.load(f)\n",
    "\n",
    "model = model_from_json(model_json)\n",
    "\n",
    "model.load_weights(os.path.join(os.getcwd(), 'model_one\\\\multi_input_keras_model_{0}dim_{1}batchsize.h5'.format(str(100), str(16))))\n",
    "\n",
    "print(type(model))\n",
    "print(\"\\nModel is loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the saved tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizers are loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "IMport the tokenizers of each input, fitted on part 3.1\n",
    "\"\"\"\n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\20000_max_features\\\\actors_tokenizer_20000_25032020.pkl'),'rb') as f:\n",
    "    actors_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\20000_max_features\\\\plot_tokenizer_20000_25032020.pkl'),'rb') as f:\n",
    "    plot_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\20000_max_features\\\\features_tokenizer_20000_25032020.pkl'),'rb') as f:\n",
    "    features_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\20000_max_features\\\\reviews_tokenizer_20000_25032020.pkl'),'rb') as f:\n",
    "    reviews_tokenizer = pickle.load(f)\n",
    "    \n",
    "print(\"Tokenizers are loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(actors_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two Fucntions have been assembled to complete the word embeddings extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_embeddings(variable, model, tokenizer):\n",
    "    \n",
    "    if variable == \"actors\":\n",
    "        \n",
    "        embeddings_layer = model.layers[4].get_weights()[0]\n",
    "\n",
    "        word_embeddings = {w:embeddings_layer[idx] for w, idx in tokenizer.word_index.items() if idx <= len(tokenizer.word_index)+1}\n",
    "        \n",
    "        print(\"\\nActor's word embeddings length: {}\\n\".format(embeddings_layer.shape))\n",
    "        \n",
    "    elif variable == \"plot\":\n",
    "        \n",
    "        embeddings_layer = model.layers[5].get_weights()[0]\n",
    "\n",
    "        word_embeddings = {w:embeddings_layer[idx] for w, idx in tokenizer.word_index.items() if idx <= len(tokenizer.word_index)+1}\n",
    "        print(\"Plot Summary's word embeddings length: {}\\n\".format(embeddings_layer.shape))\n",
    "        \n",
    "    elif variable == \"features\":\n",
    "        \n",
    "        embeddings_layer = model.layers[6].get_weights()[0]\n",
    "\n",
    "        word_embeddings = {w:embeddings_layer[idx] for w, idx in tokenizer.word_index.items() if idx <= len(tokenizer.word_index)+1}\n",
    "        print(\"Movie's Features word embeddings length: {}\\n\".format(embeddings_layer.shape))\n",
    "        \n",
    "    elif variable == \"reviews\":\n",
    "        \n",
    "        embeddings_layer = model.layers[7].get_weights()[0]\n",
    "\n",
    "        word_embeddings = {w:embeddings_layer[idx] for w, idx in tokenizer.word_index.items() if idx <= len(tokenizer.word_index)+1}\n",
    "        print(\"Movie's Reviews word embeddings length: {}\\n\".format(embeddings_layer.shape))\n",
    "        \n",
    "    return embeddings_layer, word_embeddings\n",
    "\n",
    "def assign_word_embeddings(variable, dataset, word_embeddings):\n",
    "    \n",
    "    if variable == \"actors\":\n",
    "    \n",
    "        average_vector_list_cast = []\n",
    "\n",
    "        min_vector_list_cast = []\n",
    "\n",
    "        max_vector_list_cast = []\n",
    "\n",
    "        actors_embeddings_list = []\n",
    "\n",
    "        for i in range(len(dataset.index)):\n",
    "            \n",
    "            actors = dataset.loc[:, \"clean_actors\"].iloc[i].split(\",\")\n",
    "    \n",
    "            assert [word.islower() for word in actors] # assert that all actors are present in lower case\n",
    "    \n",
    "            actors_embeddings_list.append([word_embeddings[word] if word in word_embeddings else word_embeddings['<OOV>'] for word in actors])\n",
    "    \n",
    "        dataset.loc[:, 'actors_embeddings_list'] = actors_embeddings_list\n",
    "\n",
    "        for i in range(len(dataset.index)):\n",
    "            \n",
    "            actor_embeddings = dataset[\"actors_embeddings_list\"].iloc[i]\n",
    "    \n",
    "            minimum = np.min([element for element in actor_embeddings], axis=0)\n",
    "            maximum = np.max([element for element in actor_embeddings], axis=0)\n",
    "            average = np.mean([element for element in actor_embeddings], axis=0)\n",
    "    \n",
    "            min_vector_list_cast.append(minimum)\n",
    "            max_vector_list_cast.append(maximum)\n",
    "            average_vector_list_cast.append(average)\n",
    "\n",
    "        dataset['minimum_cast_vectors'] = min_vector_list_cast\n",
    "        dataset['maximum_cast_vectors'] = max_vector_list_cast\n",
    "        dataset['average_cast_vectors'] = average_vector_list_cast\n",
    "        \n",
    "    elif variable == \"plot\":\n",
    "    \n",
    "        average_vector_list_plot = []\n",
    "\n",
    "        min_vector_list_plot = []\n",
    "\n",
    "        max_vector_list_plot = []\n",
    "\n",
    "        plot_embeddings_list = []\n",
    "\n",
    "        for i in range(len(dataset.index)):\n",
    "            \n",
    "            plot = dataset[\"clean_plot_summary\"].iloc[i]\n",
    "    \n",
    "            plot_split = plot.split(' ')\n",
    "    \n",
    "            assert [word.islower() for word in plot_split]\n",
    "    \n",
    "            plot_embeddings_list.append([word_embeddings[word] if word in word_embeddings else word_embeddings['<OOV>'] for word in plot_split])\n",
    "    \n",
    "        dataset['plot_embeddings_list'] = plot_embeddings_list\n",
    "\n",
    "        for i in range(len(dataset.index)):\n",
    "            \n",
    "            plot_embeddings = dataset[\"plot_embeddings_list\"].iloc[i]\n",
    "    \n",
    "            minimum = np.min([element for element in plot_embeddings], axis=0)\n",
    "            maximum = np.max([element for element in plot_embeddings], axis=0)\n",
    "            average = np.mean([element for element in plot_embeddings], axis=0)\n",
    "    \n",
    "            min_vector_list_plot.append(minimum)\n",
    "            max_vector_list_plot.append(maximum)\n",
    "            average_vector_list_plot.append(average)\n",
    "\n",
    "        dataset['minimum_plot_vectors'] = min_vector_list_plot\n",
    "        dataset['maximum_plot_vectors'] = max_vector_list_plot\n",
    "        dataset['average_plot_vectors'] = average_vector_list_plot\n",
    "        \n",
    "    elif variable == \"features\":\n",
    "    \n",
    "        average_vector_list_combined_features = []\n",
    "\n",
    "        min_vector_list_combined_features = []\n",
    "\n",
    "        max_vector_list_combined_features = []\n",
    "\n",
    "        combined_features_embeddings_list = []\n",
    "\n",
    "        for i in range(len(dataset.index)):\n",
    "    \n",
    "            combined_features = dataset[\"clean_combined_features\"].iloc[i]\n",
    "    \n",
    "            combined_features_split = combined_features.split(' ')\n",
    "    \n",
    "            assert [word.islower() for word in combined_features_split]\n",
    "    \n",
    "            combined_features_embeddings_list.append([word_embeddings[word] if word in word_embeddings else word_embeddings['<OOV>'] for word in combined_features_split])\n",
    "    \n",
    "        dataset['combined_features_embeddings_list'] = combined_features_embeddings_list\n",
    "\n",
    "        for i in range(len(dataset.index)):\n",
    "            \n",
    "            combined_features_embeddings = dataset[\"combined_features_embeddings_list\"].iloc[i]\n",
    "    \n",
    "            minimum = np.min([element for element in combined_features_embeddings], axis=0)\n",
    "            maximum = np.max([element for element in combined_features_embeddings], axis=0)\n",
    "            average = np.mean([element for element in combined_features_embeddings], axis=0)\n",
    "    \n",
    "            min_vector_list_combined_features.append(minimum)\n",
    "            max_vector_list_combined_features.append(maximum)\n",
    "            average_vector_list_combined_features.append(average)\n",
    "\n",
    "        dataset['minimum_combined_features_vectors'] = min_vector_list_combined_features\n",
    "        dataset['maximum_combined_features_vectors'] = max_vector_list_combined_features\n",
    "        dataset['average_combined_features_vectors'] = average_vector_list_combined_features\n",
    "        \n",
    "    elif variable == \"reviews\":\n",
    "    \n",
    "        average_vector_list_reviews = []\n",
    "\n",
    "        min_vector_list_reviews = []\n",
    "\n",
    "        max_vector_list_reviews = []\n",
    "\n",
    "        reviews_embeddings_list = []\n",
    "\n",
    "        for i in range(len(dataset.index)):\n",
    "    \n",
    "            reviews = dataset[\"clean_reviews\"].iloc[i]\n",
    "    \n",
    "            reviews_split = reviews.split(' ')\n",
    "    \n",
    "            assert [word.islower() for word in reviews_split]\n",
    "    \n",
    "            reviews_embeddings_list.append([word_embeddings[word] if word in word_embeddings else word_embeddings['<OOV>'] for word in reviews_split])\n",
    "    \n",
    "        dataset['reviews_embeddings_list'] = reviews_embeddings_list\n",
    "\n",
    "        for i in range(len(dataset.index)):\n",
    "            \n",
    "            reviews_embeddings = dataset[\"reviews_embeddings_list\"].iloc[i]\n",
    "    \n",
    "            minimum = np.min([element for element in reviews_embeddings], axis=0)\n",
    "            maximum = np.max([element for element in reviews_embeddings], axis=0)\n",
    "            average = np.mean([element for element in reviews_embeddings], axis=0)\n",
    "    \n",
    "            min_vector_list_reviews.append(minimum)\n",
    "            max_vector_list_reviews.append(maximum)\n",
    "            average_vector_list_reviews.append(average)\n",
    "\n",
    "        dataset['minimum_reviews_vectors'] = min_vector_list_reviews\n",
    "        dataset['maximum_reviews_vectors'] = max_vector_list_reviews\n",
    "        dataset['average_reviews_vectors'] = average_vector_list_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Extract the word embeddings\n",
      "\n",
      "Actor's word embeddings length: (20002, 100)\n",
      "\n",
      "\n",
      "Word embeddings for actors extracted\n",
      "\n",
      "Plot Summary's word embeddings length: (20002, 100)\n",
      "\n",
      "\n",
      "Word embeddings for plot summary extracted\n",
      "\n",
      "Movie's Features word embeddings length: (20002, 100)\n",
      "\n",
      "\n",
      "Word embeddings for movie features extracted\n",
      "\n",
      "Movie's Reviews word embeddings length: (20002, 100)\n",
      "\n",
      "\n",
      "Word embeddings for movie reviews extracted\n",
      "\n",
      "Wall time: 74.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Extract the word embeddings\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(\"\\nExtract the word embeddings\")\n",
    "\n",
    "actors_embedding_layer, word_embeddings_actors = extract_word_embeddings(\"actors\", model, actors_tokenizer)\n",
    "print(\"\\nWord embeddings for actors extracted\\n\")\n",
    "\n",
    "plot_embedding_layer, word_embeddings_plot = extract_word_embeddings(\"plot\", model, plot_tokenizer)\n",
    "print(\"\\nWord embeddings for plot summary extracted\\n\")\n",
    "\n",
    "features_embedding_layer, word_embeddings_features = extract_word_embeddings(\"features\", model, features_tokenizer)\n",
    "print(\"\\nWord embeddings for movie features extracted\\n\")\n",
    "\n",
    "reviews_embedding_layer, word_embeddings_reviews = extract_word_embeddings(\"reviews\", model, reviews_tokenizer)\n",
    "print(\"\\nWord embeddings for movie reviews extracted\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_embedding_layer = features_embedding_layer / np.linalg.norm(features_embedding_layer, axis = 1).reshape((-1, 1))\n",
    "features_embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_embedding_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dim(weights, components = 3, method = 'tsne'):\n",
    "    \"\"\"Reduce dimensions of embeddings\"\"\"\n",
    "    if method == 'tsne':\n",
    "        return TSNE(components, metric = 'cosine').fit_transform(weights)\n",
    "    elif method == 'umap':\n",
    "        # Might want to try different parameters for UMAP\n",
    "        return UMAP(n_components=components, metric = 'cosine', \n",
    "                    init = 'random', n_neighbors = 5).fit_transform(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_features = reduce_dim(features_embedding_layer, components = 2, method = 'tsne')\n",
    "movie_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres=dataset.columns[13:30].tolist()\n",
    "ints, gen = pd.factorize(genres)\n",
    "gen[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_include = [range(20002)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "\n",
    "# Plot embedding\n",
    "plt.scatter(movie_features[idx_include, 0], movie_features[idx_include, 1], \n",
    "            c = ints, cmap = plt.cm.tab10)\n",
    "\n",
    "# Add colorbar and appropriate labels\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_ticks([])\n",
    "for j, lab in enumerate(gen):\n",
    "    cbar.ax.text(1, (2 * j + 1) / ((10) * 2), lab, ha='left', va='center')\n",
    "cbar.ax.set_title('Genre', loc = 'left')\n",
    "\n",
    "\n",
    "plt.xlabel('TSNE 1'); plt.ylabel('TSNE 2'); plt.title('TSNE Visualization of Book Embeddings');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Assign the word embeddings to each different word\n",
      "Assign word embeddings to actors\n",
      "Assign word embeddings to plot summary\n",
      "Assign word embeddings to movie features\n",
      "Assign word embeddings to movie reviews\n",
      "Wall time: 29.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "# Assign the word embeddings to each different actor\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(\"\\nAssign the word embeddings to each different word\")\n",
    "\n",
    "print(\"Assign word embeddings to actors\")\n",
    "assign_word_embeddings(\"actors\", dataset, word_embeddings_actors)\n",
    "\n",
    "print(\"Assign word embeddings to plot summary\")\n",
    "assign_word_embeddings(\"plot\", dataset, word_embeddings_plot)\n",
    "\n",
    "print(\"Assign word embeddings to movie features\")\n",
    "assign_word_embeddings(\"features\", dataset, word_embeddings_features)\n",
    "\n",
    "print(\"Assign word embeddings to movie reviews\")\n",
    "assign_word_embeddings(\"reviews\", dataset, word_embeddings_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The word embedding vector of the actor 'tobey maguire' is:\n",
      "\n",
      " [-0.2724324  -0.24405827 -0.42759097 -0.488678   -0.37412882 -0.32075557\n",
      " -0.46312344 -0.22833727 -0.4938505  -0.23705494 -0.3767111  -0.31893927\n",
      " -0.29804748 -0.22008938 -0.37356162 -0.19021493 -0.3106991  -0.5046652\n",
      " -0.5067489  -0.13628903 -0.5100186  -0.4387468  -0.4045188  -0.42895505\n",
      " -0.17405698 -0.4196416  -0.32258913 -0.6304357  -0.5015337  -0.38341716\n",
      "  0.20488445 -0.34742785 -0.39623627 -0.3919231  -0.351513   -0.48129812\n",
      " -0.27094525 -0.4885028  -0.30965883 -0.348081   -0.35770744 -0.32492277\n",
      " -0.36964163 -0.44122627 -0.3689114  -0.42301044 -0.70987767 -0.3271487\n",
      " -0.23144028 -0.5712749  -0.5027816  -0.3755143  -0.47165275 -0.3278778\n",
      " -0.39265478 -0.3043721  -0.4021658  -0.47702324 -0.4514447  -0.32985735\n",
      " -0.37301627 -0.4577185  -0.4199124  -0.27355987 -0.43950403 -0.32568955\n",
      " -0.3056007  -0.45140788 -0.45237774 -0.39233974 -0.39421234 -0.38454905\n",
      "  1.4188163  -0.4648103  -0.31668818 -0.37490577 -0.44342712 -0.34322622\n",
      " -0.27576625 -0.21722399 -0.42781395 -0.38284275  1.6027194  -0.26734725\n",
      "  1.0289983  -0.49374586 -0.38163984 -0.29877508 -0.288803   -0.3508288\n",
      " -0.21785362 -0.19488317 -0.29758224 -0.33064032 -0.3155423  -0.6538183\n",
      " -0.38822037 -0.42683017 -0.20027463 -0.37474987]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nThe word embedding vector of the actor 'tobey maguire' is:\\n\\n\", word_embeddings_actors['tobey maguire'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Actors embeddings: (48992, 300)\n",
      "\n",
      "Shape of the Plot Summary embeddings: (48992, 300)\n",
      "\n",
      "Shape of the Combined Features embeddings: (48992, 300)\n",
      "\n",
      "Shape of the Reviews embeddings: (48992, 300)\n",
      "\n",
      "Shape of the concatenated embeddings(cast, plot, combined features): (48992, 1200)\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "keras_embeddings_array_cast = np.hstack([dataset['average_cast_vectors'].apply(pd.Series).values,\n",
    "                                         dataset['minimum_cast_vectors'].apply(pd.Series).values,\n",
    "                                         dataset['maximum_cast_vectors'].apply(pd.Series).values])\n",
    "\n",
    "keras_embeddings_array_plot = np.hstack([dataset['average_plot_vectors'].apply(pd.Series).values,\n",
    "                                         dataset['minimum_plot_vectors'].apply(pd.Series).values,\n",
    "                                         dataset['maximum_plot_vectors'].apply(pd.Series).values])\n",
    "\n",
    "keras_embeddings_array_combined_features = np.hstack([dataset['average_combined_features_vectors'].apply(pd.Series).values,\n",
    "                                                      dataset['minimum_combined_features_vectors'].apply(pd.Series).values,\n",
    "                                                      dataset['maximum_combined_features_vectors'].apply(pd.Series).values])\n",
    "\n",
    "keras_embeddings_array_reviews = np.hstack([dataset['average_reviews_vectors'].apply(pd.Series).values,\n",
    "                                            dataset['minimum_reviews_vectors'].apply(pd.Series).values,\n",
    "                                            dataset['maximum_reviews_vectors'].apply(pd.Series).values])\n",
    "\n",
    "keras_embeddings_array_cast_plot_combined_features_reviews = np.hstack([keras_embeddings_array_cast, \n",
    "                                                                        keras_embeddings_array_plot, \n",
    "                                                                        keras_embeddings_array_combined_features,\n",
    "                                                                        keras_embeddings_array_reviews])\n",
    "\n",
    "print(\"Shape of the Actors embeddings: {}\".format(keras_embeddings_array_cast.shape))\n",
    "print(\"\\nShape of the Plot Summary embeddings: {}\".format(keras_embeddings_array_plot.shape))\n",
    "print(\"\\nShape of the Combined Features embeddings: {}\".format(keras_embeddings_array_combined_features.shape))\n",
    "print(\"\\nShape of the Reviews embeddings: {}\".format(keras_embeddings_array_reviews.shape))\n",
    "print(\"\\nShape of the concatenated embeddings(cast, plot, combined features): {}\".format(keras_embeddings_array_cast_plot_combined_features_reviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Since the chosen model is the \"Model_1: Multi-Input Keras Model\", we saved the relevant word embeddings to the folder \"model_one\"\n",
    "\"\"\"\n",
    "with open(os.path.join(os.getcwd(), 'model_one\\\\keras_embeddings_array_cast_{0}_{1}_25032020.pkl'.format(str(100), str(16))), 'wb') as f:\n",
    "    pickle.dump(keras_embeddings_array_cast, f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), 'model_one\\\\keras_embeddings_array_plot_{0}_{1}_25032020.pkl'.format(str(100), str(16))), 'wb') as f:\n",
    "    pickle.dump(keras_embeddings_array_plot, f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), 'model_one\\\\keras_embeddings_array_combined_features_{0}_{1}_25032020.pkl'.format(str(100), str(16))), 'wb') as f:\n",
    "    pickle.dump(keras_embeddings_array_combined_features, f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), 'model_one\\\\keras_embeddings_array_reviews_{0}_{1}_25032020.pkl'.format(str(100), str(16))), 'wb') as f:\n",
    "    pickle.dump(keras_embeddings_array_combined_features, f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), 'model_one\\\\keras_embeddings_array_concatenated_{0}_{1}_25032020.pkl'.format(str(100), str(16))), 'wb') as f:\n",
    "    pickle.dump(keras_embeddings_array_cast_plot_combined_features_reviews, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle the dataset with the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pickle(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\dataset_part_4_29032020.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END OF Part 4 - Load the chosen model & Extract Word Embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
