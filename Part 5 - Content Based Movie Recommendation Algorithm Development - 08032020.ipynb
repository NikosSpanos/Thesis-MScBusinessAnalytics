{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5 - Content Based Movie Recommendation Algorithm Development (latest changes on 08.03.2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cleaning and preparing the dataset\n",
    "# -> dataframe manipulation\n",
    "# -> text manipulation\n",
    "# -> Web Scrapping\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tabulate import tabulate\n",
    "import re\n",
    "import os\n",
    "\n",
    "import random\n",
    "\n",
    "# Module to serialize the content produced from the execution of the code\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Module to monitor the progress of a python for loop\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# Module to manipulate text in python - NLTK package\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "# Module to compute word vectorizers and compute the cosine distance\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "import string\n",
    "import itertools\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:70% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\dataset_part_4_08032020.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommendation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>Version 11 - Built on 06.11.2019</b><i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions used --------------------------------------------------------------------------------------------------\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "def get_index_from_input_movie(user_input):\n",
    "    return dataset[dataset['title'].str.lower().str.replace('-', '').str.replace('the', '').str.replace(':', '').str.strip() == user_input]['index'].values[0]\n",
    "    \n",
    "def stop_and_stem(uncleaned_list):\n",
    "    ps = PorterStemmer()\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stopped_list = [i for i in uncleaned_list if i not in stop]\n",
    "    stemmed_words = [ps.stem(word) for word in stopped_list]\n",
    "    return stemmed_words\n",
    "\n",
    "def remove_stopwords(uncleaned_list):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stopped_list = [i for i in uncleaned_list if i not in stop]\n",
    "    return stopped_list\n",
    "\n",
    "def remove_punctuation(a_list):\n",
    "    for i, text in enumerate(a_list):\n",
    "        for ch in ['\\\\','`','*','_','{','}','[',']','(',')','>','#','+','-','.','!','$','\\'','?','/',\"'s\"]:\n",
    "            if ch in text:\n",
    "                a_list[i] = a_list[i].replace(ch,'')\n",
    "    return a_list\n",
    "\n",
    "def search_words(row, list_of_words):\n",
    "    ps = PorterStemmer()\n",
    "    row = [ps.stem(x) for x in row]\n",
    "    counter = 0\n",
    "    for word in list_of_words:\n",
    "        if word in row:\n",
    "            counter = counter + 1\n",
    "    return counter\n",
    "\n",
    "def find_correct_genre(user_input, genre_list):\n",
    "    scores_sim=[]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    for item in genre_list:\n",
    "        ed = nltk.edit_distance(user_input, item)\n",
    "        scores_sim.append(ed)\n",
    "    correct_genre_index = scores_sim.index(min(scores_sim))\n",
    "    correct_genre = genre_list[correct_genre_index].lower()\n",
    "    return correct_genre\n",
    "\n",
    "def find_correct_movie(user_input, movie_list):\n",
    "    scores_similarity=[]\n",
    "\n",
    "    for item in movie_list:\n",
    "        ed = nltk.edit_distance(user_input, item)\n",
    "        scores_similarity.append(ed)\n",
    "    correct_movie_index = scores_similarity.index(min(scores_similarity))\n",
    "    correct_movie = movie_list[correct_movie_index].lower()\n",
    "    return correct_movie\n",
    "\n",
    "def intersection(lst1, lst2): \n",
    "    lst3 = [value for value in lst1 if value in lst2] \n",
    "    return lst3\n",
    "\n",
    "def union(lst1, lst2): \n",
    "    final_list = list(set(lst1) | set(lst2)) \n",
    "    return final_list\n",
    "\n",
    "# I will use either the intersection or the union function. I will decide which of the two.\n",
    "\n",
    "def replace_letter_with_number(ll):\n",
    "    for j in range(len((ll))):\n",
    "        if ll[j].startswith(('0', '1', '2', '3', '4','5','6', '7', '8', '9')):\n",
    "            a = re.sub('[^0-9]', '',ll[j]) \n",
    "            ll[j] = a\n",
    "        else:\n",
    "            pass\n",
    "    return ll\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Import the dataset\n",
    "\n",
    "# dataset = pd.read_pickle('C:\\\\Users\\\\dq186sy\\\\Desktop\\\\Big Data Content Analytics\\\\Movie Recommendation System\\\\dataset_embedded_02092019.pkl')\n",
    "\n",
    "dataset = pd.read_pickle('C:\\\\Users\\\\spano\\\\Desktop\\\\GitHub-Thesis\\\\models_text_classification\\\\pickled_data_per_part\\\\dataset_part_4_08032020.pkl')\n",
    "\n",
    "dataset = dataset.reset_index()\n",
    "\n",
    "dataset['index'] = np.arange(0, len(dataset))\n",
    "\n",
    "# It is important to reset the index of the dataset in order to get the correct index per movie!\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Create the movie_genre list with the unique types of genre \n",
    "\n",
    "with open('C:\\\\Users\\\\spano\\\\Desktop\\\\GitHub-Thesis\\\\models_text_classification\\\\pickled_data_per_part\\\\genres_list_16022020.pkl', 'rb') as f:\n",
    "    movie_genre_list = pickle.load(f)\n",
    "\n",
    "movie_genre_list = [x.lower() for x in movie_genre_list]\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Phase 1: Get the user's input and transform it to the appropriate form\n",
    "\n",
    "\n",
    "input_one = input(\"Give me a movie genre (i.e romance, action, adventure): \")\n",
    "\n",
    "input_one = find_correct_genre(input_one.lower(), movie_genre_list)\n",
    "\n",
    "print(\"The movie genre selected by the user: {}\".format(input_one))\n",
    "\n",
    "\n",
    "input_movie = input(\"Give me the title of a movie: \").lower().replace('-', '').replace('the', '').replace(':', '').strip()\n",
    "\n",
    "print(\"The movie title selected by the user: {}\".format(input_movie))\n",
    "\n",
    "\n",
    "input_two = input(\"Now think of some reasons why you like '{}':\".format(input_movie)).lower().replace(',', '').replace('.', '').split(' ')\n",
    "\n",
    "inputs_list = remove_stopwords(input_two)\n",
    "\n",
    "print(\"My inputs list before removing the stop words: {}\".format(input_two))\n",
    "print(\"\\nMy inputs list after removing the stop words: {}\".format(inputs_list))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Using the genre input given by the user, isolate those movies that match the given genre (i.e Action movies)\n",
    "\n",
    "lower_case_genres = []\n",
    "\n",
    "for i in range(len(dataset.loc[:, 'reduced_genres'])):\n",
    "    lower_case_genres.append([element.lower() for element in dataset.loc[:, 'reduced_genres'].iloc[i]])\n",
    "    \n",
    "dataset.loc[:,'lower_case_genres'] = lower_case_genres\n",
    "\n",
    "selected_rows = dataset.loc[:, 'lower_case_genres'].apply(lambda x: any(item for item in x if item == input_one))\n",
    "\n",
    "locked_frame = dataset[selected_rows]\n",
    "\n",
    "indexes_list = locked_frame.loc[:, 'index'].tolist()\n",
    "\n",
    "locked_frame.loc[:, 'index'] = np.arange(0, len(locked_frame))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Phase 2: Slice the dataset based on the user's input\n",
    "\n",
    "\n",
    "# Check of the movie user gave is in the movie list of the dataset\n",
    "\n",
    "selected_genre_movies_list = locked_frame['title'].str.lower().str.replace('-', '').str.replace('the', '').str.replace(':', '').str.strip().tolist()\n",
    "\n",
    "if input_movie in selected_genre_movies_list:\n",
    "    \n",
    "    input_movie = find_correct_movie(input_movie, selected_genre_movies_list) #probably there is no reason in ckecking this!\n",
    "    \n",
    "    assert input_movie in selected_genre_movies_list\n",
    "    \n",
    "    # Isolate the movie plot of the movie provided from the user [If the movie is part of the dataset].\n",
    "\n",
    "    movie_plot_new = locked_frame.loc[:, 'plot'].loc[(locked_frame['title'].str.lower().str.replace('-', '').str.replace('the', '').str.replace(':', '').str.strip() == input_movie)].apply(lambda x: list(set(re.split(' |,|\\n', x.strip().lower())))).values[0]\n",
    "\n",
    "    print(\"\\nThe movie plot before cleaning: {}\".format(movie_plot_new))\n",
    "    \n",
    "    cleaned_movie_plot = remove_stopwords(movie_plot_new)\n",
    "\n",
    "    plot_user_input_list = inputs_list + cleaned_movie_plot\n",
    "    \n",
    "    plot_user_input_list = remove_punctuation(plot_user_input_list)\n",
    "\n",
    "    plot_user_input_list = [x for x in plot_user_input_list if x]\n",
    "\n",
    "    plot_user_input_list = list(dict.fromkeys(plot_user_input_list))\n",
    "    \n",
    "    plot_user_input_list = replace_letter_with_number(plot_user_input_list)\n",
    "    \n",
    "    print(\"\\nThe words of the movie plot after cleaning and adding the user's input: {}\".format(plot_user_input_list))\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # Get the index of the movie provied by the user\n",
    "\n",
    "    movie_index = get_index_from_input_movie(input_movie)\n",
    "    \n",
    "    print(\"\\nThe index of the movie on the initial dataset is: {}\".format(movie_index))\n",
    "    \n",
    "    # Based on the index from the initial dataset locate the same in the Locked_frame.\n",
    "    # It is important to locate the same movie!\n",
    "    \n",
    "    locked_frame_index = locked_frame.loc[locked_frame['title'].str.lower().str.replace('-', '').str.replace('the', '').str.replace(':', '').str.strip() == input_movie]['index'].values[0]\n",
    "    \n",
    "    print(\"\\nThe index of the movie on the located dataset is: {}\".format(locked_frame_index))\n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Phase 3: Keeping the words with meaning\n",
    "    \n",
    "    # What I have noticed is that many words included in both Plot Summary and Combined Features were not capable to discremenate \n",
    "    # a movie. Thus, I tried to keep only the words that could better differenciate a movie from its peer.\n",
    "    \n",
    "    # Tfidf vectorizer\n",
    "    \n",
    "    tfv = TfidfVectorizer(use_idf=True)\n",
    "    \n",
    "    x = tfv.fit_transform(locked_frame['movie_features'])\n",
    "\n",
    "    \n",
    "    # get the vector of the matched movie\n",
    "    \n",
    "    vector_tfidfvectorizer=x[locked_frame_index]\n",
    " \n",
    "    \n",
    "    # place tf-idf values in a pandas data frame\n",
    "    \n",
    "    df = pd.DataFrame(vector_tfidfvectorizer.T.todense(), index=tfv.get_feature_names(), columns=[\"Importance/word\"])\n",
    "    \n",
    "    df = df.sort_values(by=[\"Importance/word\"], ascending=False)\n",
    "    \n",
    "    df = df[df.loc[:,'Importance/word']>0.1]\n",
    "    \n",
    "    list_of_words = df.loc[:,'Importance/word'].index.tolist()\n",
    "\n",
    "    \n",
    "    plot_user_input_list_new = union(list_of_words, plot_user_input_list)\n",
    "    \n",
    "    # Having the list of words to keep, it is useful to store those words in order to use the later!\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Phase 4: Locate the word embeddings belonging to each of the three different columns (Actors, Plot, Features)\n",
    "    \n",
    "    # Phase 4.1: Locate the embeddings of the movie selected by the user!\n",
    "    \n",
    "    # Get Casting Embeddings based on the movie_index\n",
    "\n",
    "    cast_vector_average = dataset['average_cast_vectors'][dataset['index'] == movie_index]\n",
    "    cast_vector_min = dataset['minimum_cast_vectors'][dataset['index'] == movie_index]\n",
    "    cast_vector_max = dataset['maximum_cast_vectors'][dataset['index'] == movie_index]\n",
    "    \n",
    "    cast_vector = np.hstack([cast_vector_average.apply(pd.Series).values,\n",
    "                             cast_vector_min.apply(pd.Series).values,\n",
    "                             cast_vector_max.apply(pd.Series).values])\n",
    "    \n",
    "    # Get Plot Embeddings based on the movie_index\n",
    "\n",
    "    plot_vector_average = dataset['average_plot_vectors'][dataset['index'] == movie_index]\n",
    "    plot_vector_min = dataset['minimum_plot_vectors'][dataset['index'] == movie_index]\n",
    "    plot_vector_max = dataset['maximum_plot_vectors'][dataset['index'] == movie_index]\n",
    "    \n",
    "    plot_vector = np.hstack([plot_vector_average.apply(pd.Series).values,\n",
    "                             plot_vector_min.apply(pd.Series).values,\n",
    "                             plot_vector_max.apply(pd.Series).values])\n",
    "\n",
    "    # Get Features Embeddings based on the movie_index\n",
    "\n",
    "    feature_vector_average = dataset['average_combined_features_vectors'][dataset['index'] == movie_index]\n",
    "    feature_vector_min = dataset['minimum_combined_features_vectors'][dataset['index'] == movie_index]\n",
    "    feature_vector_max = dataset['maximum_combined_features_vectors'][dataset['index'] == movie_index]\n",
    "    \n",
    "    feature_vector = np.hstack([feature_vector_average.apply(pd.Series).values,\n",
    "                                feature_vector_min.apply(pd.Series).values,\n",
    "                                feature_vector_max.apply(pd.Series).values])\n",
    "\n",
    "    # Get Reviews Embeddings based on the movie_index\n",
    "\n",
    "    reviews_vector_average = dataset['average_reviews_vectors'][dataset['index'] == movie_index]\n",
    "    reviews_vector_min = dataset['minimum_reviews_vectors'][dataset['index'] == movie_index]\n",
    "    reviews_vector_max = dataset['maximum_reviews_vectors'][dataset['index'] == movie_index]\n",
    "    \n",
    "    reviews_vector = np.hstack([reviews_vector_average.apply(pd.Series).values,\n",
    "                                reviews_vector_min.apply(pd.Series).values,\n",
    "                                reviews_vector_max.apply(pd.Series).values])\n",
    "\n",
    "    \n",
    "    # Phase 4.2: Locate the embeddings of the movies that match the GENRE given by the user (i.e the embeddings of all the ACTION movies)\n",
    "    \n",
    "    # Load the saved embeddings trained by the multi-input keras classifier\n",
    "    \n",
    "    with open('C:\\\\Users\\\\spano\\\\Desktop\\\\GitHub-Thesis\\\\models_text_classification\\\\model_one\\\\keras_embeddings_array_concatenated_08032020.pkl', 'rb') as f:\n",
    "        keras_embeddings_array_concatenated = pickle.load(f)\n",
    "    \n",
    "    # Indexes list is a list of the index each \"genre\" movie has in the locked dataframe (i.e the index of all the action movies)\n",
    "    \n",
    "    locked_movie_embeddings = keras_embeddings_array_concatenated[indexes_list]\n",
    "    \n",
    "\n",
    "    # -------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # Concatenate the embeddings\n",
    "\n",
    "    selected_movie_embeddings = np.hstack([cast_vector, plot_vector, feature_vector, reviews_vector])\n",
    "    \n",
    "    print(\"\\nThe shape of the selected_movie_embeddings is: {}\".format(selected_movie_embeddings.shape))\n",
    "    \n",
    "    print(\"\\nThe shape of the locked_movie_embeddings is: {}\".format(locked_movie_embeddings.shape))\n",
    "    \n",
    "    assert selected_movie_embeddings.shape[1] == locked_movie_embeddings.shape[1]\n",
    "    \n",
    "    # The dimension of those two arrays should be the same.\n",
    "    \n",
    "    # Calculate Cosine Distance\n",
    "\n",
    "    cosine_dist = cosine_distances(locked_movie_embeddings, selected_movie_embeddings.reshape(1,-1))\n",
    "\n",
    "    \n",
    "    # Get the similar movies & Slice the dataframe on the top 5 most similar movies to the movie given  by the user\n",
    "\n",
    "    movie_return = np.argsort(cosine_dist, axis=None).tolist()[1:6]\n",
    "\n",
    "    # movie_return contains the index of the 5 movies most similar to the movie selected by the user!\n",
    "    \n",
    "    # So the next step is to isolate those 5 movies and their features\n",
    "    \n",
    "    \n",
    "    locked_frame_new = locked_frame[locked_frame.loc[:, 'index'].isin(movie_return)]\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------------------\n",
    "\n",
    "    # Phase 5: Create two new columns \"Unique Words\" + \"Number of words\"\n",
    "    \n",
    "    # This needs some extra thought and development.....!\n",
    "\n",
    "    # Create the new column of \"UNIQUE\" words of the combined features\n",
    "    \n",
    "    locked_frame_new.loc[:, 'unique_words'] = locked_frame_new.loc[:, 'movie_features'].apply(lambda x: list(set(re.split(' |,|\\n', x.strip().lower()))))\n",
    "\n",
    "    locked_frame_new.loc[:, 'unique_words'] = locked_frame_new.loc[:, 'unique_words'].apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "    locked_frame_new.loc[:, 'unique_words'] = [[x for x in lst if x] for lst in locked_frame_new.loc[:, 'unique_words']]\n",
    "  \n",
    "    # Create the column \"Number of words\" for each word contained in the unique words column\n",
    "\n",
    "    locked_frame_new.loc[:, 'number_of_words'] = locked_frame_new.loc[:, 'unique_words'].apply(search_words, args=(plot_user_input_list_new,))\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    # Phase 6.1: Recommend to the user the three most similar and highly scored movies \n",
    "    \n",
    "    \n",
    "    # Calculate the movie score\n",
    "\n",
    "    primary_genre = list((locked_frame_new.loc[:, \"lower_case_genres\"].map(lambda x: input_one in x)*0.2))\n",
    "\n",
    "    locked_frame_new.loc[:, 'movie_score'] = 0.1*locked_frame_new.loc[:, 'imdb_rating'].astype(float) + 0.5*locked_frame_new.loc[:, 'number_of_words'] + 0.2*locked_frame_new.loc[:, \"sentiment_value\"]\n",
    "\n",
    "    locked_frame_new.loc[:, 'movie_score'] = locked_frame_new.loc[:, 'movie_score'] + primary_genre[0]\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # Give to the user the proper movie recommendation\n",
    "\n",
    "    top_three_rows = locked_frame_new.nlargest(3, 'movie_score')\n",
    "    \n",
    "    # top_three_rows.rename(columns={'movie_title':'Movie Title', 'updated_rating':'IMDB Rate', 'movie_imdb_link':\"Movie's Link\"}, inplace=True)\n",
    "\n",
    "    # Recommend the movie\n",
    "\n",
    "    recommendations_list = top_three_rows.loc[:, ['title', 'imdb_rating', 'imdb_url']].values.tolist()\n",
    "    \n",
    "    print(\"Movie Recommendations: {}\".format(recommendations_list))\n",
    "    \n",
    "else:\n",
    "    \n",
    "    plot_user_input_list = inputs_list\n",
    "    \n",
    "    locked_frame.loc[:, 'unique_words'] = locked_frame.loc[:, 'movie_features'].apply(lambda x: list(set(re.split(' |,|\\n', x.strip().lower()))))\n",
    "\n",
    "    locked_frame.loc[:, 'unique_words'] = locked_frame.loc[:, 'unique_words'].apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "    locked_frame.loc[:, 'unique_words'] = [[x for x in lst if x] for lst in locked_frame.loc[:, 'unique_words']]\n",
    "  \n",
    "    # Create the column \"Number of words\" for each word contained in the unique words column\n",
    "\n",
    "    locked_frame.loc[:, 'number_of_words'] = locked_frame.unique_words.apply(search_words, args=(plot_user_input_list,))\n",
    "\n",
    "    \n",
    "    \n",
    "    # Phase 6.2: Recommend to the user the three most similar and highly scored movies\n",
    "    \n",
    "    \n",
    "    primary_genre = list((locked_frame.loc[:, \"lower_case_genres\"].map(lambda x: input_one in x)*0.2))\n",
    "\n",
    "    locked_frame.loc[:, 'movie_score'] = 0.1*locked_frame.loc[:, 'imdb_rating'].astype(float) + 0.5*locked_frame.loc[:, 'number_of_words'] + 0.2*locked_frame.loc[:, 'rating']\n",
    "\n",
    "    locked_frame.loc[:, 'movie_score'] = locked_frame.loc[:, 'movie_score'] + primary_genre[0]\n",
    "    \n",
    "    \n",
    "    # Give to the user the proper movie recommendation\n",
    "\n",
    "    top_three_rows = locked_frame.nlargest(3, 'movie_score')\n",
    "    \n",
    "    # top_three_rows.rename(columns={'movie_title':'Movie Title', 'updated_rating':'IMDB Rate', 'movie_imdb_link':\"Movie's Link\"}, inplace=True)\n",
    "\n",
    "    \n",
    "    # Recommend the movie\n",
    "\n",
    "    recommendations_list = top_three_rows.loc[:, ['title', 'imdb_rating', 'imdb_url']].values.tolist()\n",
    "    \n",
    "    print(\"\\nMovie Recommendations: {}\".format(recommendations_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discovery Notes - Further Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With using the Keras Classification Word Embeddings, the proposed movies, most of the times, belong to the same genre triple as the movie selected. This is a very good and positive thing of how good the embeddings are compaired to the FastText algorithm!\n",
    "\n",
    "* For further development I could optimize further the time taken to execute the algorithm\n",
    "* Optimize the calculation function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run app_v11.py (using the cmd terminal on Windows):\n",
    "\n",
    "Step 1: Set the path directory to: Desktop (if you have saved the chatbotapp_v11.py file in Desktop) <br>\n",
    "Step 2 (Run the command): python app.py or FLASK_APP=hello.py flask run\n",
    "\n",
    "#### Run the https protocole (using the cmd terminal on Windows): \n",
    "\n",
    "Step 1: Set the path directory to: C:\\Users\\dq186sy\\Desktop\\ngrok-stable-windows-amd64 (or the path where the ngrok.exe is saved) <br>\n",
    "Step 2 (Run the command): ngrok http 5000 <br>\n",
    "Step 3: Copy paste the **https** link that ends to .io (this link is updated every time the command is executed) <br>\n",
    "Step 4: Copy paste the link to dialogflow engine under the tab: fulfilment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Part 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
