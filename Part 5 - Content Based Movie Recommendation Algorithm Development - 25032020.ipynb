{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5 - Content Based Movie Recommendation Algorithm Development (latest changes on 08.03.2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cleaning and preparing the dataset\n",
    "# -> dataframe manipulation\n",
    "# -> text manipulation\n",
    "# -> Web Scrapping\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tabulate import tabulate\n",
    "import re\n",
    "import os\n",
    "import decimal\n",
    "\n",
    "import random\n",
    "\n",
    "# Module to serialize the content produced from the execution of the code\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Module to monitor the progress of a python for loop\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# Module to manipulate text in python - NLTK package\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Module to compute word vectorizers and compute the cosine distance\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "import string\n",
    "import itertools\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:70% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48992, 54)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_pickle(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\dataset_part_4_29032020.pkl'))\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommendation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>Version 18 - Built on 25.03.2020</b><i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions used --------------------------------------------------------------------------------------------------\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "def get_index_from_input_movie(user_input):\n",
    "    return dataset[dataset['title'].str.lower().str.replace('-', '').str.replace('the', '').str.replace(':', '').str.strip() == user_input]['index'].values[0]\n",
    "\n",
    "def search_words(row, list_of_words):\n",
    "    counter = 0\n",
    "    for word in list_of_words:\n",
    "        if word in row:\n",
    "            counter = counter + 1\n",
    "    return counter\n",
    "\n",
    "def find_correct_genre(user_input, genre_list):\n",
    "    scores_sim=[]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    for item in genre_list:\n",
    "        ed = nltk.edit_distance(user_input, item)\n",
    "        scores_sim.append(ed)\n",
    "    correct_genre_index = scores_sim.index(min(scores_sim))\n",
    "    correct_genre = genre_list[correct_genre_index].lower()\n",
    "    return correct_genre\n",
    "\n",
    "def union(lst1, lst2): \n",
    "    final_list = list(set(lst1) | set(lst2)) \n",
    "    return final_list\n",
    "\n",
    "def drange(x, y, jump):\n",
    "    while x < y:\n",
    "        yield float(x)\n",
    "        x += decimal.Decimal(jump)\n",
    "\n",
    "def create_imdb_range(x):\n",
    "    if x in list(drange(8, 10, '0.1')):\n",
    "        return 0.2\n",
    "    elif x in list(drange(6, 8, '0.1')):\n",
    "        return 0.4\n",
    "    elif x in list(drange(4, 6, '0.1')):\n",
    "        return 0.6\n",
    "    elif x in list(drange(2, 4, '0.1')):\n",
    "        return 0.8\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "def preprocess_text(raw_text):\n",
    "    \n",
    "    re_punc=re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    \n",
    "    stripped=[re_punc.sub('', w) for w in raw_text.split(' ')]\n",
    "    \n",
    "    stripped=[token for token in stripped if token.isalpha()]\n",
    "    \n",
    "    #------------------------------------------------\n",
    "    \n",
    "    stop_words=text.ENGLISH_STOP_WORDS.union([\"book\"])\n",
    "    \n",
    "    no_stopword_text=[word for word in stripped if not word.lower() in stop_words]\n",
    "    \n",
    "    no_stopword_text = ' '.join(no_stopword_text) #i joined the text once more because a new lemmatizing approach is implemented below\n",
    "    \n",
    "    #------------------------------------------------\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    #approach 1: lemmatized_text = [lemmatizer.lemmatize(word, pos='v') for word in stripped]\n",
    "    #approach 1 was used until 21.02.2020, although we observed that only some of the tokens were lemmatized while others not.\n",
    "    #Thus, we developed an alternative approach like below to lemmatize as many tokens/words as possible\n",
    "    \n",
    "    #approach 2 developed on 22.02.2020:\n",
    "    lemmatized_text = [lemmatizer.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else lemmatizer.lemmatize(i) for i,j in pos_tag(word_tokenize(no_stopword_text))]\n",
    "    \n",
    "    #------------------------------------------------\n",
    "    \n",
    "    lowercase_text = [word.lower() for word in lemmatized_text]\n",
    "    \n",
    "    return ' '.join(lowercase_text)\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Import the dataset\n",
    "\n",
    "# dataset = pd.read_pickle('C:\\\\Users\\\\dq186sy\\\\Desktop\\\\Big Data Content Analytics\\\\Movie Recommendation System\\\\dataset_embedded_02092019.pkl')\n",
    "\n",
    "dataset = pd.read_pickle(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\dataset_part_4_29032020.pkl'))\n",
    "\n",
    "dataset = dataset.reset_index()\n",
    "\n",
    "dataset['index'] = np.arange(0, len(dataset))\n",
    "\n",
    "# It is important to reset the index of the dataset in order to get the correct index per movie!\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Create the movie_genre list with the unique types of genre \n",
    "\n",
    "movie_genre_list=dataset.iloc[:, 14:31].columns.tolist()\n",
    "\n",
    "movie_genre_list = [x.lower() for x in movie_genre_list]\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Phase 1: Get the user's input and transform it to the appropriate form\n",
    "\n",
    "input_one = input(\"Give me a movie genre (i.e romance, action, adventure): \")\n",
    "\n",
    "input_one = find_correct_genre(input_one.lower(), movie_genre_list)\n",
    "\n",
    "print(\"The movie genre selected by the user: {}\".format(input_one))\n",
    "\n",
    "input_movie = input(\"Give me the title of a movie: \").lower().replace('-', '').replace('the', '').replace(':', '').strip()\n",
    "\n",
    "print(\"The movie title selected by the user: {}\".format(input_movie))\n",
    "\n",
    "input_two = input(\"Now think of some reasons why you like '{}':\".format(input_movie)).lower().replace(',', '').replace('.', '')\n",
    "\n",
    "inputs_list=preprocess_text(input_two).split(' ')\n",
    "inputs_list = list(dict.fromkeys(inputs_list)) # remove duplicate words\n",
    "\n",
    "print(\"My inputs list before cleaning the text: {}\".format(input_two))\n",
    "print(\"\\nMy inputs list after cleaning the text: {}\".format(inputs_list))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Using the genre input given by the user, isolate those movies that match the given genre (i.e Action movies)\n",
    "\n",
    "lower_case_genres = []\n",
    "\n",
    "for i in range(len(dataset.loc[:, 'reduced_genres'])):\n",
    "    lower_case_genres.append([element.lower() for element in dataset.loc[:, 'reduced_genres'].iloc[i]])\n",
    "    \n",
    "dataset.loc[:,'lower_case_genres'] = lower_case_genres\n",
    "\n",
    "selected_rows = dataset.loc[:, 'lower_case_genres'].apply(lambda x: any(item for item in x if item == input_one))\n",
    "\n",
    "locked_frame = dataset[selected_rows]\n",
    "\n",
    "indexes_list = locked_frame.loc[:, 'index'].tolist()\n",
    "\n",
    "locked_frame.loc[:, 'index'] = np.arange(0, len(locked_frame))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Phase 2: Slice the dataset based on the user's input & add to the user's list of word, the words contained in the movie features\n",
    "\n",
    "# Check of the movie user gave is in the movie list of the dataset\n",
    "\n",
    "selected_genre_movies_list = locked_frame['title'].str.lower().str.replace('-', '').str.replace('the', '').str.replace(':', '').str.strip().tolist()\n",
    "\n",
    "if input_movie in selected_genre_movies_list:\n",
    "    \n",
    "    movie_plot_new = locked_frame.loc[:, 'clean_combined_features'].loc[(locked_frame['title'].str.lower().str.replace('-', '').str.replace('the', '').str.replace(':', '').str.strip() == input_movie)].apply(lambda x: list(set(re.split(' ', x.strip().lower())))).values[0]    \n",
    "    \n",
    "    plot_user_input_list = inputs_list + movie_plot_new\n",
    "    \n",
    "    plot_user_input_list = list(dict.fromkeys(plot_user_input_list))\n",
    "    \n",
    "    print(\"\\nThe words found in the movie's features added to the user's input: {}\".format(plot_user_input_list))\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Get the index of the movie provied by the user\n",
    "\n",
    "    movie_index = get_index_from_input_movie(input_movie)\n",
    "    \n",
    "    print(\"\\nThe index of the movie on the initial dataset is: {}\".format(movie_index))\n",
    "    \n",
    "    # Based on the index from the initial dataset locate the same in the Locked_frame.\n",
    "    # It is important to locate the same movie!\n",
    "    \n",
    "    locked_frame_index = locked_frame.loc[locked_frame['title'].str.lower().str.replace('-', '').str.replace('the', '').str.replace(':', '').str.strip() == input_movie]['index'].values[0]\n",
    "    \n",
    "    print(\"\\nThe index of the movie on the located dataset is: {}\".format(locked_frame_index))\n",
    "    \n",
    "    assert dataset.title.iloc[movie_index]==locked_frame.title.iloc[locked_frame_index]\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Phase 3: Locate the word embeddings belonging to each of the three different columns (Actors, Plot, Features, Reviews)\n",
    "    \n",
    "    # Load the saved embeddings trained by the multi-input keras classifier (embeddings of 49)\n",
    "    with open(os.path.join(os.getcwd(), 'model_one\\\\keras_embeddings_array_concatenated_{0}_{1}_25032020.pkl'.format(str(100), str(16))), 'rb') as f:\n",
    "        \n",
    "        keras_embeddings_array_concatenated = pickle.load(f)\n",
    "        \n",
    "    # Phase 3.1: Locate the embeddings of the movie selected by the user!\n",
    "    \n",
    "    selected_movie_embeddings = keras_embeddings_array_concatenated[movie_index]\n",
    "    \n",
    "    selected_movie_embeddings=selected_movie_embeddings.reshape(1,-1)\n",
    "    \n",
    "    # Phase 3.2: Locate the embeddings of the movies that match the GENRE given by the user (i.e the embeddings of all the ACTION movies)\n",
    "    \n",
    "    locked_movie_embeddings = keras_embeddings_array_concatenated[indexes_list]\n",
    "    \n",
    "    assert selected_movie_embeddings.shape[1] == locked_movie_embeddings.shape[1]\n",
    "\n",
    "    # -------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Phase 4: Calculate Cosine Distance\n",
    "\n",
    "    cosine_dist = cosine_distances(locked_movie_embeddings, selected_movie_embeddings.reshape(1,-1))\n",
    "    \n",
    "    # Get the similar movies & Slice the dataframe on the top 15 most similar movies to the movie given  by the user\n",
    "\n",
    "    movie_return = np.argsort(cosine_dist, axis=None).tolist()[1:16]\n",
    "\n",
    "    # movie_return contains the index of the 15 movies most similar to the movie selected by the user!\n",
    "    \n",
    "    # So the next step is to isolate those 15 movies and their features\n",
    "    \n",
    "    locked_frame_new = locked_frame[locked_frame.loc[:, 'index'].isin(movie_return)]\n",
    "\n",
    "    # -------------------------------------------------------------------------------------\n",
    "\n",
    "    # Phase 5: Create two new columns \"Unique Words\" + \"Number of words\"\n",
    "    \n",
    "    # This needs some extra thought and development.....!\n",
    "\n",
    "    # Create the new column of \"UNIQUE\" words of the combined features\n",
    "    locked_frame_new.loc[:, 'unique_words'] = locked_frame_new.loc[:, 'clean_combined_features']+locked_frame_new.loc[:, 'clean_reviews']\n",
    "\n",
    "    locked_frame_new.loc[:, 'unique_words'] = locked_frame_new.loc[:, 'unique_words'].apply(lambda x: list(set(re.split(' ', x.strip().lower()))))\n",
    "\n",
    "    locked_frame_new.loc[:, 'unique_words'] = [[x for x in lst if x] for lst in locked_frame_new.loc[:, 'unique_words']]\n",
    "  \n",
    "    # Create the column \"Number of words\" for each word contained in the unique words column\n",
    "\n",
    "    locked_frame_new.loc[:, 'number_of_words'] = locked_frame_new.loc[:, 'unique_words'].apply(search_words, args=(plot_user_input_list,))\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------------------\n",
    "\n",
    "    # Phase 6: Recommend to the user the three most similar and highly scored movies \n",
    "    \n",
    "    # Calculate the movie score\n",
    "    \n",
    "    locked_frame_new['imdb_rating_range']=locked_frame_new['imdb_rating'].apply(create_imdb_range)\n",
    "\n",
    "    locked_frame_new.loc[:, 'movie_score'] = 1*locked_frame_new.loc[:, 'imdb_rating_range'].astype(float) + 0.5*locked_frame_new.loc[:, 'number_of_words'] + 0.5*locked_frame_new.loc[:, \"sentiment_value\"] + 1*locked_frame_new.loc[:, \"rating\"]\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------\n",
    "\n",
    "    # Give to the user the proper movie recommendation\n",
    "\n",
    "    top_four_rows = locked_frame_new.nlargest(4, 'movie_score')\n",
    "\n",
    "    # Recommend the movie\n",
    "\n",
    "    recommendations_list = top_four_rows.loc[:, ['title', 'imdb_rating', 'imdb_url']].values.tolist()\n",
    "    \n",
    "    print(\"Movie Recommendations: {}\".format(recommendations_list))\n",
    "    \n",
    "else:\n",
    "    \n",
    "    plot_user_input_list = inputs_list\n",
    "    \n",
    "    locked_frame.loc[:, 'unique_words'] = locked_frame.loc[:, 'clean_combined_features']+locked_frame.loc[:, 'clean_reviews']\n",
    "\n",
    "    locked_frame.loc[:, 'unique_words'] = locked_frame.loc[:, 'unique_words'].apply(lambda x: list(set(re.split(' ', x.strip().lower()))))\n",
    "\n",
    "    locked_frame.loc[:, 'unique_words'] = [[x for x in lst if x] for lst in locked_frame.loc[:, 'unique_words']]\n",
    "  \n",
    "    # Create the column \"Number of words\" for each word contained in the unique words column\n",
    "\n",
    "    locked_frame.loc[:, 'number_of_words'] = locked_frame.unique_words.apply(search_words, args=(plot_user_input_list,))\n",
    "\n",
    "    #Recommend to the user the three most similar and highly scored movies\n",
    "    \n",
    "    locked_frame['imdb_rating_range']=locked_frame['imdb_rating'].apply(create_imdb_range)\n",
    "\n",
    "    locked_frame.loc[:, 'movie_score'] = 1*locked_frame.loc[:, 'imdb_rating_range'].astype(float) + 0.5*locked_frame.loc[:, 'number_of_words'] + 0.5*locked_frame.loc[:, \"sentiment_value\"] + 0.5*locked_frame.loc[:, \"rating\"]\n",
    "    \n",
    "    # Give to the user the proper movie recommendation\n",
    "\n",
    "    top_four_rows = locked_frame.nlargest(4, 'movie_score')\n",
    "    \n",
    "    # Recommend the movie\n",
    "\n",
    "    recommendations_list = top_four_rows.loc[:, ['title', 'imdb_rating', 'imdb_url']].values.tolist()\n",
    "    \n",
    "    print(\"\\nMovie Recommendations: {}\".format(recommendations_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locked_frame_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>rating</th>\n",
       "      <th>imdb_url</th>\n",
       "      <th>reviews_url</th>\n",
       "      <th>actors</th>\n",
       "      <th>plot</th>\n",
       "      <th>imdb_rating</th>\n",
       "      <th>director</th>\n",
       "      <th>...</th>\n",
       "      <th>average_plot_vectors</th>\n",
       "      <th>combined_features_embeddings_list</th>\n",
       "      <th>minimum_combined_features_vectors</th>\n",
       "      <th>maximum_combined_features_vectors</th>\n",
       "      <th>average_combined_features_vectors</th>\n",
       "      <th>reviews_embeddings_list</th>\n",
       "      <th>minimum_reviews_vectors</th>\n",
       "      <th>maximum_reviews_vectors</th>\n",
       "      <th>average_reviews_vectors</th>\n",
       "      <th>lower_case_genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>14393</td>\n",
       "      <td>14393</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>[Action, Adventure, Sci-Fi]</td>\n",
       "      <td>3.65</td>\n",
       "      <td>http://www.imdb.com/title/tt0499549/</td>\n",
       "      <td>http://www.imdb.com/title/tt0499549/reviews?sp...</td>\n",
       "      <td>[Sam Worthington, Zoe Saldana, Sigourney Weave...</td>\n",
       "      <td>A paraplegic Marine dispatched to the moon Pan...</td>\n",
       "      <td>7.8</td>\n",
       "      <td>James Cameron</td>\n",
       "      <td>...</td>\n",
       "      <td>[-0.5892326, -0.65903854, -0.72043765, -0.5795...</td>\n",
       "      <td>[[-0.35044152, -0.37348762, -0.43553886, -0.42...</td>\n",
       "      <td>[-0.54247403, -0.56902313, -0.55614007, -0.596...</td>\n",
       "      <td>[9.525108, 0.22374006, 0.32066453, 0.103855066...</td>\n",
       "      <td>[-0.11567749, -0.3252455, -0.3635674, -0.39955...</td>\n",
       "      <td>[[-0.686603, -0.501658, -0.46163273, -0.536485...</td>\n",
       "      <td>[-0.686603, -0.64527255, -0.74836385, -0.75802...</td>\n",
       "      <td>[0.23848216, 0.0081959115, 0.052962944, 0.3840...</td>\n",
       "      <td>[-0.46788955, -0.46702227, -0.4696468, -0.5003...</td>\n",
       "      <td>[action, adventure, sci-fi]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index   title                       genres  rating  \\\n",
       "14393  14393  Avatar  [Action, Adventure, Sci-Fi]    3.65   \n",
       "\n",
       "                                   imdb_url  \\\n",
       "14393  http://www.imdb.com/title/tt0499549/   \n",
       "\n",
       "                                             reviews_url  \\\n",
       "14393  http://www.imdb.com/title/tt0499549/reviews?sp...   \n",
       "\n",
       "                                                  actors  \\\n",
       "14393  [Sam Worthington, Zoe Saldana, Sigourney Weave...   \n",
       "\n",
       "                                                    plot  imdb_rating  \\\n",
       "14393  A paraplegic Marine dispatched to the moon Pan...          7.8   \n",
       "\n",
       "            director  ...                               average_plot_vectors  \\\n",
       "14393  James Cameron  ...  [-0.5892326, -0.65903854, -0.72043765, -0.5795...   \n",
       "\n",
       "                       combined_features_embeddings_list  \\\n",
       "14393  [[-0.35044152, -0.37348762, -0.43553886, -0.42...   \n",
       "\n",
       "                       minimum_combined_features_vectors  \\\n",
       "14393  [-0.54247403, -0.56902313, -0.55614007, -0.596...   \n",
       "\n",
       "                       maximum_combined_features_vectors  \\\n",
       "14393  [9.525108, 0.22374006, 0.32066453, 0.103855066...   \n",
       "\n",
       "                       average_combined_features_vectors  \\\n",
       "14393  [-0.11567749, -0.3252455, -0.3635674, -0.39955...   \n",
       "\n",
       "                                 reviews_embeddings_list  \\\n",
       "14393  [[-0.686603, -0.501658, -0.46163273, -0.536485...   \n",
       "\n",
       "                                 minimum_reviews_vectors  \\\n",
       "14393  [-0.686603, -0.64527255, -0.74836385, -0.75802...   \n",
       "\n",
       "                                 maximum_reviews_vectors  \\\n",
       "14393  [0.23848216, 0.0081959115, 0.052962944, 0.3840...   \n",
       "\n",
       "                                 average_reviews_vectors  \\\n",
       "14393  [-0.46788955, -0.46702227, -0.4696468, -0.5003...   \n",
       "\n",
       "                 lower_case_genres  \n",
       "14393  [action, adventure, sci-fi]  \n",
       "\n",
       "[1 rows x 56 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset.title=='Avatar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['action',\n",
       " 'adventure',\n",
       " 'animation',\n",
       " 'children',\n",
       " 'comedy',\n",
       " 'crime',\n",
       " 'documentary',\n",
       " 'drama',\n",
       " 'fantasy',\n",
       " 'horror',\n",
       " 'musical',\n",
       " 'mystery',\n",
       " 'romance',\n",
       " 'sci-fi',\n",
       " 'thriller',\n",
       " 'war',\n",
       " 'western']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_genre_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discovery Notes - Further Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With using the Keras Classification Word Embeddings, the proposed movies, most of the times, belong to the same genre triple as the movie selected. This is a very good and positive thing of how good the embeddings are compaired to the FastText algorithm!\n",
    "\n",
    "In our latest development we changed the following:\n",
    "\n",
    "* we used many more movies (from 10,000 to 50,000 movies),\n",
    "* We add to the weighted calculation of the movie score, the user's positive or negative intent over the movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run chatbotapp_v17.py (using the cmd terminal on Windows):\n",
    "\n",
    "Step 1: Set the path directory to: Desktop (if you have saved the chatbotapp_v17.py file in Desktop) <br>\n",
    "Step 2 (Run the command): python app.py or FLASK_APP=hello.py flask run\n",
    "\n",
    "#### Run the https protocole (using the cmd terminal on Windows): \n",
    "\n",
    "Open a cmd terminal and then:\n",
    "Step 1: Set the path directory to the path where the ngrok.exe is saved (when you first downloaded) <br>\n",
    "Step 2  (Run the command): ngrok http + \"port number\" (port number where the app.py file runs)<br>\n",
    "Step 3: Copy paste the **https** link that ends to .io (this link is updated every time the command is executed) <br>\n",
    "Step 4: Copy paste the link to dialogflow engine under the tab: fulfilment.\n",
    "\n",
    "! **Important:** Webhook and MLFLOW cannot run the same port"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Part 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
