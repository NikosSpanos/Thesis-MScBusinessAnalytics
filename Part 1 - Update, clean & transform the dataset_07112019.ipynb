{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cleaning and preparing the dataset\n",
    "# -> dataframe manipulation\n",
    "# -> text manipulation\n",
    "# -> Web Scrapping\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "\n",
    "# Module to serialize the content produced from the execution of the code\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Module to monitor the progress of a python for loop\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Module to manipulate text in python - NLTK package\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Module to compute word vectorizers and compute the cosine distance\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pyodbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the two datasets (07.11.2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first dataset of 4774 movies already made used of them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle('dataset_one_07112019.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The second dataset of movies download on 08.10.2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 66: expected 44 fields, saw 46\\nSkipping line 111: expected 44 fields, saw 45\\nSkipping line 198: expected 44 fields, saw 45\\nSkipping line 222: expected 44 fields, saw 46\\nSkipping line 278: expected 44 fields, saw 45\\nSkipping line 396: expected 44 fields, saw 45\\nSkipping line 403: expected 44 fields, saw 45\\nSkipping line 421: expected 44 fields, saw 45\\nSkipping line 437: expected 44 fields, saw 45\\nSkipping line 462: expected 44 fields, saw 46\\nSkipping line 491: expected 44 fields, saw 45\\nSkipping line 515: expected 44 fields, saw 45\\nSkipping line 529: expected 44 fields, saw 45\\nSkipping line 530: expected 44 fields, saw 45\\nSkipping line 558: expected 44 fields, saw 45\\nSkipping line 623: expected 44 fields, saw 45\\nSkipping line 646: expected 44 fields, saw 45\\nSkipping line 663: expected 44 fields, saw 46\\nSkipping line 713: expected 44 fields, saw 45\\nSkipping line 730: expected 44 fields, saw 47\\nSkipping line 791: expected 44 fields, saw 45\\nSkipping line 813: expected 44 fields, saw 45\\nSkipping line 837: expected 44 fields, saw 45\\nSkipping line 861: expected 44 fields, saw 45\\nSkipping line 874: expected 44 fields, saw 45\\nSkipping line 899: expected 44 fields, saw 45\\nSkipping line 917: expected 44 fields, saw 45\\nSkipping line 944: expected 44 fields, saw 46\\nSkipping line 994: expected 44 fields, saw 45\\nSkipping line 1027: expected 44 fields, saw 45\\nSkipping line 1046: expected 44 fields, saw 45\\nSkipping line 1097: expected 44 fields, saw 45\\nSkipping line 1106: expected 44 fields, saw 45\\nSkipping line 1170: expected 44 fields, saw 45\\nSkipping line 1194: expected 44 fields, saw 45\\nSkipping line 1195: expected 44 fields, saw 45\\nSkipping line 1218: expected 44 fields, saw 45\\nSkipping line 1220: expected 44 fields, saw 45\\nSkipping line 1270: expected 44 fields, saw 45\\nSkipping line 1338: expected 44 fields, saw 47\\nSkipping line 1355: expected 44 fields, saw 45\\nSkipping line 1363: expected 44 fields, saw 45\\nSkipping line 1395: expected 44 fields, saw 45\\nSkipping line 1402: expected 44 fields, saw 46\\nSkipping line 1418: expected 44 fields, saw 45\\nSkipping line 1431: expected 44 fields, saw 45\\nSkipping line 1617: expected 44 fields, saw 45\\nSkipping line 1663: expected 44 fields, saw 45\\nSkipping line 1742: expected 44 fields, saw 46\\nSkipping line 1766: expected 44 fields, saw 45\\nSkipping line 1799: expected 44 fields, saw 45\\nSkipping line 1867: expected 44 fields, saw 45\\nSkipping line 1899: expected 44 fields, saw 45\\nSkipping line 1900: expected 44 fields, saw 45\\nSkipping line 1901: expected 44 fields, saw 45\\nSkipping line 1907: expected 44 fields, saw 45\\nSkipping line 1913: expected 44 fields, saw 45\\nSkipping line 1924: expected 44 fields, saw 45\\nSkipping line 1939: expected 44 fields, saw 45\\nSkipping line 1945: expected 44 fields, saw 45\\nSkipping line 1982: expected 44 fields, saw 45\\nSkipping line 2023: expected 44 fields, saw 45\\nSkipping line 2028: expected 44 fields, saw 45\\nSkipping line 2054: expected 44 fields, saw 45\\nSkipping line 2076: expected 44 fields, saw 45\\nSkipping line 2081: expected 44 fields, saw 45\\nSkipping line 2092: expected 44 fields, saw 45\\nSkipping line 2107: expected 44 fields, saw 45\\nSkipping line 2160: expected 44 fields, saw 45\\nSkipping line 2260: expected 44 fields, saw 45\\nSkipping line 2261: expected 44 fields, saw 45\\nSkipping line 2289: expected 44 fields, saw 46\\nSkipping line 2290: expected 44 fields, saw 45\\nSkipping line 2349: expected 44 fields, saw 45\\nSkipping line 2395: expected 44 fields, saw 45\\nSkipping line 2507: expected 44 fields, saw 45\\nSkipping line 2584: expected 44 fields, saw 45\\nSkipping line 2588: expected 44 fields, saw 46\\nSkipping line 2595: expected 44 fields, saw 45\\nSkipping line 2604: expected 44 fields, saw 45\\nSkipping line 2622: expected 44 fields, saw 45\\nSkipping line 2661: expected 44 fields, saw 45\\nSkipping line 2714: expected 44 fields, saw 45\\nSkipping line 2722: expected 44 fields, saw 45\\nSkipping line 2776: expected 44 fields, saw 45\\nSkipping line 2806: expected 44 fields, saw 45\\nSkipping line 2826: expected 44 fields, saw 45\\nSkipping line 2882: expected 44 fields, saw 45\\nSkipping line 2909: expected 44 fields, saw 45\\nSkipping line 3005: expected 44 fields, saw 45\\nSkipping line 3019: expected 44 fields, saw 45\\nSkipping line 3052: expected 44 fields, saw 45\\nSkipping line 3062: expected 44 fields, saw 45\\nSkipping line 3086: expected 44 fields, saw 45\\nSkipping line 3089: expected 44 fields, saw 45\\nSkipping line 3134: expected 44 fields, saw 46\\nSkipping line 3157: expected 44 fields, saw 45\\nSkipping line 3163: expected 44 fields, saw 45\\nSkipping line 3177: expected 44 fields, saw 45\\nSkipping line 3190: expected 44 fields, saw 45\\nSkipping line 3205: expected 44 fields, saw 45\\nSkipping line 3209: expected 44 fields, saw 45\\nSkipping line 3238: expected 44 fields, saw 45\\nSkipping line 3242: expected 44 fields, saw 45\\nSkipping line 3255: expected 44 fields, saw 45\\nSkipping line 3303: expected 44 fields, saw 45\\nSkipping line 3314: expected 44 fields, saw 45\\nSkipping line 3322: expected 44 fields, saw 45\\nSkipping line 3358: expected 44 fields, saw 45\\nSkipping line 3360: expected 44 fields, saw 46\\nSkipping line 3377: expected 44 fields, saw 45\\nSkipping line 3413: expected 44 fields, saw 45\\nSkipping line 3481: expected 44 fields, saw 45\\nSkipping line 3496: expected 44 fields, saw 45\\nSkipping line 3719: expected 44 fields, saw 45\\nSkipping line 3792: expected 44 fields, saw 45\\nSkipping line 3807: expected 44 fields, saw 46\\nSkipping line 3858: expected 44 fields, saw 45\\nSkipping line 3864: expected 44 fields, saw 45\\nSkipping line 3902: expected 44 fields, saw 45\\nSkipping line 3943: expected 44 fields, saw 45\\nSkipping line 3969: expected 44 fields, saw 45\\nSkipping line 4024: expected 44 fields, saw 47\\nSkipping line 4044: expected 44 fields, saw 45\\nSkipping line 4045: expected 44 fields, saw 45\\nSkipping line 4112: expected 44 fields, saw 45\\nSkipping line 4149: expected 44 fields, saw 45\\nSkipping line 4280: expected 44 fields, saw 45\\nSkipping line 4282: expected 44 fields, saw 45\\nSkipping line 4308: expected 44 fields, saw 45\\nSkipping line 4377: expected 44 fields, saw 45\\nSkipping line 4390: expected 44 fields, saw 45\\nSkipping line 4404: expected 44 fields, saw 45\\nSkipping line 4416: expected 44 fields, saw 45\\nSkipping line 4423: expected 44 fields, saw 46\\nSkipping line 4540: expected 44 fields, saw 45\\nSkipping line 4554: expected 44 fields, saw 45\\nSkipping line 4556: expected 44 fields, saw 46\\nSkipping line 4572: expected 44 fields, saw 45\\nSkipping line 4593: expected 44 fields, saw 45\\nSkipping line 4614: expected 44 fields, saw 45\\nSkipping line 4688: expected 44 fields, saw 45\\nSkipping line 4750: expected 44 fields, saw 45\\nSkipping line 4764: expected 44 fields, saw 45\\nSkipping line 4765: expected 44 fields, saw 45\\nSkipping line 4849: expected 44 fields, saw 45\\nSkipping line 4865: expected 44 fields, saw 45\\nSkipping line 4892: expected 44 fields, saw 45\\nSkipping line 4893: expected 44 fields, saw 45\\nSkipping line 4897: expected 44 fields, saw 45\\nSkipping line 4923: expected 44 fields, saw 45\\nSkipping line 4956: expected 44 fields, saw 45\\nSkipping line 4957: expected 44 fields, saw 45\\nSkipping line 4962: expected 44 fields, saw 45\\nSkipping line 4967: expected 44 fields, saw 45\\nSkipping line 4971: expected 44 fields, saw 45\\nSkipping line 5057: expected 44 fields, saw 45\\nSkipping line 5061: expected 44 fields, saw 45\\nSkipping line 5097: expected 44 fields, saw 45\\nSkipping line 5125: expected 44 fields, saw 45\\nSkipping line 5180: expected 44 fields, saw 45\\nSkipping line 5207: expected 44 fields, saw 45\\nSkipping line 5339: expected 44 fields, saw 45\\nSkipping line 5426: expected 44 fields, saw 45\\nSkipping line 5474: expected 44 fields, saw 45\\nSkipping line 5511: expected 44 fields, saw 45\\nSkipping line 5561: expected 44 fields, saw 45\\nSkipping line 5563: expected 44 fields, saw 45\\nSkipping line 5689: expected 44 fields, saw 45\\nSkipping line 5725: expected 44 fields, saw 45\\nSkipping line 5759: expected 44 fields, saw 45\\nSkipping line 5796: expected 44 fields, saw 45\\nSkipping line 5829: expected 44 fields, saw 45\\nSkipping line 5854: expected 44 fields, saw 45\\nSkipping line 5886: expected 44 fields, saw 45\\nSkipping line 5899: expected 44 fields, saw 45\\nSkipping line 5901: expected 44 fields, saw 45\\nSkipping line 5970: expected 44 fields, saw 45\\nSkipping line 5996: expected 44 fields, saw 45\\nSkipping line 6085: expected 44 fields, saw 45\\nSkipping line 6087: expected 44 fields, saw 45\\nSkipping line 6095: expected 44 fields, saw 45\\nSkipping line 6096: expected 44 fields, saw 45\\nSkipping line 6098: expected 44 fields, saw 45\\nSkipping line 6115: expected 44 fields, saw 46\\nSkipping line 6158: expected 44 fields, saw 46\\nSkipping line 6174: expected 44 fields, saw 45\\nSkipping line 6187: expected 44 fields, saw 45\\nSkipping line 6218: expected 44 fields, saw 45\\nSkipping line 6266: expected 44 fields, saw 45\\nSkipping line 6275: expected 44 fields, saw 45\\nSkipping line 6279: expected 44 fields, saw 45\\nSkipping line 6296: expected 44 fields, saw 45\\nSkipping line 6471: expected 44 fields, saw 46\\nSkipping line 6494: expected 44 fields, saw 45\\nSkipping line 6497: expected 44 fields, saw 45\\nSkipping line 6614: expected 44 fields, saw 46\\nSkipping line 6714: expected 44 fields, saw 45\\nSkipping line 6727: expected 44 fields, saw 45\\nSkipping line 6752: expected 44 fields, saw 45\\nSkipping line 6763: expected 44 fields, saw 45\\nSkipping line 6817: expected 44 fields, saw 45\\nSkipping line 6853: expected 44 fields, saw 45\\nSkipping line 6904: expected 44 fields, saw 45\\nSkipping line 6914: expected 44 fields, saw 45\\nSkipping line 6948: expected 44 fields, saw 45\\nSkipping line 6969: expected 44 fields, saw 45\\nSkipping line 6979: expected 44 fields, saw 45\\nSkipping line 7010: expected 44 fields, saw 47\\nSkipping line 7024: expected 44 fields, saw 45\\nSkipping line 7036: expected 44 fields, saw 45\\nSkipping line 7069: expected 44 fields, saw 45\\nSkipping line 7146: expected 44 fields, saw 45\\nSkipping line 7168: expected 44 fields, saw 45\\nSkipping line 7170: expected 44 fields, saw 45\\nSkipping line 7317: expected 44 fields, saw 45\\nSkipping line 7399: expected 44 fields, saw 45\\nSkipping line 7402: expected 44 fields, saw 45\\nSkipping line 7496: expected 44 fields, saw 45\\nSkipping line 7584: expected 44 fields, saw 45\\nSkipping line 7666: expected 44 fields, saw 45\\nSkipping line 7690: expected 44 fields, saw 45\\nSkipping line 7704: expected 44 fields, saw 47\\nSkipping line 7738: expected 44 fields, saw 45\\nSkipping line 7773: expected 44 fields, saw 45\\nSkipping line 7803: expected 44 fields, saw 45\\nSkipping line 7839: expected 44 fields, saw 45\\nSkipping line 7850: expected 44 fields, saw 45\\nSkipping line 7910: expected 44 fields, saw 45\\nSkipping line 7942: expected 44 fields, saw 45\\nSkipping line 7959: expected 44 fields, saw 45\\nSkipping line 8024: expected 44 fields, saw 45\\nSkipping line 8026: expected 44 fields, saw 45\\nSkipping line 8028: expected 44 fields, saw 45\\nSkipping line 8033: expected 44 fields, saw 45\\nSkipping line 8052: expected 44 fields, saw 45\\nSkipping line 8129: expected 44 fields, saw 45\\nSkipping line 8138: expected 44 fields, saw 45\\nSkipping line 8160: expected 44 fields, saw 46\\nSkipping line 8244: expected 44 fields, saw 45\\nSkipping line 8255: expected 44 fields, saw 45\\nSkipping line 8390: expected 44 fields, saw 45\\nSkipping line 8400: expected 44 fields, saw 45\\nSkipping line 8429: expected 44 fields, saw 45\\nSkipping line 8446: expected 44 fields, saw 46\\nSkipping line 8565: expected 44 fields, saw 46\\nSkipping line 8622: expected 44 fields, saw 46\\nSkipping line 8658: expected 44 fields, saw 45\\nSkipping line 8742: expected 44 fields, saw 45\\nSkipping line 8748: expected 44 fields, saw 45\\nSkipping line 8802: expected 44 fields, saw 45\\nSkipping line 8844: expected 44 fields, saw 45\\nSkipping line 8874: expected 44 fields, saw 45\\nSkipping line 8882: expected 44 fields, saw 45\\nSkipping line 8885: expected 44 fields, saw 48\\nSkipping line 8910: expected 44 fields, saw 45\\nSkipping line 8923: expected 44 fields, saw 45\\nSkipping line 8947: expected 44 fields, saw 45\\nSkipping line 8958: expected 44 fields, saw 45\\nSkipping line 9039: expected 44 fields, saw 46\\nSkipping line 9090: expected 44 fields, saw 45\\nSkipping line 9112: expected 44 fields, saw 45\\nSkipping line 9137: expected 44 fields, saw 45\\nSkipping line 9201: expected 44 fields, saw 45\\nSkipping line 9257: expected 44 fields, saw 45\\nSkipping line 9272: expected 44 fields, saw 45\\nSkipping line 9390: expected 44 fields, saw 45\\nSkipping line 9487: expected 44 fields, saw 45\\nSkipping line 9518: expected 44 fields, saw 45\\nSkipping line 9554: expected 44 fields, saw 45\\nSkipping line 9576: expected 44 fields, saw 45\\nSkipping line 9671: expected 44 fields, saw 45\\nSkipping line 9690: expected 44 fields, saw 45\\nSkipping line 9758: expected 44 fields, saw 45\\nSkipping line 9759: expected 44 fields, saw 45\\nSkipping line 9767: expected 44 fields, saw 45\\nSkipping line 9776: expected 44 fields, saw 45\\nSkipping line 9805: expected 44 fields, saw 45\\nSkipping line 9834: expected 44 fields, saw 45\\nSkipping line 9837: expected 44 fields, saw 45\\nSkipping line 9854: expected 44 fields, saw 45\\nSkipping line 9890: expected 44 fields, saw 45\\nSkipping line 9897: expected 44 fields, saw 45\\nSkipping line 9957: expected 44 fields, saw 45\\nSkipping line 9979: expected 44 fields, saw 45\\nSkipping line 9980: expected 44 fields, saw 45\\nSkipping line 10001: expected 44 fields, saw 45\\nSkipping line 10002: expected 44 fields, saw 45\\nSkipping line 10023: expected 44 fields, saw 45\\nSkipping line 10032: expected 44 fields, saw 45\\nSkipping line 10051: expected 44 fields, saw 45\\nSkipping line 10059: expected 44 fields, saw 46\\nSkipping line 10086: expected 44 fields, saw 45\\nSkipping line 10102: expected 44 fields, saw 45\\nSkipping line 10118: expected 44 fields, saw 45\\nSkipping line 10184: expected 44 fields, saw 45\\nSkipping line 10199: expected 44 fields, saw 45\\nSkipping line 10204: expected 44 fields, saw 45\\nSkipping line 10218: expected 44 fields, saw 45\\nSkipping line 10224: expected 44 fields, saw 45\\nSkipping line 10294: expected 44 fields, saw 45\\nSkipping line 10296: expected 44 fields, saw 45\\nSkipping line 10331: expected 44 fields, saw 45\\nSkipping line 10342: expected 44 fields, saw 45\\nSkipping line 10351: expected 44 fields, saw 45\\nSkipping line 10414: expected 44 fields, saw 45\\nSkipping line 10430: expected 44 fields, saw 45\\nSkipping line 10463: expected 44 fields, saw 45\\nSkipping line 10478: expected 44 fields, saw 46\\nSkipping line 10533: expected 44 fields, saw 45\\nSkipping line 10536: expected 44 fields, saw 45\\nSkipping line 10539: expected 44 fields, saw 45\\nSkipping line 10549: expected 44 fields, saw 45\\nSkipping line 10582: expected 44 fields, saw 45\\nSkipping line 10588: expected 44 fields, saw 45\\nSkipping line 10598: expected 44 fields, saw 45\\nSkipping line 10660: expected 44 fields, saw 45\\nSkipping line 10733: expected 44 fields, saw 45\\nSkipping line 10806: expected 44 fields, saw 45\\nSkipping line 10862: expected 44 fields, saw 45\\nSkipping line 10905: expected 44 fields, saw 45\\nSkipping line 10993: expected 44 fields, saw 45\\nSkipping line 11070: expected 44 fields, saw 45\\nSkipping line 11084: expected 44 fields, saw 45\\nSkipping line 11110: expected 44 fields, saw 45\\nSkipping line 11123: expected 44 fields, saw 45\\nSkipping line 11128: expected 44 fields, saw 45\\nSkipping line 11129: expected 44 fields, saw 45\\nSkipping line 11196: expected 44 fields, saw 45\\nSkipping line 11210: expected 44 fields, saw 45\\nSkipping line 11254: expected 44 fields, saw 45\\nSkipping line 11290: expected 44 fields, saw 45\\nSkipping line 11365: expected 44 fields, saw 45\\nSkipping line 11433: expected 44 fields, saw 45\\nSkipping line 11434: expected 44 fields, saw 45\\nSkipping line 11469: expected 44 fields, saw 45\\nSkipping line 11475: expected 44 fields, saw 45\\nSkipping line 11480: expected 44 fields, saw 45\\nSkipping line 11513: expected 44 fields, saw 45\\nSkipping line 11522: expected 44 fields, saw 45\\nSkipping line 11553: expected 44 fields, saw 45\\nSkipping line 11610: expected 44 fields, saw 45\\nSkipping line 11641: expected 44 fields, saw 45\\nSkipping line 11655: expected 44 fields, saw 46\\nSkipping line 11689: expected 44 fields, saw 45\\nSkipping line 11753: expected 44 fields, saw 45\\nSkipping line 11776: expected 44 fields, saw 45\\nSkipping line 11797: expected 44 fields, saw 45\\nSkipping line 11809: expected 44 fields, saw 45\\nSkipping line 11882: expected 44 fields, saw 45\\nSkipping line 11915: expected 44 fields, saw 45\\nSkipping line 11917: expected 44 fields, saw 46\\nSkipping line 11929: expected 44 fields, saw 45\\nSkipping line 11956: expected 44 fields, saw 45\\nSkipping line 12031: expected 44 fields, saw 45\\nSkipping line 12047: expected 44 fields, saw 46\\nSkipping line 12160: expected 44 fields, saw 45\\nSkipping line 12180: expected 44 fields, saw 45\\nSkipping line 12184: expected 44 fields, saw 45\\nSkipping line 12224: expected 44 fields, saw 45\\nSkipping line 12227: expected 44 fields, saw 45\\nSkipping line 12233: expected 44 fields, saw 45\\nSkipping line 12251: expected 44 fields, saw 45\\nSkipping line 12256: expected 44 fields, saw 45\\nSkipping line 12257: expected 44 fields, saw 45\\nSkipping line 12259: expected 44 fields, saw 45\\nSkipping line 12355: expected 44 fields, saw 45\\nSkipping line 12378: expected 44 fields, saw 45\\nSkipping line 12398: expected 44 fields, saw 45\\nSkipping line 12486: expected 44 fields, saw 45\\nSkipping line 12516: expected 44 fields, saw 45\\nSkipping line 12588: expected 44 fields, saw 45\\nSkipping line 12595: expected 44 fields, saw 45\\nSkipping line 12614: expected 44 fields, saw 45\\nSkipping line 12642: expected 44 fields, saw 45\\nSkipping line 12701: expected 44 fields, saw 45\\nSkipping line 12741: expected 44 fields, saw 46\\nSkipping line 12771: expected 44 fields, saw 45\\nSkipping line 12777: expected 44 fields, saw 45\\nSkipping line 12802: expected 44 fields, saw 45\\nSkipping line 12892: expected 44 fields, saw 45\\nSkipping line 12910: expected 44 fields, saw 47\\nSkipping line 12982: expected 44 fields, saw 46\\nSkipping line 13024: expected 44 fields, saw 45\\nSkipping line 13052: expected 44 fields, saw 45\\nSkipping line 13056: expected 44 fields, saw 45\\nSkipping line 13158: expected 44 fields, saw 45\\nSkipping line 13170: expected 44 fields, saw 45\\nSkipping line 13171: expected 44 fields, saw 45\\nSkipping line 13186: expected 44 fields, saw 45\\nSkipping line 13240: expected 44 fields, saw 45\\nSkipping line 13262: expected 44 fields, saw 45\\nSkipping line 13374: expected 44 fields, saw 45\\nSkipping line 13407: expected 44 fields, saw 45\\nSkipping line 13477: expected 44 fields, saw 45\\nSkipping line 13540: expected 44 fields, saw 46\\nSkipping line 13569: expected 44 fields, saw 45\\nSkipping line 13617: expected 44 fields, saw 46\\nSkipping line 13651: expected 44 fields, saw 45\\nSkipping line 13663: expected 44 fields, saw 45\\nSkipping line 13754: expected 44 fields, saw 46\\nSkipping line 13775: expected 44 fields, saw 45\\nSkipping line 13804: expected 44 fields, saw 45\\nSkipping line 13828: expected 44 fields, saw 45\\nSkipping line 13865: expected 44 fields, saw 46\\nSkipping line 13883: expected 44 fields, saw 45\\nSkipping line 13929: expected 44 fields, saw 45\\nSkipping line 13948: expected 44 fields, saw 45\\nSkipping line 14022: expected 44 fields, saw 45\\nSkipping line 14127: expected 44 fields, saw 45\\nSkipping line 14148: expected 44 fields, saw 45\\nSkipping line 14173: expected 44 fields, saw 45\\nSkipping line 14243: expected 44 fields, saw 45\\nSkipping line 14254: expected 44 fields, saw 45\\nSkipping line 14318: expected 44 fields, saw 45\\nSkipping line 14320: expected 44 fields, saw 45\\nSkipping line 14354: expected 44 fields, saw 45\\nSkipping line 14434: expected 44 fields, saw 45\\nSkipping line 14456: expected 44 fields, saw 45\\nSkipping line 14479: expected 44 fields, saw 45\\nSkipping line 14565: expected 44 fields, saw 45\\nSkipping line 14572: expected 44 fields, saw 45\\nSkipping line 14616: expected 44 fields, saw 45\\nSkipping line 14636: expected 44 fields, saw 45\\nSkipping line 14642: expected 44 fields, saw 47\\nSkipping line 14644: expected 44 fields, saw 45\\nSkipping line 14649: expected 44 fields, saw 45\\nSkipping line 14670: expected 44 fields, saw 45\\nSkipping line 14688: expected 44 fields, saw 45\\n'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_two = pd.read_csv('second_dataset.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fn</th>\n",
       "      <th>tid</th>\n",
       "      <th>title</th>\n",
       "      <th>wordsInTitle</th>\n",
       "      <th>url</th>\n",
       "      <th>imdbRating</th>\n",
       "      <th>ratingCount</th>\n",
       "      <th>duration</th>\n",
       "      <th>year</th>\n",
       "      <th>type</th>\n",
       "      <th>...</th>\n",
       "      <th>News</th>\n",
       "      <th>RealityTV</th>\n",
       "      <th>Romance</th>\n",
       "      <th>SciFi</th>\n",
       "      <th>Short</th>\n",
       "      <th>Sport</th>\n",
       "      <th>TalkShow</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>titles01/tt0012349</td>\n",
       "      <td>tt0012349</td>\n",
       "      <td>Der Vagabund und das Kind (1921)</td>\n",
       "      <td>der vagabund und das kind</td>\n",
       "      <td>http://www.imdb.com/title/tt0012349/</td>\n",
       "      <td>8.4</td>\n",
       "      <td>40550.0</td>\n",
       "      <td>3240.0</td>\n",
       "      <td>1921.0</td>\n",
       "      <td>video.movie</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>titles01/tt0015864</td>\n",
       "      <td>tt0015864</td>\n",
       "      <td>Goldrausch (1925)</td>\n",
       "      <td>goldrausch</td>\n",
       "      <td>http://www.imdb.com/title/tt0015864/</td>\n",
       "      <td>8.3</td>\n",
       "      <td>45319.0</td>\n",
       "      <td>5700.0</td>\n",
       "      <td>1925.0</td>\n",
       "      <td>video.movie</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>titles01/tt0017136</td>\n",
       "      <td>tt0017136</td>\n",
       "      <td>Metropolis (1927)</td>\n",
       "      <td>metropolis</td>\n",
       "      <td>http://www.imdb.com/title/tt0017136/</td>\n",
       "      <td>8.4</td>\n",
       "      <td>81007.0</td>\n",
       "      <td>9180.0</td>\n",
       "      <td>1927.0</td>\n",
       "      <td>video.movie</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>titles01/tt0017925</td>\n",
       "      <td>tt0017925</td>\n",
       "      <td>Der General (1926)</td>\n",
       "      <td>der general</td>\n",
       "      <td>http://www.imdb.com/title/tt0017925/</td>\n",
       "      <td>8.3</td>\n",
       "      <td>37521.0</td>\n",
       "      <td>6420.0</td>\n",
       "      <td>1926.0</td>\n",
       "      <td>video.movie</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>titles01/tt0021749</td>\n",
       "      <td>tt0021749</td>\n",
       "      <td>Lichter der Großstadt (1931)</td>\n",
       "      <td>lichter der gro stadt</td>\n",
       "      <td>http://www.imdb.com/title/tt0021749/</td>\n",
       "      <td>8.7</td>\n",
       "      <td>70057.0</td>\n",
       "      <td>5220.0</td>\n",
       "      <td>1931.0</td>\n",
       "      <td>video.movie</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   fn        tid                             title  \\\n",
       "0  titles01/tt0012349  tt0012349  Der Vagabund und das Kind (1921)   \n",
       "1  titles01/tt0015864  tt0015864                 Goldrausch (1925)   \n",
       "2  titles01/tt0017136  tt0017136                 Metropolis (1927)   \n",
       "3  titles01/tt0017925  tt0017925                Der General (1926)   \n",
       "4  titles01/tt0021749  tt0021749      Lichter der Großstadt (1931)   \n",
       "\n",
       "                wordsInTitle                                   url  \\\n",
       "0  der vagabund und das kind  http://www.imdb.com/title/tt0012349/   \n",
       "1                 goldrausch  http://www.imdb.com/title/tt0015864/   \n",
       "2                 metropolis  http://www.imdb.com/title/tt0017136/   \n",
       "3                der general  http://www.imdb.com/title/tt0017925/   \n",
       "4      lichter der gro stadt  http://www.imdb.com/title/tt0021749/   \n",
       "\n",
       "   imdbRating  ratingCount  duration    year         type  ...  News  \\\n",
       "0         8.4      40550.0    3240.0  1921.0  video.movie  ...     0   \n",
       "1         8.3      45319.0    5700.0  1925.0  video.movie  ...     0   \n",
       "2         8.4      81007.0    9180.0  1927.0  video.movie  ...     0   \n",
       "3         8.3      37521.0    6420.0  1926.0  video.movie  ...     0   \n",
       "4         8.7      70057.0    5220.0  1931.0  video.movie  ...     0   \n",
       "\n",
       "   RealityTV  Romance  SciFi  Short  Sport  TalkShow  Thriller  War  Western  \n",
       "0          0        0      0      0      0         0         0    0        0  \n",
       "1          0        0      0      0      0         0         0    0        0  \n",
       "2          0        0      1      0      0         0         0    0        0  \n",
       "3          0        0      0      0      0         0         0    0        0  \n",
       "4          0        1      0      0      0         0         0    0        0  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_two.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract and append the links of the second dataset to the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The first five rows of the new links:\n",
      "0    http://www.imdb.com/title/tt0012349/\n",
      "1    http://www.imdb.com/title/tt0015864/\n",
      "2    http://www.imdb.com/title/tt0017136/\n",
      "3    http://www.imdb.com/title/tt0017925/\n",
      "4    http://www.imdb.com/title/tt0021749/\n",
      "Name: url, dtype: object\n",
      "\n",
      "The length of the url column of the second dataset: 14332\n",
      "\n",
      "Thus I have 14332 rows to manilpulate and add additional content in my initial dataset!\n",
      "\n",
      "However, it is important before using all the 14332 links to understand what my data is about!\n",
      "\n",
      "Contrary to the second dataset, the first dataset I already made use of, has 4774 active imdb links\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Having imported the two datasets it is now important to check if the two datasets has duplicate links, \n",
      "since I don't want to extract the same movie second time.\n",
      "\n",
      "The number of total movies to manipulate is: 19106\n",
      "\n",
      "The number of duplicate urls in the two dataset is: 0\n",
      "\n",
      "No duplicates have been spotted!\n",
      "\n",
      "Even though no duplicate links have been spotted, the same movie but in different language may exist. \n",
      "As we will see later this is a real case!\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Cleaning the TV series of any type from the dataset\n",
      "\n",
      "Havning checked for duplicate rows now I will move on removing the rows that are not Movies, but TV episodes/series/\n",
      "short movies/gaming videos, etc.\n",
      "\n",
      "1) The first indication of a TV episode is given in the column 'RealityTV' where the value 1 indicates a reality tv show, \n",
      "whereas the value 0 indicates anything else but reality show.\n",
      "\n",
      "The shape of the second dataset that has reality series is: (125, 44)\n",
      "\n",
      "Those 125 rows should be deleted!\n",
      "\n",
      "The shape of the second dataset, with reality series removed, is: (14207, 44)\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "2) Having deleted the rows that had the value 1 in the 'RealityTV' column, now I will delete the rows that in the column\n",
      "'title' have the word 'Episode'.\n",
      "\n",
      "The number of rows that contain the word 'Episode' in their title is: 1879\n",
      "\n",
      "Some of those 1879 rows are: ['Whose Line Is It Anyway? Episode #2.7 (TV Episode 1989)', 'Smashing UK Top 10 Top 10 International Films (TV Episode 2013)', 'Siskel & Ebert & the Movies Men in Black/Wild America/Out to Sea (TV Episode 1997)', 'Parks and Recreation Eagleton (TV Episode 2011)', 'Mission Hill Plan 9 from Mission Hill (or I Married a Gay Man from Outer Space) (TV Episode 2002)']\n",
      "\n",
      "The shape of the new dataset is (12328, 44), 1879 rows less\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "3) Having deleted the 'Episodes' from the dataset now I should remove the 'TV-Series'.\n",
      "\n",
      "The number of rows that contain the word 'TV-Series' in their title is: 1280\n",
      "\n",
      "Some of those 1280 rows are: ['Der Kopfgeldjäger (TV Series 1958–1961)', 'Dragnet (TV Series 1951–1959)', 'Raumschiff Enterprise (TV Series 1966–1969)', 'Kaun Banega Crorepati? (TV Series 2000– )', 'Continuum (TV Series 2012– )']\n",
      "\n",
      "The shape of the new dataset is (11049, 44), 1280 rows less\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "4) Having deleted the 'TV-Series' from the dataset, now I should remove the rows containing the word 'Video Game'.\n",
      "\n",
      "The number of rows that contain the word 'Video Game' in their title is: 123\n",
      "\n",
      "Some of those 123 rows are: ['Destroy All Humans! (Video Game 2005)', 'Aladdin (Video Game 1993)', 'Doctor Who: The Adventure Games - Blood of the Cybermen (Video Game 2010)', 'The Secret of Monkey Island (Video Game 1990)', \"Assassin's Creed III (Video Game 2012)\"]\n",
      "\n",
      "The shape of the new dataset is (10932, 44), 123 rows less\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Having cleaned the movie titles of my dataset, it is now time to proceed with checking if any of the title in the second dataset already exist in the first dataset.\n",
      "\n",
      "The number of movie titles in the first dataset is: 4774\n",
      "\n",
      "The number of movie titles in the second dataset is: 10932\n",
      "\n",
      "The number of duplicate movie titles is: 6\n",
      "\n",
      "The duplicate movie titles are: \n",
      "Pete's Dragon                  2\n",
      "Deadpool                       2\n",
      "Welcome to the Sticks          2\n",
      "The Host                       2\n",
      "The Hateful Eight              2\n",
      "Out of the Blue                2\n",
      "Insidious: Chapter 2 (2013)    1\n",
      "dtype: int64\n",
      "\n",
      "The movies 'The Host' and 'Out of the blue', are two movies with the same title altough different content. \n",
      "Thus they won't be deleted.\n",
      "\n",
      "The dataset finally has 9147 rows. It is time now to extract the information I need from the data I have.\n",
      "\n",
      "Just to pinpoint that I have spotted two rows in the dataset, of which their url yielded a 404 error. Those rows correspond to the index 973 and 2951. Thus, I remove them from the final dataset beforehand.\n",
      "\n",
      "The shape of the final dataset is: (9145, 44)\n"
     ]
    }
   ],
   "source": [
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = \"\\033[1m\"\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = \"\\033[0m\"\n",
    "\n",
    "second_links = dataset_two.url\n",
    "\n",
    "print(\"\\nThe first five rows of the new links:\\n{}\".format(second_links.head(5)))\n",
    "\n",
    "print(\"\\nThe length of the url column of the second dataset: {}\".format(len(second_links)))\n",
    "\n",
    "print(\"\\nThus I have 14332 rows to manilpulate and add additional content in my initial dataset!\")\n",
    "print(\"\\nHowever, it is important before using all the {} links to understand what my data is about!\".format(len(second_links)))\n",
    "\n",
    "first_links = dataset.movie_imdb_link\n",
    "print(\"\\nContrary to the second dataset, the first dataset I already made use of, has {} active imdb links\".format(len(first_links)))\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(\"\\nHaving imported the two datasets it is now important to check if the two datasets has duplicate links, \\nsince I don't want to extract the same movie second time.\")\n",
    "\n",
    "my_series = [first_links, second_links]\n",
    "\n",
    "my_total_links = pd.concat(my_series)\n",
    "\n",
    "print(\"\\nThe number of total movies to manipulate is: {}\".format(len(my_total_links)))\n",
    "\n",
    "duplicates = my_total_links.value_counts().tolist()\n",
    "\n",
    "empty_l = []\n",
    "for i in duplicates:\n",
    "    if i > 1:\n",
    "        empty_l.append(i)\n",
    "        \n",
    "print(\"\\nThe number of duplicate urls in the two dataset is: {}\".format(len(empty_l)))\n",
    "\n",
    "if len(empty_l) > 0:\n",
    "    print(\"\\nDuplicates have been spotted!\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nNo duplicates have been spotted!\")\n",
    "    print(\"\\nEven though no duplicate links have been spotted, the same movie but in different language may exist. \\nAs we will see later this is a real case!\")\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(color.BOLD + \"\\nCleaning the TV series of any type from the dataset\" + color.END)\n",
    "print(\"\\nHavning checked for duplicate rows now I will move on removing the rows that are not Movies, but TV episodes/series/\\nshort movies/gaming videos, etc.\")\n",
    "\n",
    "print(\"\\n1) The first indication of a TV episode is given in the column 'RealityTV' where the value 1 indicates a reality tv show, \\nwhereas the value 0 indicates anything else but reality show.\")\n",
    "\n",
    "dataset_two[dataset_two.RealityTV==1].head(5)\n",
    "\n",
    "print(\"\\nThe shape of the second dataset that has reality series is: {}\".format(dataset_two[dataset_two.RealityTV==1].shape))\n",
    "print(\"\\nThose 125 rows should be deleted!\")\n",
    "\n",
    "dataset_two = dataset_two[dataset_two.RealityTV != 1]\n",
    "\n",
    "print(\"\\nThe shape of the second dataset, with reality series removed, is: {}\".format(dataset_two.shape))\n",
    "\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(\"\\n2) Having deleted the rows that had the value 1 in the 'RealityTV' column, now I will delete the rows that in the column\\n'title' have the word 'Episode'.\")\n",
    "\n",
    "empty_episodes = []\n",
    "\n",
    "for i in dataset_two['title']:\n",
    "    if 'Episode' in i: \n",
    "        empty_episodes.append(i)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(\"\\nThe number of rows that contain the word 'Episode' in their title is: {}\".format(len(empty_episodes)))\n",
    "print(\"\\nSome of those {} rows are: {}\".format(len(empty_episodes), empty_episodes[0:5]))\n",
    "\n",
    "dataset_two_cleaned = dataset_two[~dataset_two.title.isin(empty_episodes)]\n",
    "\n",
    "print(\"\\nThe shape of the new dataset is {}, {} rows less\".format(dataset_two_cleaned.shape, len(empty_episodes)))\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(\"\\n3) Having deleted the 'Episodes' from the dataset now I should remove the 'TV-Series'.\")\n",
    "\n",
    "empty_tvseries = []\n",
    "\n",
    "for i in dataset_two['title']:\n",
    "    if 'TV Series' in i: \n",
    "        empty_tvseries.append(i)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(\"\\nThe number of rows that contain the word 'TV-Series' in their title is: {}\".format(len(empty_tvseries)))\n",
    "print(\"\\nSome of those {} rows are: {}\".format(len(empty_tvseries), empty_tvseries[0:5]))\n",
    "\n",
    "dataset_two_cleaned = dataset_two_cleaned[~dataset_two_cleaned.title.isin(empty_tvseries)]\n",
    "print(\"\\nThe shape of the new dataset is {}, {} rows less\".format(dataset_two_cleaned.shape, len(empty_tvseries)))\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(\"\\n4) Having deleted the 'TV-Series' from the dataset, now I should remove the rows containing the word 'Video Game'.\")\n",
    "\n",
    "empty_videogame = []\n",
    "\n",
    "for i in dataset_two['title']:\n",
    "    if 'Video Game' in i: \n",
    "        empty_videogame.append(i)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(\"\\nThe number of rows that contain the word 'Video Game' in their title is: {}\".format(len(empty_videogame)))\n",
    "print(\"\\nSome of those {} rows are: {}\".format(len(empty_videogame), empty_videogame[0:5]))\n",
    "\n",
    "dataset_two_cleaned = dataset_two_cleaned[~dataset_two_cleaned.title.isin(empty_videogame)]\n",
    "print(\"\\nThe shape of the new dataset is {}, {} rows less\".format(dataset_two_cleaned.shape, len(empty_videogame)))\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(\"\\nHaving cleaned the movie titles of my dataset, it is now time to proceed with checking if any of the title in the second dataset already exist in the first dataset.\")\n",
    "\n",
    "first_titles = dataset.movie_title\n",
    "print(\"\\nThe number of movie titles in the first dataset is: {}\".format(len(first_titles)))\n",
    "\n",
    "second_titles = dataset_two_cleaned.title\n",
    "print(\"\\nThe number of movie titles in the second dataset is: {}\".format(len(second_titles)))\n",
    "\n",
    "my_series = [first_titles, second_titles]\n",
    "\n",
    "my_total_titles = pd.concat(my_series)\n",
    "\n",
    "duplicate_movie_titles = my_total_titles.value_counts().tolist()\n",
    "\n",
    "empty_duplicate_movie_titles = []\n",
    "for i in duplicate_movie_titles:\n",
    "    if i > 1:\n",
    "        empty_duplicate_movie_titles.append(i)\n",
    "        \n",
    "print(\"\\nThe number of duplicate movie titles is: {}\".format(len(empty_duplicate_movie_titles)))\n",
    "print(\"\\nThe duplicate movie titles are: \\n{}\".format(my_total_titles.value_counts()[0:7]))\n",
    "\n",
    "print(\"\\nThe movies 'The Host' and 'Out of the blue', are two movies with the same title altough different content. \\nThus they won't be deleted.\")\n",
    "\n",
    "dataset_two_cleaned = dataset_two_cleaned[dataset_two_cleaned.year > 1960]\n",
    "\n",
    "dataset_two_cleaned.shape\n",
    "\n",
    "print(\"\\nThe dataset finally has {} rows. It is time now to extract the information I need from the data I have.\".format(len(dataset_two_cleaned)))\n",
    "print(\"\\nJust to pinpoint that I have spotted two rows in the dataset, of which their url yielded a 404 error. Those rows correspond to the index 973 and 2951. Thus, I remove them from the final dataset beforehand.\")\n",
    "\n",
    "dataset_two_cleaned = dataset_two_cleaned.drop([973, 2951])\n",
    "\n",
    "print(\"\\nThe shape of the final dataset is: {}\".format(dataset_two_cleaned.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pickle(\"dataset_one_07112019.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_two_cleaned.to_pickle(\"dataset_two_07112019.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - - -  - - - - - - - - - - - - - - - - - -  - - - - - - - - - - - - - - - - - -  - - - - - - - - - - - - - - - - - -  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FIrst 5000 movies (14.10.2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1 - 14.10.2019\n",
    "Extract the first five thousands movies to request their HTML doc.\n",
    "\n",
    "*Note: I didn't requested all the 9145 urls at once, since it would result to memory error.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The **code** below will take approximately 1 hour to finish.\n",
    "\n",
    "# mock_dataset_one = dataset_two_cleaned.url.iloc[0:5000]\n",
    "\n",
    "# mylist_one = []\n",
    "\n",
    "# for i in tqdm(mock_dataset_one):\n",
    "#     mylist_one.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the requests file for further use!\n",
    "\n",
    "# Save the file\n",
    "\n",
    "# with open('requests_one_second_try_13102019.pkl', 'wb') as f:\n",
    "#     pickle.dump(mylist_one, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('requests_one_second_try_13102019.pkl', 'rb') as f:\n",
    "    mylist_one = pickle.load(f)\n",
    "    \n",
    "len(mylist_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2 - 14.10.2019\n",
    "\n",
    "##### Remove falsy indices (comment on 18.10.2019)\n",
    "\n",
    "Delete the movies of the below indexes!\n",
    "\n",
    "* 3 movies (between 1960-2017) with no plot summary!\n",
    "* 77 movies (between 1960-2017) they don't have a recorded IMDB Rating!\n",
    "* 22 movies (between 1960-2017) they don't have actors!\n",
    "* 185 movies (between 1960-2017) they dont' have a genre!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no plot_summary\n",
    "\n",
    "remove_indices = [761, 2511, 4326]\n",
    "\n",
    "mylist_one = [i for j, i in enumerate(mylist_one) if j not in remove_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no IMDB Rating\n",
    "\n",
    "remove_indices = [276,  1057, 1216, 1378, 3219, 3331, 3384, 3521, 3676, 3726, 3743, 3758, 3771, 3802, 3927, 3953, 3958, 3970, \n",
    "                  3980, 3981, 4010, 4022, 4042, 4060, 4070, 4091, 4094, 4101, 4114, 4115, 4121, 4128, 4141, 4152, 4154, 4186, \n",
    "                  4194, 4235, 4243, 4245, 4248, 4274, 4277, 4282, 4285, 4291, 4299, 4300, 4304, 4305, 4306, 4309, 4311, 4313, \n",
    "                  4315, 4318, 4322, 4324, 4329, 4330, 4340, 4426, 4630, 4665, 4676, 4683, 4696, 4700, 4731, 4738, 4755, 4778, \n",
    "                  4785, 4806, 4811, 4829, 4958]\n",
    "\n",
    "mylist_one = [i for j, i in enumerate(mylist_one) if j not in remove_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no actors\n",
    "\n",
    "remove_indices = [202, 454, 800, 1213, 1301,1911, 2567, 2568, 2569, 2570, 3017, 3140, 3457, 3760, 4014, 4092, 4333, 4410, \n",
    "                  4452, 4455, 4552, 4708]\n",
    "\n",
    "mylist_one = [i for j, i in enumerate(mylist_one) if j not in remove_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no genres\n",
    "\n",
    "remove_indices = [273,  419,  712,   882,  883,  955,  1012, 1372, 1377, 1676, 1920, 2108, 2219, 2467, 2517, 2642, 2646, 2696, \n",
    "                  2719, 2764, 2767, 2769,  2793, 2797, 2826, 2887, 2904, 2936, 3001, 3022, 3024, 3027, 3079, 3116, 3133, 3134, \n",
    "                  3140, 3158, 3162, 3176, 3193, 3208, 3210,  3217, 3221, 3231, 3242, 3245, 3249, 3267, 3278, 3288, 3291, 3292, \n",
    "                  3300, 3319, 3327, 3330, 3342, 3359, 3411, 3417, 3426, 3432,  3434, 3445, 3453, 3457, 3468, 3474, 3498, 3502, \n",
    "                  3508, 3516, 3533, 3551, 3562, 3578, 3605, 3606, 3616, 3621, 3625, 3635, 3637,  3659, 3665, 3668, 3686, 3692, \n",
    "                  3698, 3701, 3723, 3770, 3775, 3778, 3796, 3797, 3798, 3800, 3811, 3812, 3823, 3896, 3915, 3931,  3932, 3944, \n",
    "                  4001, 4005, 4006, 4008, 4028, 4033, 4037, 4049, 4062, 4075, 4085, 4086, 4102, 4106, 4107, 4124,  4140, 4142, \n",
    "                  4158, 4161, 4164, 4166, 4170, 4177, 4179, 4184, 4201, 4202, 4207, 4208, 4212, 4213, 4229, 4232, 4233,  4235, \n",
    "                  4238, 4241, 4246, 4247, 4252, 4255, 4257, 4260, 4261, 4262, 4263, 4265, 4334, 4336, 4345, 4392, 4417,  4457, \n",
    "                  4500, 4540, 4551, 4557, 4575, 4604, 4612, 4626, 4627, 4643, 4655, 4684, 4705, 4717, 4763, 4776, 4779,  4791, \n",
    "                  4846, 4855, 4864, 4866, 4897]\n",
    "\n",
    "mylist_one = [i for j, i in enumerate(mylist_one) if j not in remove_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4713"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mylist_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3 - Read the souplist at once before start extracting data from the HTML docs!! (14.10.2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract the souplist which is the same for every column extracted\n",
    "\n",
    "souplist = []\n",
    "\n",
    "for i in tqdm(mylist_one):\n",
    "    souplist.append(BeautifulSoup(i.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4 - Extract the infomration for all the columns needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract column 1: Plot Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myfield = []\n",
    "plot_summary = []\n",
    "\n",
    "for i in tqdm(souplist):\n",
    "    myfield.append(i.find_all('div', {'class':'plot_summary'}))\n",
    "\n",
    "for i in tqdm(myfield):\n",
    "    for x in tqdm(i):\n",
    "        for y in tqdm(x.find_all('div', {'class':'summary_text'})):\n",
    "            plot_summary.append(y.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('plot_summary_one_18102019.pkl', 'wb') as f:\n",
    "    pickle.dump(plot_summary, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract column 2: IMDB Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myfield_rating = []\n",
    "ratings = []\n",
    "\n",
    "for i in tqdm(souplist):\n",
    "    myfield_rating.append(i.find_all('div', {'class':'ratingValue'}))\n",
    "\n",
    "for i in tqdm(myfield_rating):\n",
    "    for x in tqdm(i):\n",
    "        for y in tqdm(x.find_all('span', {'itemprop':'ratingValue'})):\n",
    "            ratings.append(y.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ratings_one_18102019.pkl', 'wb') as f:\n",
    "    pickle.dump(ratings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract column 3: Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1\n",
    "\n",
    "myfield_cast = []\n",
    "\n",
    "for i in tqdm(souplist):\n",
    "    myfield_cast.append(i.find_all('table', {'class':'cast_list'}))\n",
    "    \n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "# Phase 2\n",
    "\n",
    "phase_two = []\n",
    "\n",
    "import re\n",
    "r_one = re.compile(\".*name\")\n",
    "\n",
    "cast_list = myfield_cast\n",
    "\n",
    "for i in tqdm(cast_list):\n",
    "    for j in tqdm(i):\n",
    "        phase_two.append(j.find_all('a', {'href':r_one}))\n",
    "\n",
    "# # ------------------------------------------------------------------------\n",
    "\n",
    "# # Phase 3\n",
    "\n",
    "phase_three = []\n",
    "\n",
    "for i in tqdm(range(len(phase_two))):\n",
    "    if len(phase_two[i]) != 0:\n",
    "        phase_three.append(phase_two[i][1::2])\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "# Phase 4\n",
    "\n",
    "actor_list = []\n",
    "for tags in phase_three:\n",
    "    actor_list.append(list(map(lambda x: x.text.strip(' ').replace('\\n', ''), tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('actors_list_one_18102019.pkl', 'wb') as f:\n",
    "    pickle.dump(actor_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract column 4: Director Name\n",
    "\n",
    "Note: find the class director and locate i + 1 which is the next element in the list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myfield_director = []\n",
    "director_name = []\n",
    "\n",
    "for i in tqdm(souplist):\n",
    "    myfield_director.append(i.find_all('div', {'class':'plot_summary'}))\n",
    "\n",
    "import re\n",
    "r_one = re.compile(\".*name\")\n",
    "\n",
    "for i in tqdm(myfield_director):\n",
    "    for j in tqdm(i):\n",
    "        director_name.append(j.find_all('a', {'href':r_one}))\n",
    "        \n",
    "director_names = [item[0].text for item in director_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('director_names_one_18102019.pkl', 'wb') as f:\n",
    "    pickle.dump(director_names, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract column 5: Plot Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myfield_keywords = []\n",
    "keywords = []\n",
    "\n",
    "for i in tqdm(souplist):\n",
    "    myfield_keywords.append(i.find_all('div', {'class':'see-more inline canwrap'}))\n",
    "\n",
    "myfield_keywords_final = [[item[0]] for item in myfield_keywords]\n",
    "\n",
    "for i in tqdm(myfield_keywords_final):\n",
    "    for j in tqdm(i):\n",
    "        keywords.append(j.find_all('span', {'class':\"itemprop\"}))\n",
    "\n",
    "keywords_final = []\n",
    "for i in keywords:\n",
    "    keywords_final.append(list(map(lambda x: x.text.strip(' ').replace('\\n', ''), i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('keywords_final_one_18102019.pkl', 'wb') as f:\n",
    "    pickle.dump(keywords_final, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract column 6: Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myfield_genres = []\n",
    "myfield_genres_final = []\n",
    "genres = []\n",
    "\n",
    "for i in tqdm(souplist):\n",
    "    myfield_genres.append(i.find_all('div', {'class':'see-more inline canwrap'}))\n",
    "\n",
    "myfield_genres_final = [[item[1]] for item in myfield_genres]\n",
    "\n",
    "import re\n",
    "r_genres = re.compile(\"(?=genres)(.*)\")\n",
    "\n",
    "for i in tqdm(myfield_genres_final):\n",
    "    for j in tqdm(i):\n",
    "        genres.append(j.find_all('a', {'href':r_genres}))\n",
    "\n",
    "genres_final = []\n",
    "for i in genres:\n",
    "    genres_final.append(list(map(lambda x: x.text.strip(' ').replace('\\n', ''), i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('genres_final_one_18102019.pkl', 'wb') as f:\n",
    "    pickle.dump(genres_final, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second 5000 movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('requests_two_second_try_13102019.pkl', 'rb') as f:\n",
    "    mylist_two = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4145"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mylist_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Clean the incorrect indices\n",
    "\n",
    "(incorrect are those indices that don't contain inforamtion for at most one of the columns needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the incorrect indices\n",
    "\n",
    "remove_indices_no_plot = [3397]\n",
    "\n",
    "remove_indices_no_rating =  [79,  94,  137, 212, 226, 243, 261, 273, 297, 298, 332, 358, 365, 379, 382, 391, 398, 404, 409, 481, \n",
    "                             515, 518, 520, 532, 545, 561, 565, 582, 605, 608, 614, 624,626, 632, 637, 642, 644, 646, 647, \n",
    "                             648, 649, 652, 656, 658, 2353, 2805, 2911, 2968, 2990, 3117, 3187, 3259, 3424, 3439, 3618, 3771, \n",
    "                             3858, 3866, 3907, 4129]\n",
    "\n",
    "remove_indices_no_actors = [231, 498, 511, 563, 1857, 2005, 2030, 2345, 2690, 2862, 2891, 3112, 3214, 3534, 3641, 3766, 3901, \n",
    "                            3918, 4070]\n",
    "\n",
    "remove_indices_no_keywords = [23, 2899, 3512]\n",
    "\n",
    "remove_indices_total = remove_indices_no_plot + remove_indices_no_rating + remove_indices_no_actors + remove_indices_no_keywords\n",
    "\n",
    "mylist_two = [i for j, i in enumerate(mylist_two) if j not in remove_indices_total]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_indices_second_round = [63, 67, 103, 104, 125, 148, 173, 185, 187, 188, 195, 199, 217, 222, 224, 225, 238, 240, 245, 251, \n",
    "                           270, 271, 308, 310, 338, 340, 345, 347, 352, 354, 379, 384, 390, 393, 399, 418, 438, 441, 445, 449, \n",
    "                           450, 454, 464, 468, 469, 477, 478, 484, 493, 501, 520, 532, 537, 539, 544, 552, 554, 560, 576, 587, \n",
    "                           588, 595, 607, 608, 611, 838, 980, 1148, 1151, 1188, 1400, 1433, 1441, 1526, 1826, 1834, 1980, 2061, \n",
    "                           2095, 2121, 2142, 2496, 2504, 2577, 2659, 2757, 2801, 2921, 2934, 3012, 3023, 3049, 3063,  3067, \n",
    "                           3093, 3098, 3105, 3109, 3138, 3155, 3172, 3189, 3204, 3221, 3226, 3228, 3260, 3263, 3267, 3269, 3276, \n",
    "                           3285, 3305, 3317, 3325,3326, 3327, 3329, 3353, 3359, 3387, 3421, 3454, 3478, 3495, 3500, 3511,3543, \n",
    "                           3551, 3571, 3576, 3584, 3631, 3658, 3665, 3668, 3674, 3690, 3699, 3716, 3719, 3727, 3742, 3749, 3751,\n",
    "                           3770, 3793, 3800, 3809, 3817, 3822, 3826, 3832, 3837, 3853, 3856,3874, 3882, 3883, 3906, 3931, 3941, \n",
    "                           3947, 3974, 3978, 3979, 3987, 4002, 4018, 4021, 4039, 4041, 4050]\n",
    "\n",
    "mylist_two = [i for j, i in enumerate(mylist_two) if j not in remove_indices_second_round]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_indices_third_round = [346, 441, 503, 505, 507, 510, 514, 2845, 2989, 3104, 3302, 3701]\n",
    "\n",
    "mylist_two = [i for j, i in enumerate(mylist_two) if j not in remove_indices_third_round]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mylist_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Souplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "souplist_two = []\n",
    "\n",
    "for i in tqdm(mylist_two):\n",
    "    souplist_two.append(BeautifulSoup(i.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(souplist_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract columns 1: Plot Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfield = []\n",
    "plot_summary = []\n",
    "\n",
    "for i in tqdm(souplist_two):\n",
    "    myfield.append(i.find_all('div', {'class':'plot_summary'}))\n",
    "\n",
    "for i in tqdm(myfield):\n",
    "    for x in tqdm(i):\n",
    "        for y in tqdm(x.find_all('div', {'class':'summary_text'})):\n",
    "            plot_summary.append(y.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('plot_summary_final_two_19102019.pkl', 'wb') as f:\n",
    "    pickle.dump(plot_summary, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract column 2: IMDB Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfield_rating = []\n",
    "ratings = []\n",
    "\n",
    "for i in tqdm(souplist_two):\n",
    "    myfield_rating.append(i.find_all('div', {'class':'ratingValue'}))\n",
    "\n",
    "for i in tqdm(myfield_rating):\n",
    "    for x in tqdm(i):\n",
    "        for y in tqdm(x.find_all('span', {'itemprop':'ratingValue'})):\n",
    "            ratings.append(y.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rating_final_two_19102019.pkl', 'wb') as f:\n",
    "    pickle.dump(ratings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract column 3: Actors List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1\n",
    "\n",
    "myfield_cast = []\n",
    "\n",
    "for i in tqdm(souplist_two):\n",
    "    myfield_cast.append(i.find_all('table', {'class':'cast_list'}))\n",
    "    \n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "# Phase 2\n",
    "\n",
    "phase_two = []\n",
    "\n",
    "import re\n",
    "r_one = re.compile(\".*name\")\n",
    "\n",
    "cast_list = myfield_cast\n",
    "\n",
    "for i in tqdm(cast_list):\n",
    "    for j in tqdm(i):\n",
    "        phase_two.append(j.find_all('a', {'href':r_one}))\n",
    "\n",
    "# # ------------------------------------------------------------------------\n",
    "\n",
    "# # Phase 3\n",
    "\n",
    "phase_three = []\n",
    "\n",
    "for i in tqdm(range(len(phase_two))):\n",
    "    if len(phase_two[i]) != 0:\n",
    "        phase_three.append(phase_two[i][1::2])\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "# Phase 4\n",
    "\n",
    "actor_list = []\n",
    "for tags in phase_three:\n",
    "    actor_list.append(list(map(lambda x: x.text.strip(' ').replace('\\n', ''), tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('actors_list_final_two_19102019.pkl', 'wb') as f:\n",
    "    pickle.dump(actor_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract column 4: Director Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfield_director = []\n",
    "director_name = []\n",
    "\n",
    "for i in tqdm(souplist_two):\n",
    "    myfield_director.append(i.find_all('div', {'class':'plot_summary'}))\n",
    "\n",
    "import re\n",
    "r_one = re.compile(\".*name\")\n",
    "\n",
    "for i in tqdm(myfield_director):\n",
    "    for j in tqdm(i):\n",
    "        director_name.append(j.find_all('a', {'href':r_one}))\n",
    "        \n",
    "director_names = [item[0].text for item in director_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('director_names_final_two_19102019.pkl', 'wb') as f:\n",
    "    pickle.dump(director_names, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exctract column 5: Plot Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfield_keywords = []\n",
    "keywords = []\n",
    "\n",
    "for i in tqdm(souplist_two):\n",
    "    myfield_keywords.append(i.find_all('div', {'class':'see-more inline canwrap'}))\n",
    "\n",
    "myfield_keywords_final = [[item[0]] for item in myfield_keywords]\n",
    "\n",
    "for i in tqdm(myfield_keywords_final):\n",
    "    for j in tqdm(i):\n",
    "        keywords.append(j.find_all('span', {'class':\"itemprop\"}))\n",
    "\n",
    "keywords_final = []\n",
    "for i in keywords:\n",
    "    keywords_final.append(list(map(lambda x: x.text.strip(' ').replace('\\n', ''), i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('keywords_final_final_two_19102019.pkl', 'wb') as f:\n",
    "    pickle.dump(keywords_final, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract column 6: Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfield_genres = []\n",
    "genres = []\n",
    "\n",
    "for i in tqdm(souplist_two):\n",
    "    myfield_genres.append(i.find_all('div', {'class':'see-more inline canwrap'}))\n",
    "\n",
    "myfield_genres_final = [[item[1]] for item in myfield_genres]\n",
    "\n",
    "import re\n",
    "r_genres = re.compile(\"(?=genres)(.*)\")\n",
    "\n",
    "for i in tqdm(myfield_genres_final):\n",
    "    for j in tqdm(i):\n",
    "        genres.append(j.find_all('a', {'href':r_genres}))\n",
    "\n",
    "genres_final = []\n",
    "for i in genres:\n",
    "    genres_final.append(list(map(lambda x: x.text.strip(' ').replace('\\n', ''), i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('genres_final_final_two_19102019.pkl', 'wb') as f:\n",
    "    pickle.dump(genres_final, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FINAL DATASET (latest modification on 07.11.2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First dataset with 4774 movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (4774, 21)\n",
      "\n",
      "Column names: \n",
      "Index(['movie_imdb_link', 'movie_title', 'director_name', 'plot_keywords',\n",
      "       'genre_0', 'genre_1', 'genre_2', 'updated_rating', 'plot_summary',\n",
      "       'combined_features', 'full_cast', 'full_cast_embeddings',\n",
      "       'minimum_cast_vectors', 'maximum_cast_vectors', 'average_cast_vectors',\n",
      "       'minimum_plot_vectors', 'maximum_plot_vectors', 'average_plot_vectors',\n",
      "       'minimum_combined_features', 'maximum_combined_features',\n",
      "       'average_combined_features'],\n",
      "      dtype='object')\n",
      "\n",
      "The new Column names of dataset_one: \n",
      "Index(['movie_title', 'movie_imdb_link', 'updated_rating', 'full_cast',\n",
      "       'director_name', 'plot_summary', 'plot_keywords', 'genre_0', 'genre_1',\n",
      "       'genre_2'],\n",
      "      dtype='object')\n",
      "\n",
      "The new shape of dataset_one: (4774, 10)\n"
     ]
    }
   ],
   "source": [
    "dataset_one = pd.read_pickle('dataset_one_07112019.pkl')\n",
    "\n",
    "print(\"Shape: {}\".format(dataset_one.shape))\n",
    "\n",
    "print(\"\\nColumn names: \\n{}\".format(dataset_one.columns))\n",
    "\n",
    "dataset_one = dataset_one[['movie_title', 'movie_imdb_link', 'updated_rating', 'full_cast', 'director_name', 'plot_summary', \n",
    "                           'plot_keywords', 'genre_0', 'genre_1', 'genre_2']]\n",
    "\n",
    "print(\"\\nThe new Column names of dataset_one: \\n{}\".format(dataset_one.columns))\n",
    "print(\"\\nThe new shape of dataset_one: {}\".format(dataset_one.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Second dataset with 9145 movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>fn</th>\n",
       "      <th>tid</th>\n",
       "      <th>title</th>\n",
       "      <th>wordsInTitle</th>\n",
       "      <th>url</th>\n",
       "      <th>imdbRating</th>\n",
       "      <th>ratingCount</th>\n",
       "      <th>duration</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>News</th>\n",
       "      <th>RealityTV</th>\n",
       "      <th>Romance</th>\n",
       "      <th>SciFi</th>\n",
       "      <th>Short</th>\n",
       "      <th>Sport</th>\n",
       "      <th>TalkShow</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>titles01/tt0054997</td>\n",
       "      <td>tt0054997</td>\n",
       "      <td>Haie der Großstadt (1961)</td>\n",
       "      <td>haie der gro stadt</td>\n",
       "      <td>http://www.imdb.com/title/tt0054997/</td>\n",
       "      <td>8.1</td>\n",
       "      <td>47138.0</td>\n",
       "      <td>8040.0</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>titles01/tt0055031</td>\n",
       "      <td>tt0055031</td>\n",
       "      <td>Das Urteil von Nürnberg (1961)</td>\n",
       "      <td>das urteil von n rnberg</td>\n",
       "      <td>http://www.imdb.com/title/tt0055031/</td>\n",
       "      <td>8.3</td>\n",
       "      <td>28790.0</td>\n",
       "      <td>11160.0</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>titles01/tt0055630</td>\n",
       "      <td>tt0055630</td>\n",
       "      <td>Die Leibwache (1961)</td>\n",
       "      <td>die leibwache</td>\n",
       "      <td>http://www.imdb.com/title/tt0055630/</td>\n",
       "      <td>8.4</td>\n",
       "      <td>53475.0</td>\n",
       "      <td>6600.0</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>titles01/tt0056172</td>\n",
       "      <td>tt0056172</td>\n",
       "      <td>Lawrence von Arabien (1962)</td>\n",
       "      <td>lawrence von arabien</td>\n",
       "      <td>http://www.imdb.com/title/tt0056172/</td>\n",
       "      <td>8.4</td>\n",
       "      <td>140727.0</td>\n",
       "      <td>12960.0</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>62</td>\n",
       "      <td>titles01/tt0056592</td>\n",
       "      <td>tt0056592</td>\n",
       "      <td>Wer die Nachtigall stört (1962)</td>\n",
       "      <td>wer die nachtigall st rt</td>\n",
       "      <td>http://www.imdb.com/title/tt0056592/</td>\n",
       "      <td>8.4</td>\n",
       "      <td>154978.0</td>\n",
       "      <td>7740.0</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                  fn        tid                            title  \\\n",
       "0     58  titles01/tt0054997  tt0054997        Haie der Großstadt (1961)   \n",
       "1     59  titles01/tt0055031  tt0055031   Das Urteil von Nürnberg (1961)   \n",
       "2     60  titles01/tt0055630  tt0055630             Die Leibwache (1961)   \n",
       "3     61  titles01/tt0056172  tt0056172      Lawrence von Arabien (1962)   \n",
       "4     62  titles01/tt0056592  tt0056592  Wer die Nachtigall stört (1962)   \n",
       "\n",
       "               wordsInTitle                                   url  imdbRating  \\\n",
       "0        haie der gro stadt  http://www.imdb.com/title/tt0054997/         8.1   \n",
       "1   das urteil von n rnberg  http://www.imdb.com/title/tt0055031/         8.3   \n",
       "2             die leibwache  http://www.imdb.com/title/tt0055630/         8.4   \n",
       "3      lawrence von arabien  http://www.imdb.com/title/tt0056172/         8.4   \n",
       "4  wer die nachtigall st rt  http://www.imdb.com/title/tt0056592/         8.4   \n",
       "\n",
       "   ratingCount  duration    year  ... News  RealityTV  Romance  SciFi  Short  \\\n",
       "0      47138.0    8040.0  1961.0  ...    0          0        0      0      0   \n",
       "1      28790.0   11160.0  1961.0  ...    0          0        0      0      0   \n",
       "2      53475.0    6600.0  1961.0  ...    0          0        0      0      0   \n",
       "3     140727.0   12960.0  1962.0  ...    0          0        0      0      0   \n",
       "4     154978.0    7740.0  1962.0  ...    0          0        0      0      0   \n",
       "\n",
       "   Sport  TalkShow  Thriller  War  Western  \n",
       "0      1         0         0    0        0  \n",
       "1      0         0         0    1        0  \n",
       "2      0         0         0    0        0  \n",
       "3      0         0         0    0        0  \n",
       "4      0         0         0    0        0  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_two = pd.read_pickle('dataset_two_07112019.pkl')\n",
    "\n",
    "dataset_two.shape\n",
    "\n",
    "dataset_two_cleaned_reset_indexed = dataset_two_cleaned.reset_index()\n",
    "\n",
    "dataset_two_cleaned_reset_indexed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First 5000 of the second dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_five = dataset_two_cleaned_reset_indexed.iloc[0:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_five_one = first_five.drop([761, 2511, 4326])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4997, 45)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_five_one.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_five_one_reset_indexed = first_five_one.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_five_two = first_five_one_reset_indexed.drop([276,  1057, 1216, 1378, 3219, 3331, 3384, 3521, 3676, 3726, 3743, 3758, 3771, 3802, 3927, 3953, 3958, 3970, \n",
    "                  3980, 3981, 4010, 4022, 4042, 4060, 4070, 4091, 4094, 4101, 4114, 4115, 4121, 4128, 4141, 4152, 4154, 4186, \n",
    "                  4194, 4235, 4243, 4245, 4248, 4274, 4277, 4282, 4285, 4291, 4299, 4300, 4304, 4305, 4306, 4309, 4311, 4313, \n",
    "                  4315, 4318, 4322, 4324, 4329, 4330, 4340, 4426, 4630, 4665, 4676, 4683, 4696, 4700, 4731, 4738, 4755, 4778, \n",
    "                  4785, 4806, 4811, 4829, 4958])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4920, 45)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_five_two.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_five_two_reset_indexed = first_five_two.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_five_three = first_five_two_reset_indexed.drop([202, 454, 800, 1213, 1301,1911, 2567, 2568, 2569, 2570, 3017, 3140, 3457,\n",
    "                                                      3760, 4014, 4092, 4333, 4410, 4452, 4455, 4552, 4708])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4898, 45)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_five_three.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_five_three_reset_indexed = first_five_three.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_five_four = first_five_three_reset_indexed.drop(\n",
    "    [273,  419,  712,   882,  883,  955,  1012, 1372, 1377, 1676, 1920, 2108, 2219, 2467, 2517, 2642, 2646, 2696, \n",
    "    2719, 2764, 2767, 2769,  2793, 2797, 2826, 2887, 2904, 2936, 3001, 3022, 3024, 3027, 3079, 3116, 3133, 3134, \n",
    "    3140, 3158, 3162, 3176, 3193, 3208, 3210,  3217, 3221, 3231, 3242, 3245, 3249, 3267, 3278, 3288, 3291, 3292, \n",
    "    3300, 3319, 3327, 3330, 3342, 3359, 3411, 3417, 3426, 3432,  3434, 3445, 3453, 3457, 3468, 3474, 3498, 3502, \n",
    "    3508, 3516, 3533, 3551, 3562, 3578, 3605, 3606, 3616, 3621, 3625, 3635, 3637,  3659, 3665, 3668, 3686, 3692, \n",
    "    3698, 3701, 3723, 3770, 3775, 3778, 3796, 3797, 3798, 3800, 3811, 3812, 3823, 3896, 3915, 3931,  3932, 3944, \n",
    "    4001, 4005, 4006, 4008, 4028, 4033, 4037, 4049, 4062, 4075, 4085, 4086, 4102, 4106, 4107, 4124,  4140, 4142, \n",
    "    4158, 4161, 4164, 4166, 4170, 4177, 4179, 4184, 4201, 4202, 4207, 4208, 4212, 4213, 4229, 4232, 4233,  4235, \n",
    "    4238, 4241, 4246, 4247, 4252, 4255, 4257, 4260, 4261, 4262, 4263, 4265, 4334, 4336, 4345, 4392, 4417,  4457, \n",
    "    4500, 4540, 4551, 4557, 4575, 4604, 4612, 4626, 4627, 4643, 4655, 4684, 4705, 4717, 4763, 4776, 4779,  4791, \n",
    "    4846, 4855, 4864, 4866, 4897])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4713, 45)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_five_four.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remember to pickle the first_five_four dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Second 5000 of the second dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_five = dataset_two_cleaned_reset_indexed.iloc[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4145"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(second_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_five_reset_indexed = second_five.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_five_one = second_five_reset_indexed.drop(\n",
    "    [3397, 79,  94,  137, 212, 226, 243, 261, 273, 297, 298, 332, 358, 365, 379, 382, 391, 398, 404, 409, 481, \n",
    "     515, 518, 520, 532, 545, 561, 565, 582, 605, 608, 614, 624,626, 632, 637, 642, 644, 646, 647, \n",
    "     648, 649, 652, 656, 658, 2353, 2805, 2911, 2968, 2990, 3117, 3187, 3259, 3424, 3439, 3618, 3771, \n",
    "     3858, 3866, 3907, 4129, 231, 498, 511, 563, 1857, 2005, 2030, 2345, 2690, 2862, 2891, 3112, 3214, 3534, 3641, 3766, 3901, \n",
    "     3918, 4070, 23, 2899, 3512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_five_one_reset_indexed = second_five_one.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_five_two = second_five_one_reset_indexed.drop(\n",
    "    [63, 67, 103, 104, 125, 148, 173, 185, 187, 188, 195, 199, 217, 222, 224, 225, 238, 240, 245, 251, \n",
    "     270, 271, 308, 310, 338, 340, 345, 347, 352, 354, 379, 384, 390, 393, 399, 418, 438, 441, 445, 449, \n",
    "     450, 454, 464, 468, 469, 477, 478, 484, 493, 501, 520, 532, 537, 539, 544, 552, 554, 560, 576, 587, \n",
    "     588, 595, 607, 608, 611, 838, 980, 1148, 1151, 1188, 1400, 1433, 1441, 1526, 1826, 1834, 1980, 2061, \n",
    "     2095, 2121, 2142, 2496, 2504, 2577, 2659, 2757, 2801, 2921, 2934, 3012, 3023, 3049, 3063,  3067, \n",
    "     3093, 3098, 3105, 3109, 3138, 3155, 3172, 3189, 3204, 3221, 3226, 3228, 3260, 3263, 3267, 3269, 3276, \n",
    "     3285, 3305, 3317, 3325,3326, 3327, 3329, 3353, 3359, 3387, 3421, 3454, 3478, 3495, 3500, 3511,3543, \n",
    "     3551, 3571, 3576, 3584, 3631, 3658, 3665, 3668, 3674, 3690, 3699, 3716, 3719, 3727, 3742, 3749, 3751,\n",
    "     3770, 3793, 3800, 3809, 3817, 3822, 3826, 3832, 3837, 3853, 3856,3874, 3882, 3883, 3906, 3931, 3941, \n",
    "     3947, 3974, 3978, 3979, 3987, 4002, 4018, 4021, 4039, 4041, 4050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_five_two_reset_indexed = second_five_two.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_five_three = second_five_two_reset_indexed.drop([346, 441, 503, 505, 507, 510, 514, 2845, 2989, 3104, 3302, 3701])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3877, 45)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_five_three.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### remember to pickle the \"second_five_three\" dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Column: Movie Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Avatar', \"Pirates of the Caribbean: At World's End\", 'Spectre', 'The Dark Knight Rises', 'John Carter']\n",
      "\n",
      " 4774\n"
     ]
    }
   ],
   "source": [
    "list_one_title = dataset_one.movie_title.tolist()\n",
    "\n",
    "print(list_one_title[0:5])\n",
    "\n",
    "print(\"\\n\",len(list_one_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Haie der Großstadt (1961)', 'Das Urteil von Nürnberg (1961)', 'Die Leibwache (1961)', 'Lawrence von Arabien (1962)', 'Wer die Nachtigall stört (1962)']\n",
      "\n",
      " 4713\n"
     ]
    }
   ],
   "source": [
    "list_two_title = first_five_four.title.tolist()\n",
    "\n",
    "print(list_two_title[0:5])\n",
    "\n",
    "print(\"\\n\",len(list_two_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Oldboy (2013)', 'Carjacked - Jeder hat seine Grenzen (2011)', 'Carlos - Der Schakal (TV Mini-Series 2010)', 'Im August in Osage County (2013)', 'Verrückt nach Dir (2010)']\n",
      "\n",
      " 3877\n"
     ]
    }
   ],
   "source": [
    "list_three_title = second_five_three.title.tolist()\n",
    "    \n",
    "print(list_three_title[0:5])\n",
    "\n",
    "print(\"\\n\",len(list_three_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Avatar', \"Pirates of the Caribbean: At World's End\", 'Spectre', 'The Dark Knight Rises', 'John Carter']\n",
      "\n",
      " 13364\n"
     ]
    }
   ],
   "source": [
    "final_list_title = list_one_title + list_two_title + list_three_title\n",
    "\n",
    "print(final_list_title[0:5])\n",
    "\n",
    "print(\"\\n\",len(final_list_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Column: IMDB link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.imdb.com/title/tt0499549/?ref_=fn_tt_tt_1', 'http://www.imdb.com/title/tt0449088/?ref_=fn_tt_tt_1', 'http://www.imdb.com/title/tt2379713/?ref_=fn_tt_tt_1', 'http://www.imdb.com/title/tt1345836/?ref_=fn_tt_tt_1', 'http://www.imdb.com/title/tt0401729/?ref_=fn_tt_tt_1']\n",
      "\n",
      " 4774\n"
     ]
    }
   ],
   "source": [
    "list_one_url = dataset_one.movie_imdb_link.tolist()\n",
    "\n",
    "print(list_one_url[0:5])\n",
    "\n",
    "print(\"\\n\",len(list_one_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.imdb.com/title/tt0054997/', 'http://www.imdb.com/title/tt0055031/', 'http://www.imdb.com/title/tt0055630/', 'http://www.imdb.com/title/tt0056172/', 'http://www.imdb.com/title/tt0056592/']\n",
      "\n",
      " 4713\n"
     ]
    }
   ],
   "source": [
    "list_two_url = first_five_four.url.tolist()\n",
    "\n",
    "print(list_two_url[0:5])\n",
    "\n",
    "print(\"\\n\",len(list_two_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.imdb.com/title/tt1321511/', 'http://www.imdb.com/title/tt1321861/', 'http://www.imdb.com/title/tt1321865/', 'http://www.imdb.com/title/tt1322269/', 'http://www.imdb.com/title/tt1322312/']\n",
      "\n",
      " 3877\n"
     ]
    }
   ],
   "source": [
    "list_three_url = second_five_three.url.tolist()\n",
    "    \n",
    "print(list_three_url[0:5])\n",
    "\n",
    "print(\"\\n\",len(list_three_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.imdb.com/title/tt0499549/?ref_=fn_tt_tt_1', 'http://www.imdb.com/title/tt0449088/?ref_=fn_tt_tt_1', 'http://www.imdb.com/title/tt2379713/?ref_=fn_tt_tt_1', 'http://www.imdb.com/title/tt1345836/?ref_=fn_tt_tt_1', 'http://www.imdb.com/title/tt0401729/?ref_=fn_tt_tt_1']\n",
      "\n",
      " 13364\n"
     ]
    }
   ],
   "source": [
    "final_list_url = list_one_url + list_two_url + list_three_url\n",
    "\n",
    "print(final_list_url[0:5])\n",
    "\n",
    "print(\"\\n\",len(final_list_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third column: IMDB Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7.8', '7.1', '6.8', '8.4', '6.6']\n",
      "\n",
      " 4774\n"
     ]
    }
   ],
   "source": [
    "list_one_rating = dataset_one.updated_rating.tolist()\n",
    "\n",
    "print(list_one_rating[0:5])\n",
    "\n",
    "print(\"\\n\",len(list_one_rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8.0', '8.2', '8.2', '8.3', '8.3']\n",
      "\n",
      " 4713\n"
     ]
    }
   ],
   "source": [
    "with open('ratings_one_18102019.pkl', 'rb') as f:\n",
    "    list_two_rating = pickle.load(f)\n",
    "    \n",
    "print(list_two_rating[0:5])\n",
    "\n",
    "print(\"\\n\",len(list_two_rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5.7', '5.0', '7.6', '7.2', '6.3']\n",
      "\n",
      " 3877\n"
     ]
    }
   ],
   "source": [
    "with open('rating_final_two_19102019.pkl', 'rb') as f:\n",
    "    list_three_rating = pickle.load(f)\n",
    "    \n",
    "print(list_three_rating[0:5])\n",
    "\n",
    "print(\"\\n\",len(list_three_rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7.8', '7.1', '6.8', '8.4', '6.6']\n",
      "\n",
      " 13364\n"
     ]
    }
   ],
   "source": [
    "final_list_rating = list_one_rating + list_two_rating + list_three_rating\n",
    "\n",
    "print(final_list_rating[0:5])\n",
    "\n",
    "print(\"\\n\",len(final_list_rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fourth column: Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Sam-Worthington', 'Zoe-Saldana', 'Sigourney-Weaver', 'Stephen-Lang', 'Michelle-Rodriguez', 'Giovanni-Ribisi', 'Joel-David-Moore', 'CCH-Pounder', 'Wes-Studi', 'Laz-Alonso', 'Dileep-Rao', 'Matt-Gerald', 'Sean-Anthony-Moran', 'Jason-Whyte', 'Scott-Lawrence'], ['Johnny-Depp', 'Geoffrey-Rush', 'Orlando-Bloom', 'Keira-Knightley', 'Jack-Davenport', 'Bill-Nighy', 'Jonathan-Pryce', 'Lee-Arenberg', 'Mackenzie-Crook', 'Kevin-McNally', 'David-Bailie', 'Stellan-Skarsgård', 'Tom-Hollander', 'Naomie-Harris', 'Martin-Klebba'], ['Daniel-Craig', 'Christoph-Waltz', 'Léa-Seydoux', 'Ralph-Fiennes', 'Monica-Bellucci', 'Ben-Whishaw', 'Naomie-Harris', 'Dave-Bautista', 'Andrew-Scott', 'Rory-Kinnear', 'Jesper-Christensen', 'Alessandro-Cremona', 'Stephanie-Sigman', 'Tenoch-Huerta', 'Adriana-Paz'], ['Christian-Bale', 'Gary-Oldman', 'Tom-Hardy', 'Joseph-Gordon-Levitt', 'Anne-Hathaway', 'Marion-Cotillard', 'Morgan-Freeman', 'Michael-Caine', 'Matthew-Modine', 'Alon-Aboutboul', 'Ben-Mendelsohn', 'Burn-Gorman', 'Daniel-Sunjata', 'Aidan-Gillen', 'Sam-Kennard'], ['Taylor-Kitsch', 'Lynn-Collins', 'Samantha-Morton', 'Willem-Dafoe', 'Thomas-Haden-Church', 'Mark-Strong', 'Ciarán-Hinds', 'Dominic-West', 'James-Purefoy', 'Bryan-Cranston', 'Polly-Walker', 'Daryl-Sabara', 'Arkie-Reece', 'Davood-Ghadami', 'Pippa-Nixon']]\n",
      "\n",
      " 4774\n"
     ]
    }
   ],
   "source": [
    "list_one_actors = dataset_one.full_cast.tolist()\n",
    "\n",
    "print(list_one_actors[0:5])\n",
    "\n",
    "print(\"\\n\",len(list_one_actors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Paul Newman', 'Jackie Gleason', 'Piper Laurie', 'George C. Scott', 'Myron McCormick', 'Murray Hamilton', 'Michael Constantine', 'Stefan Gierasch', 'Clifford A. Pellow', 'Jake LaMotta', 'Gordon B. Clarke', 'Alexander Rose', 'Carolyn Coates', 'Carl York', 'Vincent Gardenia'], ['Spencer Tracy', 'Burt Lancaster', 'Richard Widmark', 'Marlene Dietrich', 'Maximilian Schell', 'Judy Garland', 'Montgomery Clift', 'William Shatner', 'Werner Klemperer', 'Kenneth MacKenna', 'Torben Meyer', 'Joseph Bernard', 'Alan Baxter', 'Edward Binns', 'Virginia Christine'], ['Toshirô Mifune', 'Tatsuya Nakadai', 'Yôko Tsukasa', 'Isuzu Yamada', 'Daisuke Katô', 'Seizaburô Kawazu', 'Takashi Shimura', 'Hiroshi Tachikawa', 'Yôsuke Natsuki', 'Eijirô Tôno', 'Kamatari Fujiwara', 'Ikio Sawamura', 'Atsushi Watanabe', 'Susumu Fujita', 'Kyû Sazanka'], [\"Peter O'Toole\", 'Alec Guinness', 'Anthony Quinn', 'Jack Hawkins', 'Omar Sharif', 'José Ferrer', 'Anthony Quayle', 'Claude Rains', 'Arthur Kennedy', 'Donald Wolfit', 'I.S. Johar', 'Gamil Ratib', 'Michel Ray', 'John Dimech', 'Zia Mohyeddin'], ['Gregory Peck', 'John Megna', 'Frank Overton', 'Rosemary Murphy', 'Ruth White', 'Brock Peters', 'Estelle Evans', 'Paul Fix', 'Collin Wilcox Paxton', 'James Anderson', 'Alice Ghostley', 'Robert Duvall', 'William Windom', 'Crahan Denton', 'Richard Hale']]\n",
      "\n",
      " 4713\n"
     ]
    }
   ],
   "source": [
    "with open('actors_list_one_18102019.pkl', 'rb') as f:\n",
    "    list_two_actors = pickle.load(f)\n",
    "    \n",
    "print(list_two_actors[0:5])\n",
    "\n",
    "print(\"\\n\",len(list_two_actors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Josh Brolin', 'Elizabeth Olsen', 'Sharlto Copley', 'Samuel L. Jackson', 'Michael Imperioli', 'Pom Klementieff', 'James Ransone', 'Max Casella', 'Linda Emond', 'Elvis Nolasco', 'Rami Malek', 'Lance Reddick', 'Hannah Ware', 'Richard Portnow', 'Hannah Simone'], ['Maria Bello', 'Stephen Dorff', 'Connor Hill', 'Robert Peters', 'Cynthia Rube', 'Michael Arata', 'Gary Grubbs', 'Josh Gates', 'Tim Griffin', 'Catherine Dent', 'Kristen Kerr', 'Joanna Cassidy', 'Angelle Brooks', 'Jeff Joslin', 'Lenore Banks'], ['Edgar Ramírez', 'Alexander Scheer', 'Fadi Abi Samra', 'Lamia Ahmed', 'Karam Ghossein', 'Liane Sellerer', 'Philippe Tran', 'Ahmad Kaabour', 'Talal Jurdi', 'Juana Acosta', 'Nora von Waldstätten', 'Christoph Bach', 'Rodney El Haddad', 'Julia Hummer', 'Antoine Balabane', 'Rami Farah', 'Aljoscha Stadelmann', 'Zeid Hamdan', 'Fadi Yanni Turk', 'Katharina Schüttler', 'Badih Abou Chakra', 'Basim Kahar', 'Cem Sultan Ungan'], ['Meryl Streep', 'Julia Roberts', 'Chris Cooper', 'Ewan McGregor', 'Margo Martindale', 'Sam Shepard', 'Dermot Mulroney', 'Julianne Nicholson', 'Juliette Lewis', 'Abigail Breslin', 'Benedict Cumberbatch', 'Misty Upham', 'Will Coffey', 'Newell Alexander', 'Jerry Stahl'], ['Drew Barrymore', 'Justin Long', 'Charlie Day', 'Jason Sudeikis', 'Christina Applegate', 'Ron Livingston', 'Oliver Jackson-Cohen', 'Jim Gaffigan', 'Natalie Morales', 'Kelli Garner', 'June Diane Raphael', 'Rob Riggle', 'Sarah Burns', 'Terry Beaver', 'Matt Servitto']]\n",
      "\n",
      " 3877\n"
     ]
    }
   ],
   "source": [
    "with open('actors_list_final_two_19102019.pkl', 'rb') as f:\n",
    "    list_three_actors = pickle.load(f)\n",
    "    \n",
    "print(list_three_actors[0:5])\n",
    "\n",
    "print(\"\\n\",len(list_three_actors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Sam-Worthington', 'Zoe-Saldana', 'Sigourney-Weaver', 'Stephen-Lang', 'Michelle-Rodriguez', 'Giovanni-Ribisi', 'Joel-David-Moore', 'CCH-Pounder', 'Wes-Studi', 'Laz-Alonso', 'Dileep-Rao', 'Matt-Gerald', 'Sean-Anthony-Moran', 'Jason-Whyte', 'Scott-Lawrence'], ['Johnny-Depp', 'Geoffrey-Rush', 'Orlando-Bloom', 'Keira-Knightley', 'Jack-Davenport', 'Bill-Nighy', 'Jonathan-Pryce', 'Lee-Arenberg', 'Mackenzie-Crook', 'Kevin-McNally', 'David-Bailie', 'Stellan-Skarsgård', 'Tom-Hollander', 'Naomie-Harris', 'Martin-Klebba'], ['Daniel-Craig', 'Christoph-Waltz', 'Léa-Seydoux', 'Ralph-Fiennes', 'Monica-Bellucci', 'Ben-Whishaw', 'Naomie-Harris', 'Dave-Bautista', 'Andrew-Scott', 'Rory-Kinnear', 'Jesper-Christensen', 'Alessandro-Cremona', 'Stephanie-Sigman', 'Tenoch-Huerta', 'Adriana-Paz'], ['Christian-Bale', 'Gary-Oldman', 'Tom-Hardy', 'Joseph-Gordon-Levitt', 'Anne-Hathaway', 'Marion-Cotillard', 'Morgan-Freeman', 'Michael-Caine', 'Matthew-Modine', 'Alon-Aboutboul', 'Ben-Mendelsohn', 'Burn-Gorman', 'Daniel-Sunjata', 'Aidan-Gillen', 'Sam-Kennard'], ['Taylor-Kitsch', 'Lynn-Collins', 'Samantha-Morton', 'Willem-Dafoe', 'Thomas-Haden-Church', 'Mark-Strong', 'Ciarán-Hinds', 'Dominic-West', 'James-Purefoy', 'Bryan-Cranston', 'Polly-Walker', 'Daryl-Sabara', 'Arkie-Reece', 'Davood-Ghadami', 'Pippa-Nixon']]\n",
      "\n",
      " 13364\n"
     ]
    }
   ],
   "source": [
    "final_list_actors = list_one_actors + list_two_actors + list_three_actors\n",
    "\n",
    "print(final_list_actors[0:5])\n",
    "\n",
    "print(\"\\n\",len(final_list_actors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fifth column: Director Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James-Cameron', 'Gore-Verbinski', 'Sam-Mendes', 'Christopher-Nolan', 'Andrew-Stanton']\n",
      "\n",
      " 4774\n"
     ]
    }
   ],
   "source": [
    "list_one_directors = dataset_one.director_name.tolist()\n",
    "\n",
    "print(list_one_directors[0:5])\n",
    "\n",
    "print(\"\\n\",len(list_one_directors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Robert Rossen', 'Stanley Kramer', 'Akira Kurosawa', 'T.E. Lawrence', 'Robert Mulligan']\n",
      "\n",
      " 4713\n"
     ]
    }
   ],
   "source": [
    "with open('director_names_one_18102019.pkl', 'rb') as f:\n",
    "    list_two_directors = pickle.load(f)\n",
    "    \n",
    "print(list_two_directors[0:5])\n",
    "\n",
    "print(\"\\n\",len(list_two_directors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spike Lee', 'John Bonito', 'Edgar Ramírez', 'John Wells', 'Nanette Burstein']\n",
      "\n",
      " 3877\n"
     ]
    }
   ],
   "source": [
    "with open('director_names_final_two_19102019.pkl', 'rb') as f:\n",
    "    list_three_directors = pickle.load(f)\n",
    "    \n",
    "print(list_three_directors[0:5])\n",
    "\n",
    "print(\"\\n\",len(list_three_directors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James-Cameron', 'Gore-Verbinski', 'Sam-Mendes', 'Christopher-Nolan', 'Andrew-Stanton']\n",
      "\n",
      " 13364\n"
     ]
    }
   ],
   "source": [
    "final_list_directors = list_one_directors + list_two_directors + list_three_directors\n",
    "\n",
    "print(final_list_directors[0:5])\n",
    "\n",
    "print(\"\\n\",len(final_list_directors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sixth column: Plot Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A paraplegic Marine dispatched to the moon Pandora on a unique mission becomes torn between following his orders and protecting the world he feels is his home', 'Captain Barbossa Will Turner and Elizabeth Swann must sail off the edge of the map navigate treachery and betrayal find Jack Sparrow and make their final alliances for one last decisive battle', \"A cryptic message from 007's past sends him pitted against a mysterious terrorist organization called Spectre and learns of its involvement in previous events of his most dangerous missions\", \"Eight years after the Joker's reign of anarchy Batman with the help of the enigmatic Catwoman is forced from his exile to save Gotham City now on the edge of total annihilation from the brutal guerrilla terrorist Bane\", 'Transported to Barsoom a Civil War vet discovers a barren planet seemingly inhabited by 12-foot tall barbarians Finding himself prisoner of these creatures he escapes only to encounter Woola and a princess in desperate need of a savior']\n",
      "\n",
      " 4774\n"
     ]
    }
   ],
   "source": [
    "list_one_plot = dataset_one.plot_summary.tolist()\n",
    "\n",
    "print(list_one_plot[0:5])\n",
    "\n",
    "print(\"\\n\",len(list_one_plot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n                    An up-and-coming pool player plays a long-time champion in a single high-stakes match.\\n            ', '\\n                    In 1948, an American court in occupied Germany tries four Nazis judged for war crimes.\\n            ', '\\n                    A crafty ronin comes to a town divided by two criminal gangs and decides to play them against each other to free the town.\\n            ', '\\n                    The story of T.E. Lawrence, the English officer who successfully united and led the diverse, often warring, Arab tribes during World War I in order to fight the Turks.\\n            ', '\\n                    Atticus Finch, a lawyer in the Depression-era South, defends a black man against an undeserved rape charge, and his children against prejudice.\\n            ']\n",
      "\n",
      " 4713\n"
     ]
    }
   ],
   "source": [
    "with open('plot_summary_one_18102019.pkl', 'rb') as f:\n",
    "    list_two_plot = pickle.load(f)\n",
    "    \n",
    "print(list_two_plot[0:5])\n",
    "\n",
    "print(\"\\n\",len(list_two_plot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n                    Obsessed with vengeance, a man sets out to find out why he was kidnapped and locked into solitary confinement for twenty years without reason.\\n            ', '\\n                    A single mom and her child are carjacked by a bank robber.\\n            ', '\\n                    The story of Venezuelan revolutionary Ilich Ramírez Sánchez, who founded a worldwide terrorist organization and raided the 1975 OPEC meeting.\\n            ', '\\n                    A look at the lives of the strong-willed women of the Weston family, whose paths have diverged until a family crisis brings them back to the Oklahoma house they grew up in, and to the dysfunctional woman who raised them.\\n            ', '\\n                    A romantic comedy centered on a guy and a gal who try to keep their love alive as they shuttle back and forth between New York and San Francisco to see one another.\\n            ']\n",
      "\n",
      " 3877\n"
     ]
    }
   ],
   "source": [
    "with open('plot_summary_final_two_19102019.pkl', 'rb') as f:\n",
    "    list_three_plot = pickle.load(f)\n",
    "    \n",
    "print(list_three_plot[0:5])\n",
    "\n",
    "print(\"\\n\",len(list_three_plot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A paraplegic Marine dispatched to the moon Pandora on a unique mission becomes torn between following his orders and protecting the world he feels is his home', 'Captain Barbossa Will Turner and Elizabeth Swann must sail off the edge of the map navigate treachery and betrayal find Jack Sparrow and make their final alliances for one last decisive battle', \"A cryptic message from 007's past sends him pitted against a mysterious terrorist organization called Spectre and learns of its involvement in previous events of his most dangerous missions\", \"Eight years after the Joker's reign of anarchy Batman with the help of the enigmatic Catwoman is forced from his exile to save Gotham City now on the edge of total annihilation from the brutal guerrilla terrorist Bane\", 'Transported to Barsoom a Civil War vet discovers a barren planet seemingly inhabited by 12-foot tall barbarians Finding himself prisoner of these creatures he escapes only to encounter Woola and a princess in desperate need of a savior']\n",
      "\n",
      " 13364\n"
     ]
    }
   ],
   "source": [
    "final_list_plot = list_one_plot + list_two_plot + list_three_plot\n",
    "\n",
    "print(final_list_plot[0:5])\n",
    "\n",
    "print(\"\\n\",len(final_list_plot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seventh column: Plot keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['avatar', 'future', 'marine', 'native', 'paraplegic'], ['goddess', 'marriage', 'ceremony', 'marriage', 'proposal', 'pirate', 'singapore'], ['bomb', 'espionage', 'sequel', 'spy', 'terrorist'], ['deception', 'imprisonment', 'lawlessness', 'police', 'officer', 'terrorist', 'plot'], ['alien', 'american', 'civil', 'war', 'male', 'nipple', 'mars', 'princess']]\n",
      "\n",
      " 4774\n"
     ]
    }
   ],
   "source": [
    "list_one_keywords = dataset.plot_keywords.tolist()\n",
    "\n",
    "print(list_one_keywords[0:5])\n",
    "\n",
    "print(\"\\n\",len(list_one_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['playing pool', 'arrogance', 'pool hall', 'pool shark', 'pool hustler'], ['nuremberg trials', 'judge', 'nazi', 'war crime', 'courtroom drama'], ['samurai', 'ronin', 'bodyguard', 'one against many', 'man with no name'], ['arab', 'desert', 'bedouin', 'ottoman empire', 'british military'], ['trial', 'lawyer', 'false accusation', 'based on novel', 'small town']]\n",
      "\n",
      " 4713\n"
     ]
    }
   ],
   "source": [
    "with open('keywords_final_one_18102019.pkl', 'rb') as f:\n",
    "    list_two_keywords = pickle.load(f)\n",
    "    \n",
    "print(list_two_keywords[0:5])\n",
    "\n",
    "print(\"\\n\",len(list_two_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['father daughter incest', 'incest', 'male nudity', 'female nudity', 'solitary confinement'], ['blonde', 'nipples visible through clothing', 'shotgun', 'kidnapping', 'money bag'], ['war story', 'male frontal nudity', 'damascus', 'syria', 'beirut'], ['incestuous relationship', 'pills', 'cancer', 'death of husband', 'family secret'], ['long distance relationship', 'job', 'bar', 'male pubic hair', 'pubic hair']]\n",
      "\n",
      " 3877\n"
     ]
    }
   ],
   "source": [
    "with open('keywords_final_final_two_19102019.pkl', 'rb') as f:\n",
    "    list_three_keywords = pickle.load(f)\n",
    "    \n",
    "print(list_three_keywords[0:5])\n",
    "\n",
    "print(\"\\n\",len(list_three_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['avatar', 'future', 'marine', 'native', 'paraplegic'], ['goddess', 'marriage', 'ceremony', 'marriage', 'proposal', 'pirate', 'singapore'], ['bomb', 'espionage', 'sequel', 'spy', 'terrorist'], ['deception', 'imprisonment', 'lawlessness', 'police', 'officer', 'terrorist', 'plot'], ['alien', 'american', 'civil', 'war', 'male', 'nipple', 'mars', 'princess']]\n",
      "\n",
      " 13364\n"
     ]
    }
   ],
   "source": [
    "final_list_keywords = list_one_keywords + list_two_keywords + list_three_keywords\n",
    "\n",
    "print(final_list_keywords[0:5])\n",
    "\n",
    "print(\"\\n\",len(final_list_keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigth column: Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_one['genres'] = dataset_one[['genre_0', 'genre_1', 'genre_2']].apply(lambda x: ','.join(x[x.notnull()]).split(','), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Action', 'Adventure', 'Fantasy'], ['Action', 'Adventure', 'Fantasy'], ['Action', 'Adventure', 'Thriller'], ['Action', 'Thriller', '0'], ['Action', 'Adventure', 'Sci-Fi']]\n",
      "\n",
      " 4774\n"
     ]
    }
   ],
   "source": [
    "list_one_genres = dataset_one.genres.tolist()\n",
    "\n",
    "print(list_one_genres[0:5])\n",
    "\n",
    "print(\"\\n\",len(list_one_genres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Drama', 'Sport'], ['Drama', 'War'], ['Action', 'Comedy', 'Crime', 'Drama', 'Thriller'], ['Adventure', 'Biography', 'Drama', 'History', 'War'], ['Crime', 'Drama']]\n",
      "\n",
      " 4713\n"
     ]
    }
   ],
   "source": [
    "with open('genres_final_one_18102019.pkl', 'rb') as f:\n",
    "    list_two_genres = pickle.load(f)\n",
    "    \n",
    "print(list_two_genres[0:5])\n",
    "\n",
    "print(\"\\n\",len(list_two_genres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Action', 'Drama', 'Mystery', 'Thriller'], ['Crime', 'Thriller'], ['Biography', 'Crime', 'Drama', 'Thriller'], ['Comedy', 'Drama'], ['Comedy', 'Romance']]\n",
      "\n",
      " 3877\n"
     ]
    }
   ],
   "source": [
    "with open('genres_final_final_two_19102019.pkl', 'rb') as f:\n",
    "    list_three_genres = pickle.load(f)\n",
    "    \n",
    "print(list_three_genres[0:5])\n",
    "\n",
    "print(\"\\n\",len(list_three_genres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Action', 'Adventure', 'Fantasy'], ['Action', 'Adventure', 'Fantasy'], ['Action', 'Adventure', 'Thriller'], ['Action', 'Thriller', '0'], ['Action', 'Adventure', 'Sci-Fi']]\n",
      "\n",
      " 13364\n"
     ]
    }
   ],
   "source": [
    "final_list_genres = list_one_genres + list_two_genres + list_three_genres\n",
    "\n",
    "print(final_list_genres[0:5])\n",
    "\n",
    "print(\"\\n\",len(final_list_genres))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### THE FINAL DATASET OF 13364 MOVIES (21.10.2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = pd.DataFrame(list(zip(final_list_title, final_list_url, final_list_rating, final_list_actors, \n",
    "                                      final_list_directors, final_list_plot, final_list_keywords, final_list_genres)), \n",
    "                  columns =['Movie Title', 'IMDB Url', 'IMDB Rating', 'Actors', 'Director', 'Plot Summary', 'Plot Keywords', 'Genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13364, 8)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie Title</th>\n",
       "      <th>IMDB Url</th>\n",
       "      <th>IMDB Rating</th>\n",
       "      <th>Actors</th>\n",
       "      <th>Director</th>\n",
       "      <th>Plot Summary</th>\n",
       "      <th>Plot Keywords</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>http://www.imdb.com/title/tt0499549/?ref_=fn_t...</td>\n",
       "      <td>7.8</td>\n",
       "      <td>[Sam-Worthington, Zoe-Saldana, Sigourney-Weave...</td>\n",
       "      <td>James-Cameron</td>\n",
       "      <td>A paraplegic Marine dispatched to the moon Pan...</td>\n",
       "      <td>[avatar, future, marine, native, paraplegic]</td>\n",
       "      <td>[Action, Adventure, Fantasy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>http://www.imdb.com/title/tt0449088/?ref_=fn_t...</td>\n",
       "      <td>7.1</td>\n",
       "      <td>[Johnny-Depp, Geoffrey-Rush, Orlando-Bloom, Ke...</td>\n",
       "      <td>Gore-Verbinski</td>\n",
       "      <td>Captain Barbossa Will Turner and Elizabeth Swa...</td>\n",
       "      <td>[goddess, marriage, ceremony, marriage, propos...</td>\n",
       "      <td>[Action, Adventure, Fantasy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Spectre</td>\n",
       "      <td>http://www.imdb.com/title/tt2379713/?ref_=fn_t...</td>\n",
       "      <td>6.8</td>\n",
       "      <td>[Daniel-Craig, Christoph-Waltz, Léa-Seydoux, R...</td>\n",
       "      <td>Sam-Mendes</td>\n",
       "      <td>A cryptic message from 007's past sends him pi...</td>\n",
       "      <td>[bomb, espionage, sequel, spy, terrorist]</td>\n",
       "      <td>[Action, Adventure, Thriller]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>The Dark Knight Rises</td>\n",
       "      <td>http://www.imdb.com/title/tt1345836/?ref_=fn_t...</td>\n",
       "      <td>8.4</td>\n",
       "      <td>[Christian-Bale, Gary-Oldman, Tom-Hardy, Josep...</td>\n",
       "      <td>Christopher-Nolan</td>\n",
       "      <td>Eight years after the Joker's reign of anarchy...</td>\n",
       "      <td>[deception, imprisonment, lawlessness, police,...</td>\n",
       "      <td>[Action, Thriller, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>John Carter</td>\n",
       "      <td>http://www.imdb.com/title/tt0401729/?ref_=fn_t...</td>\n",
       "      <td>6.6</td>\n",
       "      <td>[Taylor-Kitsch, Lynn-Collins, Samantha-Morton,...</td>\n",
       "      <td>Andrew-Stanton</td>\n",
       "      <td>Transported to Barsoom a Civil War vet discove...</td>\n",
       "      <td>[alien, american, civil, war, male, nipple, ma...</td>\n",
       "      <td>[Action, Adventure, Sci-Fi]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Movie Title  \\\n",
       "0                                    Avatar   \n",
       "1  Pirates of the Caribbean: At World's End   \n",
       "2                                   Spectre   \n",
       "3                     The Dark Knight Rises   \n",
       "4                               John Carter   \n",
       "\n",
       "                                            IMDB Url IMDB Rating  \\\n",
       "0  http://www.imdb.com/title/tt0499549/?ref_=fn_t...         7.8   \n",
       "1  http://www.imdb.com/title/tt0449088/?ref_=fn_t...         7.1   \n",
       "2  http://www.imdb.com/title/tt2379713/?ref_=fn_t...         6.8   \n",
       "3  http://www.imdb.com/title/tt1345836/?ref_=fn_t...         8.4   \n",
       "4  http://www.imdb.com/title/tt0401729/?ref_=fn_t...         6.6   \n",
       "\n",
       "                                              Actors           Director  \\\n",
       "0  [Sam-Worthington, Zoe-Saldana, Sigourney-Weave...      James-Cameron   \n",
       "1  [Johnny-Depp, Geoffrey-Rush, Orlando-Bloom, Ke...     Gore-Verbinski   \n",
       "2  [Daniel-Craig, Christoph-Waltz, Léa-Seydoux, R...         Sam-Mendes   \n",
       "3  [Christian-Bale, Gary-Oldman, Tom-Hardy, Josep...  Christopher-Nolan   \n",
       "4  [Taylor-Kitsch, Lynn-Collins, Samantha-Morton,...     Andrew-Stanton   \n",
       "\n",
       "                                        Plot Summary  \\\n",
       "0  A paraplegic Marine dispatched to the moon Pan...   \n",
       "1  Captain Barbossa Will Turner and Elizabeth Swa...   \n",
       "2  A cryptic message from 007's past sends him pi...   \n",
       "3  Eight years after the Joker's reign of anarchy...   \n",
       "4  Transported to Barsoom a Civil War vet discove...   \n",
       "\n",
       "                                       Plot Keywords  \\\n",
       "0       [avatar, future, marine, native, paraplegic]   \n",
       "1  [goddess, marriage, ceremony, marriage, propos...   \n",
       "2          [bomb, espionage, sequel, spy, terrorist]   \n",
       "3  [deception, imprisonment, lawlessness, police,...   \n",
       "4  [alien, american, civil, war, male, nipple, ma...   \n",
       "\n",
       "                          Genres  \n",
       "0   [Action, Adventure, Fantasy]  \n",
       "1   [Action, Adventure, Fantasy]  \n",
       "2  [Action, Adventure, Thriller]  \n",
       "3          [Action, Thriller, 0]  \n",
       "4    [Action, Adventure, Sci-Fi]  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_dataset.to_pickle('dataset_part_1_07112019.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TO SQL SERVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = pd.read_pickle('final_dataset_07112019.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "connStr = pyodbc.connect(\"DRIVER={SQL Server Native Client 11.0};\"\n",
    "                         \"SERVER=GR2211336W2;\"\n",
    "                         \"DATABASE=Movies_Dataset;\"\n",
    "                         \"Trusted_Connection=yes\")\n",
    "\n",
    "cursor = connStr.cursor()\n",
    " \n",
    "delete_table =  \"\"\"           \n",
    "                        IF dbo.TableExists('data_table_one') = 1\n",
    "                             DELETE FROM data_table_one      \n",
    "                \"\"\"\n",
    "\n",
    "insert_values = \"\"\" \n",
    "                        EXEC [dbo].[store_movies] @Movie_Title = ?, @IMDB_Rating = ?, @Director = ?, @Plot_Summary = ?;\n",
    "                \"\"\"   \n",
    "\n",
    "cursor.execute(delete_table)\n",
    "\n",
    "for index, row in final_dataset.iterrows():\n",
    "\n",
    "    params = (row['Movie Title'], row['IMDB Rating'], row['Director'], row['Plot Summary'])\n",
    "    cursor.execute(insert_values, params)\n",
    "\n",
    "connStr.commit()\n",
    "\n",
    "cursor.close()\n",
    "\n",
    "connStr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_actors = final_dataset.loc[:, ['Movie Title', 'Actors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13364, 2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_actors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = column_actors.apply(lambda x: pd.Series(x['Actors']), axis=1).stack().reset_index(level=1, drop=True)\n",
    "\n",
    "s.name = 'Actors'\n",
    "\n",
    "actor_names = column_actors.drop('Actors', axis=1).join(s)\n",
    "\n",
    "actor_names['Actors'] = pd.Series(actor_names['Actors'], dtype=object)\n",
    "\n",
    "actor_names.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "connStr = pyodbc.connect(\"DRIVER={SQL Server Native Client 11.0};\"\n",
    "                         \"SERVER=GR2211336W2;\"\n",
    "                         \"DATABASE=Movies_Dataset;\"\n",
    "                         \"Trusted_Connection=yes\")\n",
    "\n",
    "cursor = connStr.cursor()\n",
    " \n",
    "delete_table =  \"\"\"           \n",
    "                        IF dbo.TableExists('data_table_two') = 1\n",
    "                             DELETE FROM data_table_two      \n",
    "                \"\"\"\n",
    "\n",
    "insert_values = \"\"\" \n",
    "                        EXEC [dbo].[store_actors] @Movie_Title = ?, @Actors = ?;\n",
    "                \"\"\"   \n",
    "\n",
    "cursor.execute(delete_table)\n",
    "\n",
    "for index, row in actor_names.iterrows():\n",
    "\n",
    "    params = (row['Movie Title'], row['Actors'])\n",
    "    cursor.execute(insert_values, params)\n",
    "\n",
    "connStr.commit()\n",
    "\n",
    "cursor.close()\n",
    "\n",
    "connStr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_plotkeywords = final_dataset.loc[:, ['Movie Title', 'Plot Keywords']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie Title</th>\n",
       "      <th>Plot Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avatar</td>\n",
       "      <td>avatar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avatar</td>\n",
       "      <td>future</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avatar</td>\n",
       "      <td>marine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avatar</td>\n",
       "      <td>native</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avatar</td>\n",
       "      <td>paraplegic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>goddess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>marriage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>ceremony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>marriage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>proposal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>pirate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spectre</td>\n",
       "      <td>bomb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spectre</td>\n",
       "      <td>espionage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spectre</td>\n",
       "      <td>sequel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Movie Title Plot Keywords\n",
       "0                                    Avatar        avatar\n",
       "0                                    Avatar        future\n",
       "0                                    Avatar        marine\n",
       "0                                    Avatar        native\n",
       "0                                    Avatar    paraplegic\n",
       "1  Pirates of the Caribbean: At World's End       goddess\n",
       "1  Pirates of the Caribbean: At World's End      marriage\n",
       "1  Pirates of the Caribbean: At World's End      ceremony\n",
       "1  Pirates of the Caribbean: At World's End      marriage\n",
       "1  Pirates of the Caribbean: At World's End      proposal\n",
       "1  Pirates of the Caribbean: At World's End        pirate\n",
       "1  Pirates of the Caribbean: At World's End     singapore\n",
       "2                                   Spectre          bomb\n",
       "2                                   Spectre     espionage\n",
       "2                                   Spectre        sequel"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = column_plotkeywords.apply(lambda x: pd.Series(x['Plot Keywords']), axis=1).stack().reset_index(level=1, drop=True)\n",
    "\n",
    "s.name = 'Plot Keywords'\n",
    "\n",
    "plot_keywords = column_plotkeywords.drop('Plot Keywords', axis=1).join(s)\n",
    "\n",
    "plot_keywords['Plot Keywords'] = pd.Series(plot_keywords['Plot Keywords'], dtype=object)\n",
    "\n",
    "plot_keywords.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "connStr = pyodbc.connect(\"DRIVER={SQL Server Native Client 11.0};\"\n",
    "                         \"SERVER=GR2211336W2;\"\n",
    "                         \"DATABASE=Movies_Dataset;\"\n",
    "                         \"Trusted_Connection=yes\")\n",
    "\n",
    "cursor = connStr.cursor()\n",
    " \n",
    "delete_table =  \"\"\"           \n",
    "                        IF dbo.TableExists('data_table_three') = 1\n",
    "                             DELETE FROM data_table_three      \n",
    "                \"\"\"\n",
    "\n",
    "insert_values = \"\"\" \n",
    "                        EXEC [dbo].[store_keywords] @Movie_Title = ?, @Plot_Keywords = ?;\n",
    "                \"\"\"   \n",
    "\n",
    "cursor.execute(delete_table)\n",
    "\n",
    "for index, row in plot_keywords.iterrows():\n",
    "\n",
    "    params = (row['Movie Title'], row['Plot Keywords'])\n",
    "    cursor.execute(insert_values, params)\n",
    "\n",
    "connStr.commit()\n",
    "\n",
    "cursor.close()\n",
    "\n",
    "connStr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_genres = final_dataset.loc[:, ['Movie Title', 'Genres']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avatar</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avatar</td>\n",
       "      <td>Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avatar</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spectre</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spectre</td>\n",
       "      <td>Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spectre</td>\n",
       "      <td>Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Dark Knight Rises</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Dark Knight Rises</td>\n",
       "      <td>Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Dark Knight Rises</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>John Carter</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>John Carter</td>\n",
       "      <td>Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>John Carter</td>\n",
       "      <td>Sci-Fi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Movie Title     Genres\n",
       "0                                    Avatar     Action\n",
       "0                                    Avatar  Adventure\n",
       "0                                    Avatar    Fantasy\n",
       "1  Pirates of the Caribbean: At World's End     Action\n",
       "1  Pirates of the Caribbean: At World's End  Adventure\n",
       "1  Pirates of the Caribbean: At World's End    Fantasy\n",
       "2                                   Spectre     Action\n",
       "2                                   Spectre  Adventure\n",
       "2                                   Spectre   Thriller\n",
       "3                     The Dark Knight Rises     Action\n",
       "3                     The Dark Knight Rises   Thriller\n",
       "3                     The Dark Knight Rises          0\n",
       "4                               John Carter     Action\n",
       "4                               John Carter  Adventure\n",
       "4                               John Carter     Sci-Fi"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = column_genres.apply(lambda x: pd.Series(x['Genres']), axis=1).stack().reset_index(level=1, drop=True)\n",
    "\n",
    "s.name = 'Genres'\n",
    "\n",
    "genres = column_genres.drop('Genres', axis=1).join(s)\n",
    "\n",
    "genres['Genres'] = pd.Series(genres['Genres'], dtype=object)\n",
    "\n",
    "genres.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "connStr = pyodbc.connect(\"DRIVER={SQL Server Native Client 11.0};\"\n",
    "                         \"SERVER=GR2211336W2;\"\n",
    "                         \"DATABASE=Movies_Dataset;\"\n",
    "                         \"Trusted_Connection=yes\")\n",
    "\n",
    "cursor = connStr.cursor()\n",
    " \n",
    "delete_table =  \"\"\"           \n",
    "                        IF dbo.TableExists('data_table_four') = 1\n",
    "                             DELETE FROM data_table_four      \n",
    "                \"\"\"\n",
    "\n",
    "insert_values = \"\"\" \n",
    "                        EXEC [dbo].[store_genres] @Movie_Title = ?, @Genres = ?;\n",
    "                \"\"\"   \n",
    "\n",
    "cursor.execute(delete_table)\n",
    "\n",
    "for index, row in genres.iterrows():\n",
    "\n",
    "    params = (row['Movie Title'], row['Genres'])\n",
    "    cursor.execute(insert_values, params)\n",
    "\n",
    "connStr.commit()\n",
    "\n",
    "cursor.close()\n",
    "\n",
    "connStr.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of Part 1 - Update, clean & transfrom the dataset of movies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
