{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 - Load the chosen model & Extract its embeddings (latest changes on 08.03.2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cleaning and preparing the dataset\n",
    "# -> dataframe manipulation\n",
    "# -> text manipulation\n",
    "# -> Web Scrapping\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tabulate import tabulate\n",
    "import re\n",
    "import os\n",
    "\n",
    "import random\n",
    "\n",
    "# Module to serialize the content produced from the execution of the code\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Module to monitor the progress of a python for loop\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# Module to manipulate text in python - NLTK package\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "# Module to compute word vectorizers and compute the cosine distance\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "import string\n",
    "import itertools\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:70% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from time import time\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "\n",
    "import pydot\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "from IPython.display import SVG\n",
    "from tensorflow.keras.utils import model_to_dot\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import json\n",
    "\n",
    "# Import ML FLow\n",
    "import mlflow.tensorflow\n",
    "import mlflow.pyfunc\n",
    "from tensorflow.keras import regularizers\n",
    "import datetime\n",
    "\n",
    "# Import TensorBoard\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots as tfplots\n",
    "import tensorflow_docs.modeling as tfmodel\n",
    "from tensorflow.keras import regularizers\n",
    "# from tensorboard import default\n",
    "# from tensorboard import program\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "from bert import tokenization\n",
    "from bert.tokenization import FullTokenizer\n",
    "\n",
    "#Visualize Model\n",
    "\n",
    "def visualize_model(model):\n",
    "    return SVG(model_to_dot(model, show_shapes= True, show_layer_names=True, dpi=65).create(prog='dot', format='svg'))\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from packaging import version\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the dataset from part 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(os.path.join(os.getcwd(), \"pickled_data_per_part\\\\dataset_part_3.1_06032020.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the already trained chosen model\n",
    "This is the model that perfomed better than those trained on part 3. <br>\n",
    "<i> The chosen model is the \"Multi-input keras model\". <i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "# Load the already trained chosen model\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(\"\\nLoad the chosen trained model\")\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'model_one\\\\model_multy_input.json'),'r') as f:\n",
    "    model_json = json.load(f)\n",
    "\n",
    "model = model_from_json(model_json)\n",
    "\n",
    "model.load_weights(os.path.join(os.getcwd(), 'model_one\\\\model_multy_input.h5'))\n",
    "\n",
    "print(type(model))\n",
    "print(\"\\nModel is loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the saved tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IMport the tokenizers of each input, fitted on part 3.1\n",
    "\"\"\"\n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\actors_tokenizer_06032020.pkl'),'rb') as f:\n",
    "    actors_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\plot_tokenizer_06032020.pkl'),'rb') as f:\n",
    "    plot_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\features_tokenizer_06032020.pkl'),'rb') as f:\n",
    "    features_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\reviews_tokenizer_06032020.pkl'),'rb') as f:\n",
    "    reviews_tokenizer = pickle.load(f)\n",
    "    \n",
    "print(\"Tokenizers are loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two Fucntions have been assembled to complete the word embeddings extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_embeddings(variable, model, tokenizer):\n",
    "    \n",
    "    if variable == \"actors\":\n",
    "        \n",
    "        embeddings_layer = model.layers[4].get_weights()[0]\n",
    "\n",
    "        word_embeddings = {w:embeddings_layer[idx] for w, idx in tokenizer.word_index.items() if idx <= 16333+1}\n",
    "        \n",
    "        print(\"\\nActor's word embeddings length: {}\\n\".format(embeddings_layer.shape))\n",
    "        \n",
    "    elif variable == \"plot\":\n",
    "        \n",
    "        embeddings_layer = model.layers[5].get_weights()[0]\n",
    "\n",
    "        word_embeddings = {w:embeddings_layer[idx] for w, idx in tokenizer.word_index.items() if idx <= 10083+1}\n",
    "        print(\"Plot Summary's word embeddings length: {}\\n\".format(embeddings_layer.shape))\n",
    "        \n",
    "    elif variable == \"features\":\n",
    "        \n",
    "        embeddings_layer = model.layers[6].get_weights()[0]\n",
    "\n",
    "        word_embeddings = {w:embeddings_layer[idx] for w, idx in tokenizer.word_index.items() if idx <= 14439+1}\n",
    "        print(\"Movie's Features word embeddings length: {}\\n\".format(embeddings_layer.shape))\n",
    "        \n",
    "    elif variable == \"reviews\":\n",
    "        \n",
    "        embeddings_layer = model.layers[7].get_weights()[0]\n",
    "\n",
    "        word_embeddings = {w:embeddings_layer[idx] for w, idx in tokenizer.word_index.items() if idx <= 15250+1}\n",
    "        print(\"Movie's Reviews word embeddings length: {}\\n\".format(embeddings_layer.shape))\n",
    "        \n",
    "    return embeddings_layer, word_embeddings\n",
    "\n",
    "def assign_word_embeddings(variable, dataset, word_embeddings):\n",
    "    \n",
    "    if variable == \"actors\":\n",
    "    \n",
    "        average_vector_list_cast = []\n",
    "\n",
    "        min_vector_list_cast = []\n",
    "\n",
    "        max_vector_list_cast = []\n",
    "\n",
    "        actors_embeddings_list = []\n",
    "\n",
    "        for i in range(len(dataset.index)):\n",
    "            \n",
    "            actors = dataset.loc[:, \"clean_actors\"].iloc[i].split(\",\")\n",
    "    \n",
    "            assert [word.islower() for word in actors] # assert that all actors are present in lower case\n",
    "    \n",
    "            actors_embeddings_list.append([word_embeddings[word] if word in word_embeddings else word_embeddings['<OOV>'] for word in actors])\n",
    "    \n",
    "        dataset.loc[:, 'actors_embeddings_list'] = actors_embeddings_list\n",
    "\n",
    "        for i in range(len(dataset.index)):\n",
    "            \n",
    "            actor_embeddings = dataset[\"actors_embeddings_list\"].iloc[i]\n",
    "    \n",
    "            minimum = np.min([element for element in actor_embeddings], axis=0)\n",
    "            maximum = np.max([element for element in actor_embeddings], axis=0)\n",
    "            average = np.mean([element for element in actor_embeddings], axis=0)\n",
    "    \n",
    "            min_vector_list_cast.append(minimum)\n",
    "            max_vector_list_cast.append(maximum)\n",
    "            average_vector_list_cast.append(average)\n",
    "\n",
    "        dataset['minimum_cast_vectors'] = min_vector_list_cast\n",
    "        dataset['maximum_cast_vectors'] = max_vector_list_cast\n",
    "        dataset['average_cast_vectors'] = average_vector_list_cast\n",
    "        \n",
    "    elif variable == \"plot\":\n",
    "    \n",
    "        average_vector_list_plot = []\n",
    "\n",
    "        min_vector_list_plot = []\n",
    "\n",
    "        max_vector_list_plot = []\n",
    "\n",
    "        plot_embeddings_list = []\n",
    "\n",
    "        for i in range(len(dataset.index)):\n",
    "            \n",
    "            plot = dataset[\"clean_plot_summary\"].iloc[i]\n",
    "    \n",
    "            plot_split = plot.split(' ')\n",
    "    \n",
    "            assert [word.islower() for word in plot_split]\n",
    "    \n",
    "            plot_embeddings_list.append([word_embeddings[word] if word in word_embeddings else word_embeddings['<OOV>'] for word in plot_split])\n",
    "    \n",
    "        dataset['plot_embeddings_list'] = plot_embeddings_list\n",
    "\n",
    "        for i in range(len(dataset.index)):\n",
    "            \n",
    "            plot_embeddings = dataset[\"plot_embeddings_list\"].iloc[i]\n",
    "    \n",
    "            minimum = np.min([element for element in plot_embeddings], axis=0)\n",
    "            maximum = np.max([element for element in plot_embeddings], axis=0)\n",
    "            average = np.mean([element for element in plot_embeddings], axis=0)\n",
    "    \n",
    "            min_vector_list_plot.append(minimum)\n",
    "            max_vector_list_plot.append(maximum)\n",
    "            average_vector_list_plot.append(average)\n",
    "\n",
    "        dataset['minimum_plot_vectors'] = min_vector_list_plot\n",
    "        dataset['maximum_plot_vectors'] = max_vector_list_plot\n",
    "        dataset['average_plot_vectors'] = average_vector_list_plot\n",
    "        \n",
    "    elif variable == \"features\":\n",
    "    \n",
    "        average_vector_list_combined_features = []\n",
    "\n",
    "        min_vector_list_combined_features = []\n",
    "\n",
    "        max_vector_list_combined_features = []\n",
    "\n",
    "        combined_features_embeddings_list = []\n",
    "\n",
    "        for i in range(len(dataset.index)):\n",
    "    \n",
    "            combined_features = dataset[\"clean_combined_features\"].iloc[i]\n",
    "    \n",
    "            combined_features_split = combined_features.split(' ')\n",
    "    \n",
    "            assert [word.islower() for word in combined_features_split]\n",
    "    \n",
    "            combined_features_embeddings_list.append([word_embeddings[word] if word in word_embeddings else word_embeddings['<OOV>'] for word in combined_features_split])\n",
    "    \n",
    "        dataset['combined_features_embeddings_list'] = combined_features_embeddings_list\n",
    "\n",
    "        for i in range(len(dataset.index)):\n",
    "            \n",
    "            combined_features_embeddings = dataset[\"combined_features_embeddings_list\"].iloc[i]\n",
    "    \n",
    "            minimum = np.min([element for element in combined_features_embeddings], axis=0)\n",
    "            maximum = np.max([element for element in combined_features_embeddings], axis=0)\n",
    "            average = np.mean([element for element in combined_features_embeddings], axis=0)\n",
    "    \n",
    "            min_vector_list_combined_features.append(minimum)\n",
    "            max_vector_list_combined_features.append(maximum)\n",
    "            average_vector_list_combined_features.append(average)\n",
    "\n",
    "        dataset['minimum_combined_features_vectors'] = min_vector_list_combined_features\n",
    "        dataset['maximum_combined_features_vectors'] = max_vector_list_combined_features\n",
    "        dataset['average_combined_features_vectors'] = average_vector_list_combined_features\n",
    "        \n",
    "    elif variable == \"reviews\":\n",
    "    \n",
    "        average_vector_list_reviews = []\n",
    "\n",
    "        min_vector_list_reviews = []\n",
    "\n",
    "        max_vector_list_reviews = []\n",
    "\n",
    "        reviews_embeddings_list = []\n",
    "\n",
    "        for i in range(len(dataset.index)):\n",
    "    \n",
    "            reviews = dataset[\"clean_reviews\"].iloc[i]\n",
    "    \n",
    "            reviews_split = reviews.split(' ')\n",
    "    \n",
    "            assert [word.islower() for word in reviews_split]\n",
    "    \n",
    "            reviews_embeddings_list.append([word_embeddings[word] if word in word_embeddings else word_embeddings['<OOV>'] for word in reviews_split])\n",
    "    \n",
    "        dataset['reviews_embeddings_list'] = reviews_embeddings_list\n",
    "\n",
    "        for i in range(len(dataset.index)):\n",
    "            \n",
    "            reviews_embeddings = dataset[\"reviews_embeddings_list\"].iloc[i]\n",
    "    \n",
    "            minimum = np.min([element for element in reviews_embeddings], axis=0)\n",
    "            maximum = np.max([element for element in reviews_embeddings], axis=0)\n",
    "            average = np.mean([element for element in reviews_embeddings], axis=0)\n",
    "    \n",
    "            min_vector_list_reviews.append(minimum)\n",
    "            max_vector_list_reviews.append(maximum)\n",
    "            average_vector_list_reviews.append(average)\n",
    "\n",
    "        dataset['minimum_reviews_vectors'] = min_vector_list_reviews\n",
    "        dataset['maximum_reviews_vectors'] = max_vector_list_reviews\n",
    "        dataset['average_reviews_vectors'] = average_vector_list_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Extract the word embeddings\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(\"\\nExtract the word embeddings\")\n",
    "\n",
    "actors_embedding_layer, word_embeddings_actors = extract_word_embeddings(\"actors\", model, actors_tokenizer)\n",
    "print(\"\\nWord embeddings for actors extracted\\n\")\n",
    "\n",
    "plot_embedding_layer, word_embeddings_plot = extract_word_embeddings(\"plot\", model, plot_tokenizer)\n",
    "print(\"\\nWord embeddings for plot summary extracted\\n\")\n",
    "\n",
    "features_embedding_layer, word_embeddings_features = extract_word_embeddings(\"features\", model, features_tokenizer)\n",
    "print(\"\\nWord embeddings for movie features extracted\\n\")\n",
    "\n",
    "reviews_embedding_layer, word_embeddings_reviews = extract_word_embeddings(\"reviews\", model, reviews_tokenizer)\n",
    "print(\"\\nWord embeddings for movie reviews extracted\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_embedding_layer = features_embedding_layer / np.linalg.norm(features_embedding_layer, axis = 1).reshape((-1, 1))\n",
    "features_embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_embedding_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dim(weights, components = 3, method = 'tsne'):\n",
    "    \"\"\"Reduce dimensions of embeddings\"\"\"\n",
    "    if method == 'tsne':\n",
    "        return TSNE(components, metric = 'cosine').fit_transform(weights)\n",
    "    elif method == 'umap':\n",
    "        # Might want to try different parameters for UMAP\n",
    "        return UMAP(n_components=components, metric = 'cosine', \n",
    "                    init = 'random', n_neighbors = 5).fit_transform(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_features = reduce_dim(features_embedding_layer, components = 2, method = 'tsne')\n",
    "movie_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres=dataset.columns[13:30].tolist()\n",
    "ints, gen = pd.factorize(genres)\n",
    "gen[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_include = [range(14441)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "\n",
    "# Plot embedding\n",
    "plt.scatter(movie_features[idx_include, 0], movie_features[idx_include, 1], \n",
    "            c = ints, cmap = plt.cm.tab10)\n",
    "\n",
    "# Add colorbar and appropriate labels\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_ticks([])\n",
    "for j, lab in enumerate(gen):\n",
    "    cbar.ax.text(1, (2 * j + 1) / ((10) * 2), lab, ha='left', va='center')\n",
    "cbar.ax.set_title('Genre', loc = 'left')\n",
    "\n",
    "\n",
    "plt.xlabel('TSNE 1'); plt.ylabel('TSNE 2'); plt.title('TSNE Visualization of Book Embeddings');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "# Assign the word embeddings to each different actor\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(\"\\nAssign the word embeddings to each different word\")\n",
    "\n",
    "print(\"Assign word embeddings to actors\")\n",
    "assign_word_embeddings(\"actors\", dataset, word_embeddings_actors)\n",
    "\n",
    "print(\"Assign word embeddings to plot summary\")\n",
    "assign_word_embeddings(\"plot\", dataset, word_embeddings_plot)\n",
    "\n",
    "print(\"Assign word embeddings to movie features\")\n",
    "assign_word_embeddings(\"features\", dataset, word_embeddings_features)\n",
    "\n",
    "print(\"Assign word embeddings to movie reviews\")\n",
    "assign_word_embeddings(\"reviews\", dataset, word_embeddings_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nThe word embedding vector of the actor 'tobey maguire' is:\\n\\n\", word_embeddings_actors['tobey maguire'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "keras_embeddings_array_cast = np.hstack([dataset['average_cast_vectors'].apply(pd.Series).values,\n",
    "                                         dataset['minimum_cast_vectors'].apply(pd.Series).values,\n",
    "                                         dataset['maximum_cast_vectors'].apply(pd.Series).values])\n",
    "\n",
    "keras_embeddings_array_plot = np.hstack([dataset['average_plot_vectors'].apply(pd.Series).values,\n",
    "                                         dataset['minimum_plot_vectors'].apply(pd.Series).values,\n",
    "                                         dataset['maximum_plot_vectors'].apply(pd.Series).values])\n",
    "\n",
    "keras_embeddings_array_combined_features = np.hstack([dataset['average_combined_features_vectors'].apply(pd.Series).values,\n",
    "                                                      dataset['minimum_combined_features_vectors'].apply(pd.Series).values,\n",
    "                                                      dataset['maximum_combined_features_vectors'].apply(pd.Series).values])\n",
    "\n",
    "keras_embeddings_array_reviews = np.hstack([dataset['average_reviews_vectors'].apply(pd.Series).values,\n",
    "                                            dataset['minimum_reviews_vectors'].apply(pd.Series).values,\n",
    "                                            dataset['maximum_reviews_vectors'].apply(pd.Series).values])\n",
    "\n",
    "keras_embeddings_array_cast_plot_combined_features_reviews = np.hstack([keras_embeddings_array_cast, \n",
    "                                                                        keras_embeddings_array_plot, \n",
    "                                                                        keras_embeddings_array_combined_features,\n",
    "                                                                        keras_embeddings_array_reviews])\n",
    "\n",
    "print(\"Shape of the Actors embeddings: {}\".format(keras_embeddings_array_cast.shape))\n",
    "print(\"\\nShape of the Plot Summary embeddings: {}\".format(keras_embeddings_array_plot.shape))\n",
    "print(\"\\nShape of the Combined Features embeddings: {}\".format(keras_embeddings_array_combined_features.shape))\n",
    "print(\"\\nShape of the Reviews embeddings: {}\".format(keras_embeddings_array_reviews.shape))\n",
    "print(\"\\nShape of the concatenated embeddings(cast, plot, combined features): {}\".format(keras_embeddings_array_cast_plot_combined_features_reviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Since the chosen model is the \"Model_1: Multi-Input Keras Model\", we saved the relevant word embeddings to the folder \"model_one\"\n",
    "\"\"\"\n",
    "with open(os.path.join(os.getcwd(), 'model_one\\\\keras_embeddings_array_cast_08032020.pkl'), 'wb') as f:\n",
    "    pickle.dump(keras_embeddings_array_cast, f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), 'model_one\\\\keras_embeddings_array_plot_08032020.pkl'), 'wb') as f:\n",
    "    pickle.dump(keras_embeddings_array_plot, f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), 'model_one\\\\keras_embeddings_array_combined_features_08032020.pkl'), 'wb') as f:\n",
    "    pickle.dump(keras_embeddings_array_combined_features, f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), 'model_one\\\\keras_embeddings_array_reviews_08032020.pkl'), 'wb') as f:\n",
    "    pickle.dump(keras_embeddings_array_combined_features, f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), 'model_one\\\\keras_embeddings_array_concatenated_08032020.pkl'), 'wb') as f:\n",
    "    pickle.dump(keras_embeddings_array_cast_plot_combined_features_reviews, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle the dataset with the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pickle(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\dataset_part_4_08032020.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END OF Part 4 - Load the chosen model & Extract Word Embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
