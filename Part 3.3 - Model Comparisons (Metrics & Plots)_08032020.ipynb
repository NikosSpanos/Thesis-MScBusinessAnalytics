{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.3: Model comparisons (latest changes on 08.03.2020)\n",
    "\n",
    "* Classification report -> (dataframe)\n",
    "* Metrics (Accuracy, Validation_loss) -> (dataframe)\n",
    "* Metrics per epoch -> (plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cleaning and preparing the dataset\n",
    "# -> dataframe manipulation\n",
    "# -> text manipulation\n",
    "# -> Web Scrapping\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tabulate import tabulate\n",
    "import re\n",
    "import os\n",
    "\n",
    "import random\n",
    "\n",
    "# Module to serialize the content produced from the execution of the code\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Module to monitor the progress of a python for loop\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# Module to manipulate text in python - NLTK package\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Module to compute word vectorizers and compute the cosine distance\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:70% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Text Classification (For creating the word embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from time import time\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "\n",
    "import pydot\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "from IPython.display import SVG\n",
    "from tensorflow.keras.utils import model_to_dot\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import json\n",
    "\n",
    "# Import ML FLow\n",
    "import mlflow.tensorflow\n",
    "import mlflow.pyfunc\n",
    "from tensorflow.keras import regularizers\n",
    "import datetime\n",
    "\n",
    "# Import TensorBoard\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots as tfplots\n",
    "import tensorflow_docs.modeling as tfmodel\n",
    "from tensorflow.keras import regularizers\n",
    "# from tensorboard import default\n",
    "# from tensorboard import program\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "# from bert import tokenization\n",
    "# from bert.tokenization import FullTokenizer\n",
    "\n",
    "#Visualize Model\n",
    "\n",
    "def visualize_model(model):\n",
    "    return SVG(model_to_dot(model, show_shapes= True, show_layer_names=True, dpi=65).create(prog='dot', format='svg'))\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from packaging import version\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #1 Import & compare each different model's score-dataframe\n",
    "\n",
    "* Test Accuracy\n",
    "* Test Loss\n",
    "* Hamming Loss\n",
    "* Zero on Loss\n",
    "* F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1: Multi-input\n",
    "dataframe_1=pd.read_pickle(os.path.join(os.getcwd(), \"model_one\\\\df_metrics_multy_input_keras_08032020.pkl\"))\n",
    "\n",
    "# model 2: GloVe\n",
    "dataframe_2=pd.read_pickle(os.path.join(os.getcwd(), \"model_two\\\\df_metrics_glove_embeddings_08032020.pkl\"))\n",
    "\n",
    "# model 3: Google news 130GB (without OOV tokens)\n",
    "dataframe_3=pd.read_pickle(os.path.join(os.getcwd(), \"model_three\\\\df_metrics_google_news_130_without_OOV_tokens_08032020.pkl\"))\n",
    "\n",
    "# model 4: Google news 130GB (with OOV tokens)\n",
    "dataframe_4=pd.read_pickle(os.path.join(os.getcwd(), \"model_four\\\\df_metrics_google_news_130_with_OOV_tokens_08032020.pkl\"))\n",
    "\n",
    "# model 5: Google news 7B\n",
    "dataframe_5=pd.read_pickle(os.path.join(os.getcwd(), \"model_five\\\\df_metrics_english_google_news_7b_corpus_08032020.pkl\"))\n",
    "\n",
    "#model 6: Google news 200B\n",
    "dataframe_6=pd.read_pickle(os.path.join(os.getcwd(), \"model_six\\\\df_metrics_english_google_news_7b_corpus_08032020.pkl\"))\n",
    "\n",
    "#model 7: Universal Sentence Encoder\n",
    "dataframe_7=pd.read_pickle(os.path.join(os.getcwd(), \"model_seven\\\\df_metrics_universal_sentence_encoder_08032020.pkl\"))\n",
    "\n",
    "#model 8:\n",
    "#To be done\n",
    "\n",
    "#model 9:\n",
    "#To be done\n",
    "\n",
    "#Union the dataframes!\n",
    "dataframe_union = dataframe_1.append([dataframe_2, dataframe_3, dataframe_4, dataframe_5, dataframe_6, dataframe_7], ignore_index=True)\n",
    "\n",
    "dataframe_union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #2 Import & plot each different model's accuracy-loss per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1: Multi-input\n",
    "metrics_dataframe_1=pd.read_pickle(os.path.join(os.getcwd(), \"model_one\\\\metrics_histogram_multi_input_keras.pkl\"))\n",
    "\n",
    "# model 2: GloVe\n",
    "metrics_dataframe_2=pd.read_pickle(os.path.join(os.getcwd(), \"model_two\\\\metrics_histogram_glove_embeddings.pkl\"))\n",
    "\n",
    "# model 3: Google news 130GB (without OOV tokens)\n",
    "metrics_dataframe_3=pd.read_pickle(os.path.join(os.getcwd(), \"model_three\\\\metrics_histogram_english_google_news_without_oovtokens_08032020.pkl\"))\n",
    "\n",
    "# model 4: Google news 130GB (with OOV tokens)\n",
    "metrics_dataframe_4=pd.read_pickle(os.path.join(os.getcwd(), \"model_four\\\\metrics_histogram_english_google_news_with_oovtokens_08032020.pkl\"))\n",
    "\n",
    "# model 5: Google news 7B\n",
    "metrics_dataframe_5=pd.read_pickle(os.path.join(os.getcwd(), \"model_five\\\\metrics_histogram_english_google_news_7b_corpus_08032020.pkl\"))\n",
    "\n",
    "#model 6: Google news 200B\n",
    "metrics_dataframe_6=pd.read_pickle(os.path.join(os.getcwd(), \"model_six\\\\metrics_histogram_english_google_news_200b_corpus_08032020.pkl\"))\n",
    "\n",
    "#model 7: Universal Sentence Encoder\n",
    "metrics_dataframe_6=pd.read_pickle(os.path.join(os.getcwd(), \"model_seven\\\\metrics_histogram_universal_sentence_encoder_08032020.pkl\"))\n",
    "\n",
    "#model 8:\n",
    "#To be done\n",
    "\n",
    "#model 9:\n",
    "#To be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dataframe_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Val_loss\n",
    "\n",
    "fig=px.line()\n",
    "fig.add_scatter(x=metrics_dataframe_1[\"epoch\"], y=metrics_dataframe_1[\"val_loss\"], mode=\"lines+markers\", name='model 1: multi-input keras')\n",
    "fig.add_scatter(x=metrics_dataframe_3[\"epoch\"], y=metrics_dataframe_3[\"val_loss\"], mode=\"lines+markers\", name='model 3: google news 130GB (no OOV tokens)')\n",
    "fig.add_scatter(x=metrics_dataframe_4[\"epoch\"], y=metrics_dataframe_4[\"val_loss\"], mode=\"lines+markers\", name='model 4: google news 130GB (with OOV tokens)')\n",
    "fig.add_scatter(x=metrics_dataframe_5[\"epoch\"], y=metrics_dataframe_5[\"val_loss\"], mode=\"lines+markers\", name='model 5: google news 7B')\n",
    "fig.add_scatter(x=metrics_dataframe_6[\"epoch\"], y=metrics_dataframe_6[\"val_loss\"], mode=\"lines+markers\", name='model 6: google news 200B (no OOV tokens)')\n",
    "\n",
    "fig.update_layout(title='Loss Function comparison (per model)',\n",
    "                   xaxis_title='Epoch',\n",
    "                   yaxis_title='Test Loss (model loss)',\n",
    "                   legend_title='<b> Model name </b>',\n",
    "                   legend_orientation=\"h\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Val_accuracy\n",
    "\n",
    "fig=px.line()\n",
    "fig.add_scatter(x=metrics_dataframe_1[\"epoch\"], y=metrics_dataframe_1[\"val_accuracy\"], mode=\"lines+markers\", name='model 1: multi-input keras')\n",
    "fig.add_scatter(x=metrics_dataframe_3[\"epoch\"], y=metrics_dataframe_3[\"val_accuracy\"], mode=\"lines+markers\", name='model 3: google news 130GB (no OOV tokens)')\n",
    "fig.add_scatter(x=metrics_dataframe_4[\"epoch\"], y=metrics_dataframe_4[\"val_accuracy\"], mode=\"lines+markers\", name='model 4: google news 130GB (with OOV tokens)')\n",
    "fig.add_scatter(x=metrics_dataframe_5[\"epoch\"], y=metrics_dataframe_5[\"val_accuracy\"], mode=\"lines+markers\", name='model 5: google news 7B')\n",
    "fig.add_scatter(x=metrics_dataframe_6[\"epoch\"], y=metrics_dataframe_6[\"val_accuracy\"], mode=\"lines+markers\", name='model 6: google news 200B (no OOV tokens)')\n",
    "\n",
    "fig.update_layout(title='Accuracy comparison (per model)',\n",
    "                   xaxis_title='Epoch',\n",
    "                   yaxis_title='Test Accuracy (model metric)',\n",
    "                   legend_title='<b> Model name </b>',\n",
    "                   legend_orientation=\"h\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #3 Import & compare each different model's classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model 2: GloVe\n",
    "#To be Done\n",
    "\n",
    "# model 1: Multi-input\n",
    "y_predictions_model_1=np.load(os.path.join(os.getcwd(), \"model_one\\\\y_predictions_multi_input_keras_07032020.npy\"))\n",
    "y_true_model_1=np.load(os.path.join(os.getcwd(), \"model_one\\\\y_true_multi_input_keras_07032020.npy\"))\n",
    "classification_table_1= classification_report(y_true=y_true_model_1, y_pred=y_predictions_model_1)\n",
    "print(\"Model 1: Multi-input Keras\\n\")\n",
    "print(classification_table_1)\n",
    "\n",
    "# model 2: Glove Embeddings\n",
    "y_predictions_model_2=np.load(os.path.join(os.getcwd(), \"model_two\\\\y_predictions_glove_embeddings_08032020.npy\"))\n",
    "y_true_model_2=np.load(os.path.join(os.getcwd(), \"model_two\\\\y_true_glove_embeddings_08032020.npy\"))\n",
    "classification_table_2= classification_report(y_true=y_true_model_2, y_pred=y_predictions_model_2)\n",
    "print(\"\\nModel 2: Glove Embeddings\\n\")\n",
    "print(classification_table_2)\n",
    "\n",
    "# model 3: Google news 130GB (without OOV tokens)\n",
    "y_predictions_model_3=np.load(os.path.join(os.getcwd(), \"model_three\\\\y_predictions_english_google_news_without_oovtokens_08032020.npy\"))\n",
    "y_true_model_3=np.load(os.path.join(os.getcwd(), \"model_three\\\\y_true_english_google_news_without_oovtokens_08032020.npy\"))\n",
    "classification_table_3= classification_report(y_true=y_true_model_3, y_pred=y_predictions_model_3)\n",
    "print(\"\\nModel 3: Google news 130GB (without OOV tokens)\\n\")\n",
    "print(classification_table_3)\n",
    "\n",
    "# model 4: Google news 130GB (with OOV tokens)\n",
    "y_predictions_model_4=np.load(os.path.join(os.getcwd(), \"model_four\\\\y_predictions_english_google_news_with_oovtokens_08032020.npy\"))\n",
    "y_true_model_4=np.load(os.path.join(os.getcwd(), \"model_four\\\\y_true_english_google_news_with_oovtokens_08032020.npy\"))\n",
    "classification_table_4= classification_report(y_true=y_true_model_4, y_pred=y_predictions_model_4)\n",
    "print(\"\\nModel 4: Google news 130GB (with OOV tokens)\\n\")\n",
    "print(classification_table_4)\n",
    "\n",
    "# model 5: Google news 7B\n",
    "y_predictions_model_5=np.load(os.path.join(os.getcwd(), \"model_five\\\\y_predictions_english_google_news_7b_corpus_08032020.npy\"))\n",
    "y_true_model_5=np.load(os.path.join(os.getcwd(), \"model_five\\\\y_true_english_google_news_7b_corpus_08032020.npy\"))\n",
    "classification_table_5= classification_report(y_true=y_true_model_5, y_pred=y_predictions_model_5)\n",
    "print(\"\\nModel 5: Google news 7B\\n\")\n",
    "print(classification_table_5)\n",
    "\n",
    "#model 6: Google news 200B\n",
    "y_predictions_model_6=np.load(os.path.join(os.getcwd(), \"model_six\\\\y_predictions_english_google_news_200b_corpus_08032020.npy\"))\n",
    "y_true_model_6=np.load(os.path.join(os.getcwd(), \"model_six\\\\y_true_english_google_news_200b_corpus_08032020.npy\"))\n",
    "classification_table_6= classification_report(y_true=y_true_model_6, y_pred=y_predictions_model_6)\n",
    "print(\"\\nModel 6: Google news 200B\\n\")\n",
    "print(classification_table_6)\n",
    "\n",
    "#model 7: Universal Sentence Encoder\n",
    "y_predictions_model_7=np.load(os.path.join(os.getcwd(), \"model_seven\\\\y_predictions_universal_sentence_encoder_08032020.npy\"))\n",
    "y_true_model_7=np.load(os.path.join(os.getcwd(), \"model_seven\\\\y_true_universal_sentence_encoder_08032020.npy\"))\n",
    "classification_table_7= classification_report(y_true=y_true_model_7, y_pred=y_predictions_model_7)\n",
    "print(\"\\nModel 7: Universal Sentence Encoder\\n\")\n",
    "print(classification_table_7)\n",
    "\n",
    "#model 8:\n",
    "#To be done\n",
    "\n",
    "#model 9:\n",
    "#To be done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
