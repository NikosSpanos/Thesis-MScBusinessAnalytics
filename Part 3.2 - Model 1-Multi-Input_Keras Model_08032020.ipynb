{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.2 - Model 1: Multi-Input Keras neural model (latest changes on 08.03.2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import standard libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tabulate import tabulate\n",
    "import re\n",
    "import os\n",
    "\n",
    "import random\n",
    "\n",
    "# Module to serialize the content produced from the execution of the code\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Module to monitor the progress of a python for loop\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# Module to manipulate text in python - NLTK package\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Module to compute word vectorizers and compute the cosine distance\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "import string\n",
    "import itertools\n",
    "\n",
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:70% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Text Classification (For creating the word embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from time import time\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "\n",
    "import pydot\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "from IPython.display import SVG\n",
    "from tensorflow.keras.utils import model_to_dot\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import json\n",
    "\n",
    "# Import ML FLow\n",
    "import mlflow.tensorflow\n",
    "import mlflow.pyfunc\n",
    "from tensorflow.keras import regularizers\n",
    "import datetime\n",
    "\n",
    "# Import TensorBoard\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots as tfplots\n",
    "import tensorflow_docs.modeling as tfmodel\n",
    "from tensorflow.keras import regularizers\n",
    "# from tensorboard import default\n",
    "# from tensorboard import program\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "from bert import tokenization\n",
    "from bert.tokenization import FullTokenizer\n",
    "\n",
    "#Visualize Model\n",
    "\n",
    "def visualize_model(model):\n",
    "    return SVG(model_to_dot(model, show_shapes= True, show_layer_names=True, dpi=65).create(prog='dot', format='svg'))\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import hamming_loss, zero_one_loss, f1_score\n",
    "\n",
    "from packaging import version\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the data already tokenized and transformed from Part 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 80-20 split - Non-balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq_actors=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\x_train_seq_actors_80-20_non-balanced_06032020.npy\"))\n",
    "X_train_seq_plot=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\x_train_seq_plot_80-20_non-balanced_06032020.npy\"))\n",
    "X_train_seq_features=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\x_train_seq_features_80-20_non-balanced_06032020.npy\"))\n",
    "X_train_seq_reviews=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\x_train_seq_reviews_80-20_non-balanced_06032020.npy\"))\n",
    "\n",
    "print(\"X_train data inputs have been loaded!\\n\")\n",
    "\n",
    "X_test_seq_actors=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\x_test_seq_actors_80-20_non-balanced_06032020.npy\"))\n",
    "X_test_seq_plot=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\x_test_seq_plot_80-20_non-balanced_06032020.npy\"))\n",
    "X_test_seq_features=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\x_test_seq_features_80-20_non-balanced_06032020.npy\"))\n",
    "X_test_seq_reviews=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\x_test_seq_reviews_80-20_non-balanced_06032020.npy\"))\n",
    "\n",
    "print(\"X_test data inputs have been loaded!\\n\")\n",
    "\n",
    "y_train=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\y_train_80-20_non-balanced_06032020.npy\"))\n",
    "y_test=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\y_test_80-20_non-balanced_06032020.npy\"))\n",
    "\n",
    "print(\"y_train & y_test have been loaded!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the saved tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IMport the tokenizers of each input, fitted on part 3.1\n",
    "\"\"\"\n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\actors_tokenizer_06032020.pkl'),'rb') as f:\n",
    "    actors_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\plot_tokenizer_06032020.pkl'),'rb') as f:\n",
    "    plot_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\features_tokenizer_06032020.pkl'),'rb') as f:\n",
    "    features_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\reviews_tokenizer_06032020.pkl'),'rb') as f:\n",
    "    reviews_tokenizer = pickle.load(f)\n",
    "    \n",
    "print(\"Tokenizers are loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "KFold cross validation\n",
    "\n",
    "Maybe I could run a Kfold for with k=1 to reshuffle the data.\n",
    "\"\"\"\n",
    "X_seq_actors=np.load(\"C:\\\\Users\\\\spano\\\\Desktop\\\\GitHub-Thesis\\\\models_text_classification\\\\80-20 split_non-balanced\\\\x_seq_actors_80-20_non-balanced_28022020.npy\")\n",
    "X_seq_plot=np.load(\"C:\\\\Users\\\\spano\\\\Desktop\\\\GitHub-Thesis\\\\models_text_classification\\\\80-20 split_non-balanced\\\\x_seq_plot_80-20_non-balanced_28022020.npy\")\n",
    "X_seq_features=np.load(\"C:\\\\Users\\\\spano\\\\Desktop\\\\GitHub-Thesis\\\\models_text_classification\\\\80-20 split_non-balanced\\\\x_seq_features_80-20_non-balanced_28022020.npy\")\n",
    "X_seq_reviews=np.load(\"C:\\\\Users\\\\spano\\\\Desktop\\\\GitHub-Thesis\\\\models_text_classification\\\\80-20 split_non-balanced\\\\x_seq_reviews_80-20_non-balanced_28022020.npy\")\n",
    "\n",
    "print(\"X featurs data inputs have been loaded!\\n\")\n",
    "\n",
    "y=np.load(\"C:\\\\Users\\\\spano\\\\Desktop\\\\GitHub-Thesis\\\\models_text_classification\\\\80-20 split_non-balanced\\\\y_80-20_non-balanced_28022020.npy\")\n",
    "\n",
    "print(\"y variable has been loaded!\\n\")\n",
    "\n",
    "print(\"X_seq_actors shape:{}\".format(X_seq_actors.shape))\n",
    "print(\"X_seq_plot shape:{}\".format(X_seq_plot.shape))\n",
    "print(\"X_seq_features shape:{}\".format(X_seq_features.shape))\n",
    "print(\"X_seq_reviews shape:{}\\n\".format(X_seq_reviews.shape))\n",
    "\n",
    "print(\"y shape:{}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML-FLOW approach (contrary to TensorBoard) - <i> 15.01.2020 </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "# %reload_text tensorboard\n",
    "\n",
    "logdir=\".\\\\logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "# Callback function with early stopping to avodid overfit\n",
    "\n",
    "class Callback_Configurations():\n",
    "    \n",
    "    MONITOR_METRIC = 'val_loss'\n",
    "    MINIMUM_DELTA = 0.1\n",
    "    PATIENCE = 100\n",
    "    VERBOSE = 1\n",
    "    MODE = 'min'\n",
    "    \n",
    "def callback(saved_model, model, logdir):\n",
    "    \n",
    "    weights_fname = os.path.join(os.getcwd(), 'model_one\\\\{}.h5'.format(saved_model))\n",
    "\n",
    "    try:\n",
    "        with open(os.path.join(os.getcwd(), 'model_one\\\\{}.json'.format(save_model)), 'r') as f:\n",
    "            \n",
    "            model_json = json.load(f)\n",
    "        \n",
    "            model = model_from_json(model_json)\n",
    "        \n",
    "            model.load_weights('{}').format(weights_fname)\n",
    "\n",
    "    except:\n",
    "        print('\\nPre-trained weights not found. Fitting from start')\n",
    "        pass\n",
    "\n",
    "    monitor_metric = Callback_Configurations.MONITOR_METRIC\n",
    "    \n",
    "    callbacks = [\n",
    "        tfmodel.EpochDots(),\n",
    "        \n",
    "        EarlyStopping(monitor=monitor_metric,\n",
    "                      min_delta=Callback_Configurations.MINIMUM_DELTA,\n",
    "                      patience=Callback_Configurations.PATIENCE,\n",
    "                      verbose=Callback_Configurations.VERBOSE,\n",
    "                      mode=Callback_Configurations.MODE,\n",
    "                      restore_best_weights=True),\n",
    "\n",
    "        ModelCheckpoint(filepath=weights_fname,\n",
    "                        monitor=monitor_metric,\n",
    "                        verbose=Callback_Configurations.VERBOSE,\n",
    "                        save_best_only=True,\n",
    "                        save_weights_only=True), #True, False\n",
    "        \n",
    "        tf.keras.callbacks.TensorBoard(logdir)\n",
    "    ]\n",
    "    return callbacks\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "# Function to fit the keras multy input model\n",
    "\n",
    "def fitting_model(method,\n",
    "                  model,\n",
    "                  x_train_seq_actors, \n",
    "                  x_train_seq_plot, \n",
    "                  x_train_seq_features,\n",
    "                  x_train_seq_reviews,\n",
    "                  x_test_seq_actors, \n",
    "                  x_test_seq_plot, \n",
    "                  x_test_seq_features,\n",
    "                  x_test_seq_reviews,\n",
    "                  y_train, \n",
    "                  y_test,\n",
    "                  callbacks,\n",
    "                  steps_per_epoch,\n",
    "                  epoch,\n",
    "                  verbose_fit,\n",
    "                  batch_size_fit,\n",
    "                  val_split):\n",
    "    \"\"\"\n",
    "    Instead of using validation_data, I used the validation_split parameter\n",
    "    \"\"\"\n",
    "    s = time()\n",
    "    \n",
    "    if method==\"validation_split\":\n",
    "        \n",
    "        fit_model = model.fit([x_train_seq_actors, x_train_seq_plot, x_train_seq_features, x_train_seq_reviews], y_train,\n",
    "                              steps_per_epoch = steps_per_epoch,\n",
    "                              epochs=epoch,\n",
    "                              verbose=verbose_fit,\n",
    "                              batch_size=batch_size_fit,\n",
    "                              validation_split=val_split, # 90% for train and 10% for validation\n",
    "                              #validation_data=([x_test_seq_actors, x_test_seq_plot, x_test_seq_features, x_test_seq_reviews], y_test),\n",
    "                              callbacks=callbacks) #(callbacks)\n",
    "    elif method==\"validation_data\":\n",
    "        \n",
    "        fit_model = model.fit([x_train_seq_actors, x_train_seq_plot, x_train_seq_features, x_train_seq_reviews], y_train,\n",
    "                              steps_per_epoch = steps_per_epoch,\n",
    "                              epochs=epoch,\n",
    "                              verbose=verbose_fit,\n",
    "                              batch_size=batch_size_fit,\n",
    "                              validation_data=([x_test_seq_actors, x_test_seq_plot, x_test_seq_features, x_test_seq_reviews], y_test),\n",
    "                              callbacks=callbacks) #(callbacks)\n",
    "\n",
    "    duration = time() - s\n",
    "    print(\"\\nTraining time finished. Duration {} secs\".format(duration))\n",
    "    \n",
    "    return fit_model\n",
    "\n",
    "# For Kfold cross validation!\n",
    "def fitting_model_kfold(model, \n",
    "                        x_seq_actors_train, \n",
    "                        x_seq_plot_train, \n",
    "                        x_seq_features_train,\n",
    "                        x_seq_reviews_train,\n",
    "                        x_seq_actors_test, \n",
    "                        x_seq_plot_test, \n",
    "                        x_seq_features_test,\n",
    "                        x_seq_reviews_test,\n",
    "                        y_train,\n",
    "                        y_test,\n",
    "                        callbacks,\n",
    "                        steps_per_epoch,\n",
    "                        epoch,\n",
    "                        verbose_fit,\n",
    "                        batch_size_fit):\n",
    "    s = time()\n",
    "\n",
    "    fit_model = model.fit([x_seq_actors_train, x_seq_plot_train, x_seq_features_train, x_seq_reviews_train], y_train,\n",
    "                          steps_per_epoch = steps_per_epoch,\n",
    "                          epochs=epoch,\n",
    "                          verbose=verbose_fit,\n",
    "                          batch_size=batch_size_fit,\n",
    "                          validation_data=([x_seq_actors_test, x_seq_plot_test, x_seq_features_test, x_seq_reviews_test], y_test),\n",
    "                          callbacks=callbacks) #(callbacks)\n",
    "\n",
    "    duration = time() - s\n",
    "    print(\"\\nTraining time finished. Duration {} secs\".format(duration))\n",
    "    \n",
    "    return fit_model\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "def save_model(model, model_name):\n",
    "    \n",
    "    model_json = model.to_json()\n",
    "\n",
    "    with open(os.path.join(os.getcwd(), \"model_one\\\\{}.json\".format(model_name)), \"w\") as json_file:\n",
    "        json.dump(model_json, json_file)\n",
    "\n",
    "    model.save_weights(os.path.join(os.getcwd(), \"model_one\\\\{}.h5\".format(model_name)))\n",
    "    \n",
    "    print(\"\\nModel's weights are saved\")\n",
    "    \n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "# function to plot the model metrics (deprecated)\n",
    "\n",
    "def plot_model_metrics(fit_model):\n",
    "\n",
    "    rcParams['figure.figsize'] = 12, 6\n",
    "\n",
    "    plt.plot(fit_model.history['accuracy'] , 'g') # acc\n",
    "    plt.plot(fit_model.history['val_accuracy'] , 'b') # val_acc\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    rcParams['figure.figsize'] = 12, 6\n",
    "\n",
    "    plt.plot(fit_model.history['loss'] , 'g')\n",
    "    plt.plot(fit_model.history['val_loss'] , 'b')\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "# alternative function to plot the model metrics (used)\n",
    "\n",
    "def plot_keras_history(history): #where history =  model.fit()\n",
    "    \"\"\"\n",
    "    \n",
    "    :param history: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # the history object gives the metrics keys. \n",
    "    # we will store the metrics keys that are from the training sesion.\n",
    "    metrics_names = [key for key in history.history.keys() if not key.startswith('val_')]\n",
    "\n",
    "    for i, metric in enumerate(metrics_names):\n",
    "        \n",
    "        # getting the training values\n",
    "        metric_train_values = history.history.get(metric, [])\n",
    "        \n",
    "        # getting the validation values\n",
    "        metric_val_values = history.history.get(\"val_{}\".format(metric), [])\n",
    "\n",
    "        # As loss always exists as a metric we use it to find the \n",
    "        epochs = range(1, len(metric_train_values) + 1)\n",
    "        \n",
    "        # leaving extra spaces to allign with the validation text\n",
    "        training_text = \"   Training {}: {:.5f}\".format(metric,\n",
    "                                                        metric_train_values[-1])\n",
    "\n",
    "        # metric\n",
    "        plt.figure(i, figsize=(12, 6))\n",
    "\n",
    "        plt.plot(epochs,\n",
    "                 metric_train_values,\n",
    "                 'b',\n",
    "                 label=training_text)\n",
    "        \n",
    "        # if we validation metric exists, then plot that as well\n",
    "        if metric_val_values:\n",
    "            validation_text = \"Validation {}: {:.5f}\".format(metric,\n",
    "                                                             metric_val_values[-1])\n",
    "\n",
    "            plt.plot(epochs,\n",
    "                     metric_val_values,\n",
    "                     'g',\n",
    "                     label=validation_text)\n",
    "        \n",
    "        # add title, xlabel, ylabe, and legend\n",
    "        plt.title('Model Metric: {}'.format(metric))\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel(metric.title())\n",
    "        plt.legend()\n",
    "    \n",
    "    fig1 = plt.gcf()\n",
    "    #plt.savefig('multi-input-keras.png')\n",
    "    plt.show()\n",
    "    plt.draw()\n",
    "    fig1.savefig(os.path.join(os.getcwd(), 'model_one\\\\multi_input_keras.png'), dpi=100)\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "# Create function that will get as input a dataframe with the metrics (validation, accuracy) and create a plot per epoch\n",
    "# Proposed modules: seaborn, plotly, matplolib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Structure\n",
    "\n",
    "neural_network_parameters = {}\n",
    "optimizer_parameters = {}\n",
    "fit_parameters = {}\n",
    "\n",
    "neural_network_parameters['embedding_dimension'] = 50 #Grid-search on that value\n",
    "neural_network_parameters['pool_size'] = None\n",
    "neural_network_parameters['padding'] = 'valid'\n",
    "neural_network_parameters['batch_size'] = 128 #Grid-search on that value\n",
    "neural_network_parameters['l2_regularization'] = 0.01 #Grid-search on that value\n",
    "neural_network_parameters['dropout_rate'] = 0.0 #Grid-search on that value\n",
    "neural_network_parameters['dense_activation'] = 'relu'\n",
    "neural_network_parameters['output_activation'] = 'sigmoid' #softmax, sigmoid\n",
    "neural_network_parameters['number_target_variables'] = len(y_train[0].tolist())\n",
    "\n",
    "#It’s important to notice, that we use a sigmoid activation function with a multiclass output-layer. \n",
    "#The sigmoid gives us independent propabilities for each class. So DON’T use softmax here!\n",
    "\n",
    "neural_network_parameters['model_loss'] = 'binary_crossentropy' #sparse_categorical_crossentropy, binary_crossentropy, categorical_crossentropy\n",
    "neural_network_parameters['model_metric'] = 'accuracy' #sparse_categorical_accuracy, accuracy\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "fit_parameters[\"steps_per_epoch\"] = len(X_train_seq_features)//neural_network_parameters['batch_size']\n",
    "fit_parameters[\"epoch\"] = 150\n",
    "fit_parameters[\"verbose_fit\"] = 0\n",
    "fit_parameters[\"batch_size_fit\"] = 128 #Grid-search on that value\n",
    "\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "# Optimizer: ADAM (version_1)\n",
    "\n",
    "optimizer_parameters['adam_learning_rate'] = 0.001\n",
    "optimizer_parameters['adam_beta_1'] = 0.9\n",
    "optimizer_parameters['adam_beta_2'] = 0.999\n",
    "optimizer_parameters['adam_amsgrad'] = False\n",
    "\n",
    "def optimizer_adam_v1():\n",
    "    \n",
    "    return keras.optimizers.Adam(learning_rate=optimizer_parameters['adam_learning_rate'], \n",
    "                                 beta_1=optimizer_parameters['adam_beta_1'], \n",
    "                                 beta_2=optimizer_parameters['adam_beta_2'], \n",
    "                                 amsgrad=optimizer_parameters['adam_amsgrad'])\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "# Optimizer: ADAM (version_2)\n",
    "\n",
    "# lr_schedule_learning_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "# lr_schedule_decay_rate = [1, 2, 3, 4]\n",
    "\n",
    "optimizer_parameters['steps_per_epoch'] = len(X_train_seq_features)//neural_network_parameters['batch_size']\n",
    "# Steps per epoch...fine tune\n",
    "optimizer_parameters['lr_schedule_learning_rate'] = 0.01 #Grid-search on that value\n",
    "optimizer_parameters['lr_schedule_decay_steps'] = optimizer_parameters['steps_per_epoch']*1000\n",
    "optimizer_parameters['lr_schedule_decay_rate'] = 1 #Grid-search on that value\n",
    "optimizer_parameters['staircase'] = False\n",
    "\n",
    "#STEPS_PER_EPOCH = len(X_train_seq_features)//neural_network_parameters['batch_size'] #(512 = BATCH SIZE)\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    optimizer_parameters['lr_schedule_learning_rate'],\n",
    "    decay_steps=optimizer_parameters['lr_schedule_decay_steps'],\n",
    "    decay_rate=optimizer_parameters['lr_schedule_decay_rate'],\n",
    "    staircase=optimizer_parameters['staircase'])\n",
    "\n",
    "def optimizer_adam_v2():\n",
    "    \n",
    "    return keras.optimizers.Adam(lr_schedule)\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "# Optimizer: SDG (version 1)\n",
    "\n",
    "optimizer_parameters['SGD_learning_rate'] = 0.01\n",
    "optimizer_parameters['SGD_decay'] = 1e-6\n",
    "optimizer_parameters['SGD_momentum'] = 0.9\n",
    "optimizer_parameters['SGD_nesterov'] = True\n",
    "\n",
    "def optimizer_SDG_v1():\n",
    "    \n",
    "    return keras.optimizers.SGD(lr=optimizer_parameters['SGD_learning_rate'],\n",
    "                                decay=optimizer_parameters['SGD_decay'],\n",
    "                                momentum=optimizer_parameters['SGD_momentum'],\n",
    "                                nesterov=optimizer_parameters['SGD_nesterov'])\n",
    "\n",
    "#---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_multy_classification_model_v6(maxlen_actors, \n",
    "                                        maxlen_plot, \n",
    "                                        maxlen_features, \n",
    "                                        maxlen_reviews,\n",
    "                                        actors_max_words, \n",
    "                                        plot_max_words,\n",
    "                                        features_max_words,\n",
    "                                        reviews_max_words,\n",
    "                                        optimizer_version = None):\n",
    "    \n",
    "    sentenceLength_actors = maxlen_actors\n",
    "    vocab_size_frequent_words_actors = actors_max_words #20001, 12527, 11066, 16334\n",
    "    \n",
    "    sentenceLength_plot = maxlen_plot\n",
    "    vocab_size_frequent_words_plot = plot_max_words #17501, 13295, 10910, 10084\n",
    "    \n",
    "    sentenceLength_features = maxlen_features\n",
    "    vocab_size_frequent_words_features = features_max_words #20001, 17666, 14965, 14440\n",
    "    \n",
    "    sentenceLength_reviews = maxlen_reviews\n",
    "    vocab_size_frequent_words_reviews = reviews_max_words #40001, 16100, 15405, 15251\n",
    "    \n",
    "    model = keras.Sequential(name='MultyInput_Keras_Classification_model')\n",
    "    \n",
    "    actors = keras.Input(shape=(sentenceLength_actors,), name='actors_input')\n",
    "    plot = keras.Input(shape=(sentenceLength_plot,), name='plot_input')\n",
    "    features = keras.Input(shape=(sentenceLength_features,), name='features_input')\n",
    "    reviews = keras.Input(shape=(sentenceLength_reviews,), name='reviews_input')\n",
    "    \n",
    "    emb1 = layers.Embedding(input_dim = vocab_size_frequent_words_actors + 1,\n",
    "                            output_dim = neural_network_parameters['embedding_dimension'],\n",
    "                            embeddings_initializer = 'uniform',\n",
    "                            mask_zero = True,\n",
    "                            input_length = sentenceLength_actors,\n",
    "                            name=\"actors_embedding_layer\")(actors)\n",
    "    \n",
    "    encoded_layer1 = layers.GlobalMaxPooling1D(name=\"globalmaxpooling1\")(emb1)\n",
    "    \n",
    "    emb2 = layers.Embedding(input_dim = vocab_size_frequent_words_plot + 1,\n",
    "                            output_dim = neural_network_parameters['embedding_dimension'],\n",
    "                            embeddings_initializer = 'uniform',\n",
    "                            mask_zero = True,\n",
    "                            input_length = sentenceLength_plot,\n",
    "                            name=\"plot_embedding_layer\")(plot)\n",
    "    \n",
    "    encoded_layer2 = layers.GlobalMaxPooling1D(name=\"globalmaxpooling2\")(emb2)\n",
    "\n",
    "    emb3 = layers.Embedding(input_dim = vocab_size_frequent_words_features + 1,\n",
    "                            output_dim = neural_network_parameters['embedding_dimension'],\n",
    "                            embeddings_initializer = 'uniform',\n",
    "                            mask_zero = True,\n",
    "                            input_length = sentenceLength_features,\n",
    "                            name=\"features_embedding_layer\")(features)\n",
    "    \n",
    "    encoded_layer3 = layers.GlobalMaxPooling1D(name=\"globalmaxpooling3\")(emb3)\n",
    "    \n",
    "    emb4 = layers.Embedding(input_dim = vocab_size_frequent_words_reviews + 1,\n",
    "                            output_dim = neural_network_parameters['embedding_dimension'],\n",
    "                            embeddings_initializer = 'uniform',\n",
    "                            mask_zero = True,\n",
    "                            input_length = sentenceLength_reviews,\n",
    "                            name=\"reviews_embedding_layer\")(reviews)\n",
    "    \n",
    "    encoded_layer4 = layers.GlobalMaxPooling1D(name=\"globalmaxpooling4\")(emb4) #2D for images\n",
    "    \n",
    "    merged = layers.concatenate([encoded_layer1, encoded_layer2, encoded_layer3, encoded_layer4], axis=-1)\n",
    "\n",
    "    dense_layer_1 = layers.Dense(neural_network_parameters['batch_size'], \n",
    "                                 kernel_regularizer=regularizers.l2(neural_network_parameters['l2_regularization']),\n",
    "                                 activation=neural_network_parameters['dense_activation'],\n",
    "                                 name=\"1st_dense_layer\")(merged)\n",
    "    layers.Dropout(neural_network_parameters['dropout_rate'])(dense_layer_1)\n",
    "    \n",
    "    output_layer = layers.Dense(neural_network_parameters['number_target_variables'], activation=neural_network_parameters['output_activation'],\n",
    "                                name='output_layer')(dense_layer_1)\n",
    "    \n",
    "    model = keras.Model(inputs=[actors, plot, features, reviews], outputs=output_layer)\n",
    "    \n",
    "    print(model.output_shape)\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Version_1 of Adam\n",
    "    if optimizer_version is None:\n",
    "        \n",
    "        optimizer = optimizer_adam_v2()\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=neural_network_parameters['model_loss'],\n",
    "                  metrics=[neural_network_parameters['model_metric']])\n",
    "    \n",
    "    plot_model(model, to_file=os.path.join(os.getcwd(), 'model_one\\\\structure_multy_input_keras_model.png'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior to fitting the model: \n",
    "\n",
    "* X_train, X_test should have the form of an array with sequence of numbers.\n",
    "* y_train, y_test should have the form of a multi-hot encoded dataframe.\n",
    "\n",
    "<b> General observations: </b>\n",
    "\n",
    "* Reducing batch size can produce a better model (I should grid search on batch size).\n",
    "* Reducing the general number of parameters can produce better results.\n",
    "* Removing the second dense layer improved the results.\n",
    "* Removing regularization also affected the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RUN MLFLOW program\n",
    "\n",
    "model_repository = {}\n",
    "\n",
    "model_repository['Multiple Input Model'] = keras_multy_classification_model_v6(X_train_seq_actors.shape[1], \n",
    "                                                                               X_train_seq_plot.shape[1], \n",
    "                                                                               X_train_seq_features.shape[1], \n",
    "                                                                               X_train_seq_reviews.shape[1],\n",
    "                                                                               len(actors_tokenizer.word_index),\n",
    "                                                                               len(plot_tokenizer.word_index),\n",
    "                                                                               len(features_tokenizer.word_index),\n",
    "                                                                               len(reviews_tokenizer.word_index),\n",
    "                                                                               optimizer_version = None)\n",
    "mlflow.set_experiment(\"/multi_input_keras_model\")\n",
    "with mlflow.start_run():\n",
    "    \n",
    "    model_directory = os.path.join(os.getcwd(), \"model_one\")\n",
    "    logdir = \".\\\\logs_test\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    #callbacks = callback(\"model_multy_input\", model_repository['Multiple Input Model'], logdir)\n",
    "\n",
    "    model_history = {}\n",
    "    model_history['experiment'] = fit_keras_multy_input(model_repository['Multiple Input Model'],\n",
    "                                                        X_train_seq_actors, #input_1\n",
    "                                                        X_train_seq_plot, #input_2\n",
    "                                                        X_train_seq_features, #input_3\n",
    "                                                        X_train_seq_reviews, #input4\n",
    "                                                        X_test_seq_actors,\n",
    "                                                        X_test_seq_plot,\n",
    "                                                        X_test_seq_features,\n",
    "                                                        X_test_seq_reviews,\n",
    "                                                        y_train, #output\n",
    "                                                        y_test,\n",
    "                                                        callback(\"multi_input_keras_model\", model_repository['Multiple Input Model'], logdir), #callback function\n",
    "                                                        fit_parameters[\"steps_per_epoch\"],\n",
    "                                                        fit_parameters[\"epoch\"],\n",
    "                                                        fit_parameters[\"verbose_fit\"],\n",
    "                                                        fit_parameters[\"batch_size_fit\"])\n",
    "\n",
    "    hist = pd.DataFrame(model_history['experiment'].history)\n",
    "    hist['epoch'] = model_history['experiment'].epoch\n",
    "    hist['epoch']+= 1\n",
    "    hist.index += 1\n",
    "    print(\"\\nTable of training the keras text classification model\\n\")\n",
    "    print(tabulate(hist, headers='keys', tablefmt='psql'))\n",
    "    \n",
    "    hist.to_pickle(os.path.join(os.getcwd(), \"model_one\\\\metrics_histogram_multi_input_keras.pkl\"))\n",
    "    \n",
    "    #save the model\n",
    "    save_model(model_repository['Multiple Input Model'], \"multi_input_keras_model\")\n",
    "    \n",
    "    #plot the model's accuracy & loss\n",
    "    plot_keras_history(model_history['experiment'])\n",
    "    \n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    \n",
    "    # evaluate the model\n",
    "    model_evaluation = model_repository['Multiple Input Model'].evaluate([X_test_seq_actors, X_test_seq_plot, X_test_seq_features, X_test_seq_reviews], \n",
    "                                                                         y_test,\n",
    "                                                                         batch_size=fit_parameters[\"batch_size_fit\"],\n",
    "                                                                         verbose=2)\n",
    "    print('\\nTest Score:', model_evaluation[0])\n",
    "\n",
    "    print('\\nTest Accuracy:', model_evaluation[1])\n",
    "    \n",
    "    #neural_model params\n",
    "    mlflow.log_param(\"embedding_dimension\", neural_network_parameters['embedding_dimension'] )\n",
    "    mlflow.log_param(\"pool_size\", neural_network_parameters['pool_size'])\n",
    "    mlflow.log_param(\"padding\", neural_network_parameters['padding'])\n",
    "    mlflow.log_param(\"batch_size\", neural_network_parameters['batch_size'])\n",
    "    mlflow.log_param(\"l2_regularization\", neural_network_parameters['l2_regularization'])\n",
    "    mlflow.log_param(\"dropout_rate\", neural_network_parameters['dropout_rate'])\n",
    "    mlflow.log_param(\"dense_activation\", neural_network_parameters['dense_activation'])\n",
    "    mlflow.log_param(\"output_activation\",neural_network_parameters['output_activation'])\n",
    "    mlflow.log_param(\"model_loss\",neural_network_parameters['model_loss']) #takes any data type\n",
    "    mlflow.log_param(\"model_metric\",neural_network_parameters['model_metric'])\n",
    "    \n",
    "    #optimizer params\n",
    "    mlflow.log_param(\"lr_schedule_learning_rate\",optimizer_parameters['lr_schedule_learning_rate'])\n",
    "    mlflow.log_param(\"lr_schedule_decay_steps\",optimizer_parameters['lr_schedule_decay_steps'])\n",
    "    mlflow.log_param(\"lr_schedule_decay_rate\",optimizer_parameters['lr_schedule_decay_rate'])\n",
    "    mlflow.log_param(\"adam_amsgrad\",optimizer_parameters['staircase'])\n",
    "    \n",
    "    #fit_model params\n",
    "    mlflow.log_param(\"steps_per_epoch\",fit_parameters['steps_per_epoch'])\n",
    "    mlflow.log_param(\"fit_epoch\",fit_parameters['epoch'])\n",
    "    mlflow.log_param(\"verbose_fit\",fit_parameters['verbose_fit'])\n",
    "    mlflow.log_param(\"batch_size_fit\",fit_parameters['batch_size_fit']) #in generl batch_size_fit = neurons batch size\n",
    "    \n",
    "    #logging the model metrics\n",
    "    mlflow.log_metric(\"model_validation_loss\",model_evaluation[0]) #take only floats/integers\n",
    "    mlflow.log_metric(\"model_validation_accuracy\",model_evaluation[1])\n",
    "    \n",
    "    #mlflow.keras.save_model(model_repository['Multiple Input Model'], model_directory) -> deprecated\n",
    "    mlflow.keras.log_model(model_repository['Multiple Input Model'], \"multi_input_keras_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RUN MLFLOW program with KFold Cross Validation\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "seed=123\n",
    "mlflow.set_experiment(\"multi_input_keras_model\")\n",
    "with mlflow.start_run():\n",
    "    \n",
    "    model_repository = {}\n",
    "    model_repository['Multiple Input Model'] = keras_multy_classification_model_v6(X_seq_actors.shape[1], \n",
    "                                                                                   X_seq_plot.shape[1], \n",
    "                                                                                   X_seq_features.shape[1], \n",
    "                                                                                   X_seq_reviews.shape[1], \n",
    "                                                                                   optimizer_version = None)\n",
    "    \n",
    "    model_directory = \"C:\\\\Users\\\\spano\\\\Desktop\\\\GitHub-Thesis\\\\models_text_classification\\\\model_one\"\n",
    "    \n",
    "    logdir = \".\\\\logs_test\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    callbacks = callback(\"model_multy_input\", model_repository['Multiple Input Model'], logdir)\n",
    "    \n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    cvscores = []\n",
    "    for train, test in kfold.split(X_seq_actors, y):\n",
    "    \n",
    "        model_history = {}\n",
    "        model_history['experiment'] = fit_keras_multy_input(model_repository['Multiple Input Model'], \n",
    "                                                            X_seq_actors[train], #input_1\n",
    "                                                            X_seq_plot[train], #input_2\n",
    "                                                            X_seq_features[train], #input_3\n",
    "                                                            X_seq_reviews[train], #input4\n",
    "                                                            X_seq_actors[test], \n",
    "                                                            X_seq_plot[test], \n",
    "                                                            X_seq_features[test],\n",
    "                                                            X_seq_reviews[test],\n",
    "                                                            y[train], #output\n",
    "                                                            y[test], \n",
    "                                                            callbacks, #callback function\n",
    "                                                            fit_parameters[\"steps_per_epoch\"],\n",
    "                                                            fit_parameters[\"epoch\"],\n",
    "                                                            fit_parameters[\"verbose_fit\"],\n",
    "                                                            fit_parameters[\"batch_size_fit\"])\n",
    "\n",
    "    hist = pd.DataFrame(model_history['experiment'].history)\n",
    "    hist['epoch'] = model_history['experiment'].epoch\n",
    "    hist['epoch']+= 1\n",
    "    hist.index += 1\n",
    "    print(\"\\nTable of training the keras text classification model\\n\")\n",
    "    print(tabulate(hist, headers='keys', tablefmt='psql'))\n",
    "    \n",
    "    hist.to_pickle(\".\\\\model_one\\\\metrics_histogram_multi_input_keras.pkl\")\n",
    "    \n",
    "    save_model(model_repository['Multiple Input Model'], \"model_multy_input\")\n",
    "    \n",
    "    #version_1 of plot model\n",
    "    #plot_model_metrics(model_history['experiment'])\n",
    "    \n",
    "    #version_2 of plot model\n",
    "    plot_keras_history(model_history['experiment'])\n",
    "    \n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "    model_evaluation = model_repository['Multiple Input Model'].evaluate([X_seq_actors[test], X_seq_plot[test], X_seq_features[test], X_seq_reviews[test]], \n",
    "                                                                         y[test],\n",
    "                                                                         batch_size=fit_parameters[\"batch_size_fit\"],\n",
    "                                                                         verbose=2)\n",
    "    print('\\nTest Score:', model_evaluation[0])\n",
    "\n",
    "    print('\\nTest Accuracy:', model_evaluation[1])\n",
    "    \n",
    "    print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(model_evaluation), np.std(model_evaluation)))\n",
    "    \n",
    "    #neural_model params\n",
    "    mlflow.log_param(\"embedding_dimension\", neural_network_parameters['embedding_dimension'] )\n",
    "    mlflow.log_param(\"pool_size\", neural_network_parameters['pool_size'])\n",
    "    mlflow.log_param(\"padding\", neural_network_parameters['padding'])\n",
    "    mlflow.log_param(\"batch_size\", neural_network_parameters['batch_size'])\n",
    "    mlflow.log_param(\"l2_regularization\", neural_network_parameters['l2_regularization'])\n",
    "    mlflow.log_param(\"dropout_rate\", neural_network_parameters['dropout_rate'])\n",
    "    mlflow.log_param(\"dense_activation\", neural_network_parameters['dense_activation'])\n",
    "    mlflow.log_param(\"output_activation\",neural_network_parameters['output_activation'])\n",
    "    mlflow.log_param(\"model_loss\",neural_network_parameters['model_loss']) #takes any data type\n",
    "    mlflow.log_param(\"model_metric\",neural_network_parameters['model_metric'])\n",
    "    \n",
    "    #optimizer params\n",
    "    mlflow.log_param(\"lr_schedule_learning_rate\",optimizer_parameters['lr_schedule_learning_rate'])\n",
    "    mlflow.log_param(\"lr_schedule_decay_steps\",optimizer_parameters['lr_schedule_decay_steps'])\n",
    "    mlflow.log_param(\"lr_schedule_decay_rate\",optimizer_parameters['lr_schedule_decay_rate'])\n",
    "    mlflow.log_param(\"adam_amsgrad\",optimizer_parameters['staircase'])\n",
    "    \n",
    "    #fit_model params\n",
    "    mlflow.log_param(\"steps_per_epoch\",fit_parameters['steps_per_epoch'])\n",
    "    mlflow.log_param(\"fit_epoch\",fit_parameters['epoch'])\n",
    "    mlflow.log_param(\"verbose_fit\",fit_parameters['verbose_fit'])\n",
    "    mlflow.log_param(\"batch_size_fit\",fit_parameters['batch_size_fit']) #in generl batch_size_fit = neurons batch size\n",
    "    \n",
    "    #logging the model metrics\n",
    "    mlflow.log_metric(\"model_validation_loss\",model_evaluation[0]) #take only floats/integers\n",
    "    mlflow.log_metric(\"model_validation_accuracy\",model_evaluation[1])\n",
    "    \n",
    "    #mlflow.keras.save_model(model_repository['Multiple Input Model'], model_directory) -> deprecated\n",
    "    mlflow.keras.log_model(model_repository['Multiple Input Model'], \"multi-input-keras-model\")\n",
    "\n",
    "    #mlflow.tensorflow.log_model(tf_saved_model_dir = \"C:\\\\Users\\\\spano\\\\Desktop\\\\GitHub-Thesis\\\\models_text_classification\\\\model_one\\\\model_multy_input.h5\")\n",
    "#   mlflow.tensorflow.save_model(model_repository['Multiple Input Model'], model_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Model prediction </b>\n",
    "\n",
    "A good practice is to save the model predictions, to reproduce the classification report and compare it to other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.getcwd(), \"pickled_data_per_part\\\\genres_list_16022020.pkl\"), 'rb') as handle:\n",
    "    genres_list = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use to yield probability distribution over the categories\n",
    "y_test_pred_probs = model_repository['Multiple Input Model'].predict([X_test_seq_actors, X_test_seq_plot, X_test_seq_features, X_test_seq_reviews])\n",
    "y_test_pred_probs[0]\n",
    "\n",
    "# y_predicted probabilities for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predictions = (y_test_pred_probs>0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predictions[4314]\n",
    "# y_predicted genre if equals to 1 (predicted genres of the movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[4314]\n",
    "# the real genres of the movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(os.getcwd(), \"model_one//y_predictions_multi_input_keras_07032020\"), y_test_predictions)\n",
    "np.save(os.path.join(os.getcwd(), \"model_one//y_true_multi_input_keras_07032020\"), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "\n",
    "classification_table= classification_report(y_true=y_test, y_pred=y_test_predictions)\n",
    "\n",
    "print(\"Classification report\\n\" + str(classification_table))\n",
    "\n",
    "# Hamming Loss\n",
    "print(\"Hamming loss: \", hamming_loss(y_test, y_test_predictions))\n",
    "\n",
    "# Zero_one loss\n",
    "print(\"Zero one loss: \", zero_one_loss(y_test, y_test_predictions, normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat=confusion_matrix(y_test.argmax(axis=1), y_test_predictions.argmax(axis=1))\n",
    "\n",
    "conf_matrix=pd.DataFrame(conf_mat,\n",
    "             columns=genres_list,\n",
    "             index=genres_list)\n",
    "conf_matrix.to_pickle(os.path.join(os.getcwd(), \"model_one\\\\confusion_matrix_07032020.pkl\"))\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame({'Keras Model':pd.Series(\"Multy Input Keras\", dtype='str'),\n",
    "                         'Test Loss':pd.Series([model_evaluation[0]], dtype='float'),\n",
    "                         'Test Accuracy':pd.Series([model_evaluation[1]], dtype='float'),\n",
    "                         'Hamming Loss':pd.Series([hamming_loss(y_test, y_test_predictions)], dtype='float'),\n",
    "                         'Zero_one Loss':pd.Series([zero_one_loss(y_test, y_test_predictions, normalize=False)], dtype='float'),\n",
    "                         'F1_score':pd.Series([f1_score(y_test, y_test_predictions, average=\"micro\")], dtype='float')})\n",
    "\n",
    "df_scores.to_pickle(os.path.join(os.getcwd(), \"model_one\\\\df_metrics_multy_input_keras_08032020.pkl\"))\n",
    "\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicted vs Actual Genre Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_genre_tags(indx, model, genres_list):\n",
    "        \n",
    "    test_sequence_actors = X_test_seq_actors[indx:indx+1]\n",
    "    \n",
    "    test_sequence_plot = X_test_seq_plot[indx:indx+1]\n",
    "    \n",
    "    test_sequence_features = X_test_seq_features[indx:indx+1]\n",
    "    \n",
    "    test_sequence_reviews = X_test_seq_reviews[indx:indx+1]\n",
    "    \n",
    "    text_prediction = model.predict([test_sequence_actors, test_sequence_plot, test_sequence_features, test_sequence_reviews])\n",
    "    \n",
    "    [float(i) for i in text_prediction[0]]\n",
    "    \n",
    "    tag_probabilities = text_prediction[0][np.argsort(text_prediction[0])[-3:]]\n",
    "    \n",
    "    indexes = np.argsort(text_prediction[0])[::-1][:3]\n",
    "\n",
    "    predicted_tags = []\n",
    "    \n",
    "    for i, tag in enumerate(genres_list):\n",
    "        if i in indexes:\n",
    "            predicted_tags.append(genres_list[i])\n",
    "    \n",
    "    return predicted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_numbers = random.sample(range(1, y_test.shape[0]), 5)\n",
    "\n",
    "save_index_of_numbers = random_numbers\n",
    "\n",
    "print(\"Randomly saved numbers to make predictions: {}\".format(save_index_of_numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=pd.read_pickle(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\x_test_06032020.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = pd.DataFrame({'Movie Title':pd.Series([X_test['title'].iloc[save_index_of_numbers[0]]], dtype='str'),\n",
    "                               'Predicted Genre tags':pd.Series([predict_genre_tags(save_index_of_numbers[0], model_repository['Multiple Input Model'], genres_list)], dtype='str'),\n",
    "                               'Real Genre tags':pd.Series([X_test['reduced_genres'].iloc[save_index_of_numbers[0]]], dtype='str')})\n",
    "\n",
    "for i in range(len(save_index_of_numbers)):\n",
    "\n",
    "    df_predictions = df_predictions.append({'Movie Title':X_test['title'].iloc[save_index_of_numbers[i]], \n",
    "                                            'Predicted Genre tags':predict_genre_tags(save_index_of_numbers[i], model_repository['Multiple Input Model'], genres_list),\n",
    "                                            'Real Genre tags':X_test['reduced_genres'].iloc[save_index_of_numbers[i]]} , ignore_index=True)\n",
    "\n",
    "df_predictions = df_predictions.drop(df_predictions.index[0])\n",
    "df_predictions.to_pickle(\"model_one\\\\model_one_df_predictions_07032020.pkl\")\n",
    "df_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  </b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
