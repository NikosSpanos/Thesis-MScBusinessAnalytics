{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.1 - Data Tokenization-Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the version data control parameter (to save the outputs of this notebook at their latest date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_data_control=\"13072020\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tabulate import tabulate\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from humanfriendly import format_timespan\n",
    "import random\n",
    "\n",
    "# Module to serialize the content produced from the execution of the code\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Module to monitor the progress of a python for loop\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Module to manipulate text in python - NLTK package\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import spacy\n",
    "import unidecode\n",
    "from word2number import w2n\n",
    "\n",
    "# Module to compute word vectorizers and compute the cosine distance\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "import string\n",
    "import itertools\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_pandas_display_options() -> None:\n",
    "    display = pd.options.display\n",
    "\n",
    "    display.max_columns = 1000\n",
    "    display.max_rows = 1000\n",
    "    display.max_colwidth = 199\n",
    "    display.width = None\n",
    "    # display.precision = 2  # set as needed\n",
    "\n",
    "set_pandas_display_options()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Tokenization and Plotting Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "\n",
    "from packaging import version\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the dataset (this demonstrates how the genres have been cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= pd.read_pickle(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\dataset_part_2_20072020.pkl'.format(version_data_control))) #should be the latest version of the data created from part 2.\n",
    "print(\"\\nThe shape of the dataset that will be used in Keras classifier is: {}\".format(dataset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset.title==\"Les Mis√©rables\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understand the dependent variable: Genres of each movie\n",
    "\n",
    "Check their frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['genres'].explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(dataset['genres'].explode().value_counts(normalize=True) * 100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Remove genres less than 1% frequency\n",
    "\n",
    "dataset['reduced_genres'] = dataset['genres'].apply(\n",
    "    lambda row: [val for val in row if val not in ['IMAX', 'Sport', 'Adult', 'News', 'Reality-TV',\n",
    "                                                   'Film-Noir', 'Short', 'Family', 'Biography', 'Music', 'History']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['reduced_genres'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Find indexes with EMPTY LISTS\n",
    "\n",
    "dataset_empty_lists = dataset[dataset.reduced_genres.apply(lambda c: c==[])]\n",
    "\n",
    "remove_indices = dataset_empty_lists.index.to_list()\n",
    "\n",
    "dataset_empty_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Remove the indexes with EMPTY LISTS\n",
    "\n",
    "dataset_frequent_genres =  dataset[~dataset.index.isin(remove_indices)]\n",
    "\n",
    "dataset_frequent_genres.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_frequent_genres = dataset_frequent_genres.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Having cut the most scarse occurences of genres it is still obvious that genres \"Drama\" & \"Comedy\" belong to 40% of the movies.\n",
    "A good approach is either to up-sample the dataset or down-sample it.\n",
    "What we chose was to down-sample the two dominant genres \"Drama\" & \"Comedy\". However, in the sub-part 3.2 \n",
    "We used the imbalanced dataset to train and test the keras text classification models.\n",
    "\"\"\"\n",
    "round(dataset_frequent_genres['reduced_genres'].explode().value_counts(normalize=True) * 100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The dataset below contains 17 out of 27 genres. The 11 genres cut were not frequent enough compared to the rest of the genres.\n",
    "\"\"\"\n",
    "# dataset_frequent_genres.to_pickle(\"pickled_data_per_part\\\\dataset_part_2_cleaned_of_redundant_genres_20072020.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #1 Import cleaned of redundant genres dataset and genres_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset that was pickled in the previous cell (above)\n",
    "dataset_frequent_genres = pd.read_pickle(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\dataset_part_2_cleaned_of_redundant_genres_20072020.pkl'))\n",
    "\n",
    "# REPLACE THE TEXT OF THOSE MOVIE PLOTS & FEATURES WITH THOSE BELOW, OTHERWISE THE PREPROCESSING WON'T COMPLETE CORRECTLY\n",
    "dataset_frequent_genres['plot'].iloc[10802]='A documentary looking at the Nigga word'\n",
    "dataset_frequent_genres['plot'].iloc[15148]=\"A shy Arun works as an accountant. On daily basis he travels by a bus to office where he sees Prabha who works in the same premises. Arun starts to like Prabha and follows her frequently but hasn't got the confidence in him to approach her. When ever Arun gains confidence to share about his feelings to Prabha her friend Nagesh steps in spoiling Arun's plans. In order to win his love for Prabha, Arun travels all the way to Lonavala to take the help of expert Colonel Julius Naganderanath Wilfred Singh. During this phase Prabha to starts missing Arun.\"\n",
    "dataset_frequent_genres['plot'].iloc[31993]=\"MARTIN ARMSTRONG, once a US based trillion dollar financial adviser, used the number pi to predict economic turning points with precision. When some big New York bankers asked him to join the club to help them to take over Russia, he refused to join the manipulation. A few days later the FBI stormed his offices accusing him of a 3 billion dollar Ponzi Scheme - an attempt to stop him talking about the real Ponzi Scheme of debts that the US has build up over the years and which he thinks starts to collapse after October 1, 2015, a major pi turning point he is predicting.\"\n",
    "dataset_frequent_genres['plot'].iloc[32857]=\"Notorious gangster asks a career thief by the name of Nandu to steal a package worth over one crore rupees for a fee, which Nandu does so. Things do not go as planned, when Nandu finds out that the package is worth more than twice the amount, he refuses to hand over the package to Pinky, until he raises his fees. An enraged Pinky instructs his men to locate Nandu, find the package and bring it back, and thereafter kill Nandu. While on the run from Pinky's gangsters, the police find out that Nandu was involved in this theft, and they launch a man-hunt. What Nandu does not know that the package that he has on his person does not contain money or gold, but a nuclear bomb that is set to explode.\"\n",
    "dataset_frequent_genres['plot'].iloc[33094]=\"Before New Year Eve the windows of vast city are glowing from within by colored Christmas lights. It seems that behind each of these windows carefully stored favorite holiday. It seems that each of them are waiting for the New Year's wish fulfillment. By the silhouette of an elderly woman at one of the windows of the old house in Moscow, neighbors in the yard probably accustomed as something unchangeable. Sophia Ivanovna ten years does not get up from her chair, all day looking out the window, glues paper figurines and listening to Dickens, who reads aloud to her, her only daughter Tanya. The only heiress, Tanya, whom Sophia Ivanovna must to pass all the family jewels, seems resigned to the position of an old maid, and her whole life consists only of the care of the sick mother.\"\n",
    "dataset_frequent_genres['plot'].iloc[36100]=\"Mr. Giant has kidnapped the brilliant Dr. Van Kohler and is planning to use the Doctor's invention, the N-bomb, to hold the world hostage. The only one who can foil Mr. Giant's evil scheme is Agent 00, a 3-foot-tall filipino martial arts master, expert marksman, top-class romancer and all-around superspy. Can Agent 00 rescue Dr. Kohler before it's too late?\"\n",
    "dataset_frequent_genres['plot'].iloc[36669]=\"HEADER portrays the grueling psychological journey taken by ATF Agent Stewart Cummings. On the surface, Stewart struggles to solve a string of bizarre murders, but in secret, his life falls into a world of corruption that's impossible to escape. Deceit, rape, and murder spiral out of control triggering a hellish conclusion that defies description.\"\n",
    "dataset_frequent_genres['plot'].iloc[47725]=\"Raj is one of city's top lawyer known never to loose a case. He his introduced to Sargam during the launch of her new album by Ashwin Mehta owner of a music company. Next day Ashwin is found murdered and Tarang is held for the murder, Sargam and Tarang are childhood friends and she approaches Raj to fight for his case. During this period Raj notices that Tarang suffers from mental illness of split personality and when he suffers the attack he is not Tarang but Ranjeet and doesn't remember things he does when he turns Ranjeet. Tarang had killed Ashwin as the previous night he saw him forcing on Sargam but doesn't remember killing him as he was Ranjeet that time. Raj wins the case in court by proving Tarang innocent as he is suffering from mental illness. Tarang is due to release and gives Raj a shock to that he had planned everything regarding split personality as he was caught red handed on the site of murder.\"\n",
    "\n",
    "dataset_frequent_genres['movie_features'].iloc[8102]=\"Spinning Boris Jeff Goldblum Anthony LaPaglia Liev Schreiber Boris Lee Krutonog Svetlana Efremova Shauna MacDonald Gregory Hlady Vladimir Radian Ilia Volok Konstantin Kazakov Judah Katz Maria Syrgiannis Ola Sturik Gillian Vanderburgh Serge Timokhin Roger Spottiswoode Early in 1996, three Republican campaign operatives take a job in secret assisting Boris Yeltsin's reelection. Once in Moscow, they find he's polling at 6 percent with the election a few months away. While Dick Dresner wants to go home, George Gorton and Joe Shumate vote to stay. First, they must get someone's attention; they succeed finally with Yeltsin's daughter. Then it's polling, focus groups, messages and spin. Even as Yeltsin's numbers go up, the trio are unsure who hired them and whether Yeltsin's allies have a different plan in mind than election victory. When the going gets toughest, it's Gorton who puts a spin on our stake: democracy and capitalism must win. Comedy Drama\"\n",
    "dataset_frequent_genres['movie_features'].iloc[10802]=\"The N Word F. Lee Bailey Sandra Bernhard Donald Bogle Todd Boyd Elaine Brown LeVar Burton George Carlin Morris Chestnut Chuck D Johnnie L. Cochran Jr. Stanley Crouch Damon Dash Dr. Dre Eazy-E Laurence Fishburne Todd Williams A documentary looking at the Nigga word Documentary\"\n",
    "dataset_frequent_genres['movie_features'].iloc[15148]=\"Chhoti Si Baat Ashok Kumar Vidya Sinha Amol Palekar Asrani Nandita Thakur Rajan Haksar Ashim Kumar Devendra Khandelwal Baba Majgoakar Noni Ganguly C.S. Dubey Amol Sen R.S. Chopra Sudarshan Sahni Milon Mukerji Basu Chatterjee A shy Arun works as an accountant. On daily basis he travels by a bus to office where he sees Prabha who works in the same premises. Arun starts to like Prabha and follows her frequently but hasn't got the confidence in him to approach her. When ever Arun gains confidence to share about his feelings to Prabha her friend Nagesh steps in spoiling Arun's plans. In order to win his love for Prabha, Arun travels all the way to Lonavala to take the help of expert Colonel Julius Naganderanath Wilfred Singh. During this phase Prabha to starts missing Arun. Comedy Romance\"\n",
    "dataset_frequent_genres['movie_features'].iloc[31993]=\"The Forecaster Martin Armstrong Vicky Armstrong Oliver Brown Michael Campbell Larry Edelson Tony Godin Nigel Kirwan Barclay Leib Neill MacPherson Thomas Sjoblom Marcus Vetter MARTIN ARMSTRONG, once a US based trillion dollar financial adviser, used the number pi to predict economic turning points with precision. When some big New York bankers asked him to join the club to help them to take over Russia, he refused to join the manipulation. A few days later the FBI stormed his offices accusing him of a 3 billion dollar Ponzi Scheme - an attempt to stop him talking about the real Ponzi Scheme of debts that the US has build up over the years and which he thinks starts to collapse after October 1, 2015, a major pi turning point he is predicting. Documentary\"\n",
    "dataset_frequent_genres['movie_features'].iloc[32857]=\"Daud Sanjay Dutt Urmila Matondkar Paresh Rawal Neeraj Vora Ram Mohan Ashish Vidyarthi Manoj Bajpayee Rajeev Mehta Jeetendra Shastri Sumukhi Tarzan Vineeth Narsing Yadav Rana Jung Bahadur Sunil Shende Ram Gopal Varma Notorious gangster asks a career thief by the name of Nandu to steal a package worth over one crore rupees for a fee, which Nandu does so. Things do not go as planned, when Nandu finds out that the package is worth more than twice the amount, he refuses to hand over the package to Pinky, until he raises his fees. An enraged Pinky instructs his men to locate Nandu, find the package and bring it back, and thereafter kill Nandu. While on the run from Pinky's gangsters, the police find out that Nandu was involved in this theft, and they launch a man-hunt. What Nandu does not know that the package that he has on his person does not contain money or gold, but a nuclear bomb that is set to explode. Action\"\n",
    "dataset_frequent_genres['movie_features'].iloc[33094]=\"Come Look at Me Oleg Yankovskiy Irina Kupchenko Yekaterina Vasilyeva Natalya Shchukina Mark Rudinshtejn Ivan Yankovskiy Mikhail Agranovich Before New Year Eve the windows of vast city are glowing from within by colored Christmas lights. It seems that behind each of these windows carefully stored favorite holiday. It seems that each of them are waiting for the New Year's wish fulfillment. By the silhouette of an elderly woman at one of the windows of the old house in Moscow, neighbors in the yard probably accustomed as something unchangeable. Sophia Ivanovna ten years does not get up from her chair, all day looking out the window, glues paper figurines and listening to Dickens, who reads aloud to her, her only daughter Tanya. The only heiress, Tanya, whom Sophia Ivanovna must to pass all the family jewels, seems resigned to the position of an old maid, and her whole life consists only of the care of the sick mother Comedy Drama Romance\"\n",
    "dataset_frequent_genres['movie_features'].iloc[36100]=\"For Y'ur Height Only Weng Weng Yehlen Catral Carmi Martin Anna Marie Gutierrez Beth Sandoval Eddie Nicart Mr. Giant has kidnapped the brilliant Dr. Van Kohler and is planning to use the Doctor's invention, the N-bomb, to hold the world hostage. The only one who can foil Mr. Giant's evil scheme is Agent 00, a 3-foot-tall filipino martial arts master, expert marksman, top-class romancer and all-around superspy. Can Agent 00 rescue Dr. Kohler before it's too late? Action Comedy\"\n",
    "dataset_frequent_genres['movie_features'].iloc[36669]=\"Header Jake Suffian Elliot V. Kotek Dick Mullaney Michael Philip Anthony Stacey Brooks Tara Brooks Jim Coope Bill Corry Andrew Cowen Amanda Czelinski Stephen DeCaires Kevin Dedes Lauren Devlin Ruth Dimino Morris Fazzi Jr. Archibald Flancranstin HEADER portrays the grueling psychological journey taken by ATF Agent Stewart Cummings. On the surface, Stewart struggles to solve a string of bizarre murders, but in secret, his life falls into a world of corruption that's impossible to escape. Deceit, rape, and murder spiral out of control triggering a hellish conclusion that defies description. Horror\"\n",
    "dataset_frequent_genres['movie_features'].iloc[47725]=\"Deewangee Ajay Devgn Akshaye Khanna Urmila Matondkar Farida Jalal Vijayendra Ghatge Seema Biswas Tiku Talsania Tanaaz Currim Irani Mohan Kapoor Nirmal Pandey Nishigandha Wad Rana Jung Bahadur Suresh Oberoi Sushovan Banerjee Suhasini Mulay Anees Bazmee Raj is one of city's top lawyer known never to loose a case. He his introduced to Sargam during the launch of her new album by Ashwin Mehta owner of a music company. Next day Ashwin is found murdered and Tarang is held for the murder, Sargam and Tarang are childhood friends and she approaches Raj to fight for his case. During this period Raj notices that Tarang suffers from mental illness of split personality and when he suffers the attack he is not Tarang but Ranjeet and doesn't remember things he does when he turns Ranjeet. Tarang had killed Ashwin as the previous night he saw him forcing on Sargam but doesn't remember killing him as he was Ranjeet that time. Raj wins the case in court by proving Tarang innocent as he is suffering from mental illness. Tarang is due to release and gives Raj a shock to that he had planned everything regarding split personality as he was caught red handed on the site of murder. Crime Drama Mystery\"\n",
    "\n",
    "# DELETE THAT MOVIE\n",
    "dataset_frequent_genres = dataset_frequent_genres[dataset_frequent_genres['title'] != \"6 Days to Air: The Making of South Park\"]\n",
    "\n",
    "dataset_frequent_genres=dataset_frequent_genres.reset_index(drop=True)\n",
    "print(\"\\nThe shape of the dataset that will be used in Keras classifier is: {}\".format(dataset_frequent_genres.shape))\n",
    "# Comment: From now on, \"reduced_genres\" column will be used for model classification and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multi-hot encoding is a good practice to transform the value y into a data structure appropriate for multi-label text calssification.\n",
    "\"\"\"\n",
    "# Multy hot encoding since a Movie can have more than 1 genres assigned!\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "dataset_frequent_genres = dataset_frequent_genres.join(pd.DataFrame(mlb.fit_transform(dataset_frequent_genres['reduced_genres']),\n",
    "                                                                    columns=mlb.classes_,\n",
    "                                                                    index=dataset_frequent_genres.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import genres\n",
    "with open(os.path.join(os.getcwd(), \"pickled_data_per_part\\\\genres_list_06032020.pkl\"), 'rb') as handle:\n",
    "    genres_list = pickle.load(handle)\n",
    "genres_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #2 Prune the movie reviews (keep only the first review for each movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_frequent_genres['reviews_length'] = dataset_frequent_genres.reviews.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_frequent_genres['reviews_length'][dataset_frequent_genres['reviews_length']==1])\n",
    "\n",
    "# Since we don't want to loose 3326 movies, we will keep only the first review for each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_frequent_genres.loc[:, 'reviews_pruned'] = dataset_frequent_genres.reviews.apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We observed that a plain text of a reviews as such of a plot summary, contain a lot of stop-words, punctuations and \"noisy\" words\n",
    "that could spoil the results of a text classification model.\n",
    "\"\"\"\n",
    "print(\"Raw text of a movie review:\", dataset_frequent_genres.reviews_pruned.iloc[7])\n",
    "print('\\n')\n",
    "print(\"Raw text of a plot summary: \", dataset_frequent_genres['plot'].iloc[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #3 Unify (join) the columns of Actors and Reviews in order to achive a dataframe cell with a unique TEXT (corpus) and not a LIST of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_single_letter_actors(cast_list):\n",
    "\n",
    "    cleaned_actors=[actor for actor in cast_list if len(actor)>=4]\n",
    "    \n",
    "    return cleaned_actors\n",
    "    \n",
    "def apply_clean_single_letter_actors(column_name, dataset):\n",
    "    \n",
    "    tqdm.pandas()\n",
    "    \n",
    "    dataset.loc[:, 'actors_cleaned'] = dataset.loc[:, column_name].progress_apply(lambda x: clean_single_letter_actors(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_clean_single_letter_actors(\"actors\", dataset_frequent_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_frequent_genres.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the movies do not contain actors with names of single, 2, or 3 letters\n",
    "mask=dataset_frequent_genres.actors_cleaned.explode().str.len().eq(6)\n",
    "res=dataset_frequent_genres[['title', 'actors_cleaned']].loc[np.unique(mask.loc[mask].index)]\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 1: Actors\n",
    "def unify_actors(row):\n",
    "    return ','.join(row['actors_cleaned']).strip()\n",
    "\n",
    "# Function 2: Reviews\n",
    "def unify_reviews(row):\n",
    "    return ', '.join(row['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_frequent_genres['actors_unified'] = dataset_frequent_genres.apply(unify_actors, axis=1)\n",
    "dataset_frequent_genres['reviews_unified'] = dataset_frequent_genres.apply(unify_reviews, axis=1)\n",
    "\n",
    "print(\"Actors before: {}\".format(dataset_frequent_genres.actors.iloc[0]))\n",
    "print(\"\\nActors after: {}\\n\".format(dataset_frequent_genres.actors_unified.iloc[0]))\n",
    "\n",
    "print(\"\\nReviews before: {}\".format(dataset_frequent_genres.reviews.iloc[0]))\n",
    "print(\"\\nReviews after: {}\".format(dataset_frequent_genres.reviews_unified.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #4 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions used across the whole notebook.\n",
    "Those functions are explisetely used to pre-process the raw data input of texts\n",
    "\"\"\"\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "# Function 1\n",
    "nlp=spacy.load('en_core_web_md')\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific contractions & number warnings\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"multimillion\", \"multi million\", phrase)\n",
    "    phrase = re.sub(r\"multibillion\", \"multi billion\", phrase)\n",
    "    phrase = re.sub(r\"trillion\", \"1000000000000\", phrase)\n",
    "    phrase = re.sub(r\"billion\", \"1000000000\", phrase)\n",
    "    phrase = re.sub(r\"crore\", \"10000000\", phrase)\n",
    "    phrase = re.sub(r\"mln\", \"1000000\", phrase)\n",
    "    phrase = re.sub(r\"III\", \"3\", phrase)\n",
    "    phrase = re.sub(r\"II\", \"2\", phrase)\n",
    "    phrase = re.sub(r\"iii\", \"3\", phrase)\n",
    "    phrase = re.sub(r\"world war ii\", \"world war 2\", phrase)\n",
    "    phrase = re.sub(r\"world war i\", \"world war 1\", phrase)\n",
    "    \n",
    "    # specific phrases mismatched as NUM part of speach\n",
    "    phrase = re.sub(r\"HEADER\", \"The movie\", phrase)\n",
    "    phrase = re.sub(r\"named V\", \"\", phrase)\n",
    "    phrase = re.sub(r\"(die fetten jahre sind vorbei)\", \"\", phrase)\n",
    "    phrase = re.sub(r\"thiry\", \"Thiry\", phrase)\n",
    "    phrase = re.sub(r\"Kirsten deLohr Helland\", \"Kirsten Helland\", phrase)\n",
    "    phrase = re.sub(r\"a.k.a\", \"\", phrase)\n",
    "    phrase = re.sub(r'(?<=[.,\"])(?=[^\\s])', \" \", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def correct_words_with_punctuation(phrase):\n",
    "    \n",
    "    phrase = re.sub(r\"sci-fi\", \"science fiction\", phrase)\n",
    "    phrase = re.sub(r\"N-bomb\", \"nuclear bomb\", phrase)\n",
    "    phrase = re.sub(r\"U-boat\", \"submarine\", phrase)\n",
    "    \n",
    "    return phrase\n",
    "    \n",
    "def preprocess_text_movie_content(raw_text): #Movie Content aka column name: movie_features\n",
    "    \n",
    "    # 1.Convert Accented Characters to ASCII\n",
    "    raw_text = unidecode.unidecode(raw_text)\n",
    "    \n",
    "    #------------------------------------------------\n",
    "    \n",
    "    # 2.1 Expand Contractions\n",
    "    raw_text_decontracted=decontracted(raw_text)\n",
    "    \n",
    "    # 2.2 Correct punctuation\n",
    "    raw_text_correct_punctuation=correct_words_with_punctuation(raw_text_decontracted)\n",
    "    #------------------------------------------------\n",
    "    \n",
    "    # 3.Remove punctuation\n",
    "    re_punc=re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "    stripped_punctuation=[re_punc.sub(' ', w) for w in raw_text_correct_punctuation.split(' ')]\n",
    "    \n",
    "    #------------------------------------------------\n",
    "    \n",
    "    # 4.Strip white space\n",
    "    stripped_white_space=[w.strip() for w in stripped_punctuation]\n",
    "    \n",
    "    #------------------------------------------------\n",
    "    \n",
    "    # 5.Remove numbers\n",
    "    # otpion 1: strip only existed numbers\n",
    "    # stripped=[token for token in stripped if token.isalpha()]\n",
    "    \n",
    "    # otpion 2: transform \"three\" to \"3\" and also strip it from the sentence\n",
    "    # 5.1 Strip Dates\n",
    "    stripped_string_format=' '.join(stripped_white_space)\n",
    "    stripped_date_string=re.sub(r'\\w*\\d\\w*', \"\", stripped_string_format).strip()\n",
    "    \n",
    "    # 5.2 From text to numeric form and delete\n",
    "    doc=nlp(stripped_date_string)\n",
    "    tokens = [w2n.word_to_num(token.text) if token.pos_ == 'NUM' and token.text not in ['N', 'm', 'V'] else token for token in doc]\n",
    "    \n",
    "    stripped = [i.text if not str(i).isnumeric() else str(i) for i in tokens]\n",
    "    tokens_white_space_stripped_again=[w.strip() for w in stripped]\n",
    "\n",
    "    stripped_no_numbers = [i for i in tokens_white_space_stripped_again if not i.isnumeric()]\n",
    "    \n",
    "    stripped_no_numbers = list(filter(None, stripped_no_numbers))\n",
    "    \n",
    "    #------------------------------------------------\n",
    "    \n",
    "    # 6.Remove stop words\n",
    "    stop_words=text.ENGLISH_STOP_WORDS.union([\"book\"])\n",
    "    \n",
    "    no_stopword_text=[word for word in stripped_no_numbers if not word.lower() in stop_words]\n",
    "    \n",
    "    no_stopword_text = ' '.join(no_stopword_text) #i joined the text once more because a new lemmatizing approach is implemented below\n",
    "    #------------------------------------------------\n",
    "    \n",
    "    # 7.Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    #approach 1: lemmatized_text = [lemmatizer.lemmatize(word, pos='v') for word in stripped]\n",
    "    #approach 1 was used until 21.02.2020, although we observed that only some of the tokens were lemmatized while others not.\n",
    "    #Thus, we developed an alternative approach like below to lemmatize as many tokens/words as possible\n",
    "    \n",
    "    #approach 2 developed on 22.02.2020:\n",
    "    lemmatized_text = [lemmatizer.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else lemmatizer.lemmatize(i) for i,j in pos_tag(word_tokenize(no_stopword_text))]\n",
    "\n",
    "    #------------------------------------------------\n",
    "    \n",
    "    # 8.Lowercase text\n",
    "    lowercase_text = [word.lower() for word in lemmatized_text]\n",
    "\n",
    "    #------------------------------------------------\n",
    "    lowercase_text=' '.join(lowercase_text)\n",
    "    return lowercase_text\n",
    "\n",
    "def preprocess_text_reviews(raw_text):\n",
    "    \n",
    "    # 1.Convert Accented Characters to ASCII\n",
    "    raw_text = unidecode.unidecode(raw_text)\n",
    "\n",
    "    # 2.Expand Contractions\n",
    "    raw_text_decontracted=decontracted(raw_text)\n",
    "\n",
    "    #------------------------------------------------\n",
    "\n",
    "    # 3.Remove punctuation\n",
    "    re_punc=re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "    stripped_punctuation=[re_punc.sub(' ', w) for w in raw_text_decontracted.split(' ')]\n",
    "\n",
    "    #------------------------------------------------\n",
    "\n",
    "    # 4.Strip white space\n",
    "    stripped_white_space=[w.strip() for w in stripped_punctuation]\n",
    "\n",
    "    #------------------------------------------------    \n",
    "    # 5.Remove numbers\n",
    "    # otpion 1: strip only existed numbers\n",
    "    # stripped=[token for token in stripped if token.isalpha()]\n",
    "    \n",
    "    # otpion 2: Strip numerical text\n",
    "    # 5.1 Strip Dates\n",
    "    stripped_string_format=' '.join(stripped_white_space)\n",
    "    stripped_date_string=re.sub(r'\\w*\\d\\w*', \"\", stripped_string_format).strip()\n",
    "\n",
    "    tokens=stripped_date_string.split(' ')\n",
    "    \n",
    "    tokens_white_space_stripped_again=[w.strip() for w in tokens]\n",
    "    \n",
    "    stripped_no_numbers = [i for i in tokens_white_space_stripped_again if not i.isnumeric()]\n",
    "    \n",
    "    stripped_no_numbers = list(filter(None, stripped_no_numbers))\n",
    "    #------------------------------------------------\n",
    "    \n",
    "    # 6.Remove stop words\n",
    "    stop_words=text.ENGLISH_STOP_WORDS.union([\"book\"])\n",
    "    \n",
    "    no_stopword_text=[word for word in stripped_no_numbers if not word.lower() in stop_words]\n",
    "    \n",
    "    no_stopword_text = ' '.join(no_stopword_text) #i joined the text once more because a new lemmatizing approach is implemented below\n",
    "\n",
    "    #------------------------------------------------\n",
    "    \n",
    "    # 7.Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    #approach 1: lemmatized_text = [lemmatizer.lemmatize(word, pos='v') for word in stripped]\n",
    "    #approach 1 was used until 21.02.2020, although we observed that only some of the tokens were lemmatized while others not.\n",
    "    #Thus, we developed an alternative approach like below to lemmatize as many tokens/words as possible\n",
    "    \n",
    "    #approach 2 developed on 22.02.2020:\n",
    "    lemmatized_text = [lemmatizer.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else lemmatizer.lemmatize(i) for i,j in pos_tag(word_tokenize(no_stopword_text))]\n",
    "   \n",
    "    #------------------------------------------------\n",
    "    \n",
    "    # 8.Lowercase text\n",
    "    lowercase_text = [word.lower() for word in lemmatized_text]\n",
    "\n",
    "    lowercase_text=' '.join(lowercase_text)\n",
    "    \n",
    "    return lowercase_text\n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "def transform_actors(column_name, dataset):\n",
    "    \n",
    "    tqdm.pandas()\n",
    "    \n",
    "    dataset.loc[:, 'clean_actors'] = dataset.loc[:, column_name].progress_apply(lambda x: x.lower()) #if column \"actors_unified\" is used. Because this transformation is applied on a single element and not on list elements.\n",
    "\n",
    "def transform_plot(column_name, dataset):\n",
    "    \n",
    "    tqdm.pandas()\n",
    "    \n",
    "    dataset.loc[:, 'clean_plot_summary'] = dataset.loc[:, column_name].progress_apply(lambda x: preprocess_text_movie_content(x))\n",
    "\n",
    "def transform_features(column_name, dataset):\n",
    "    \n",
    "    tqdm.pandas()\n",
    "    \n",
    "    dataset.loc[:, 'clean_combined_features'] = dataset.loc[:, column_name].progress_apply(lambda x: preprocess_text_movie_content(x))\n",
    "    \n",
    "def transform_reviews(column_name, dataset):\n",
    "    \n",
    "    tqdm.pandas()\n",
    "    \n",
    "    dataset.loc[:, 'clean_reviews'] = dataset.loc[:, column_name].progress_apply(lambda x: preprocess_text_reviews(x))\n",
    "\n",
    "def transform_movie_title(column_name, dataset): # added on 12.07.2020 in an attempt to plot similar movies\n",
    "    \n",
    "    tqdm.pandas()\n",
    "    \n",
    "    dataset.loc[:, 'clean_movie_title'] = dataset.loc[:, column_name].progress_apply(lambda x: x.lower())\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "#Function 2\n",
    "\n",
    "def split_dataset(method, labels, dataset, split_ratio):\n",
    "    \"\"\"\n",
    "    Random shuffle split, with an option to split it into a stratified manner.\n",
    "    However, when the stratified method was tested it didn't work out.\n",
    "    \n",
    "    Thus, we created a second function using the StratifiedShuffleSplit of the sklearn module.\n",
    "    \"\"\"\n",
    "    #As mentioned earler \"reduced genres\" are now used and NOT the column \"genres\"\n",
    "    X = dataset[['title', 'clean_actors', 'clean_plot_summary', 'clean_combined_features', 'clean_reviews', 'clean_movie_title', 'reduced_genres']]\n",
    "    \n",
    "    y = labels\n",
    "    \n",
    "    if method==\"stratified\":\n",
    "    \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_ratio, random_state=123, shuffle= True, stratify=y)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_ratio, random_state=123, shuffle= True)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "# Function 3\n",
    "\n",
    "def keras_tokenization(variable, maximum_words, dataset, x_train, x_test):\n",
    "    \"\"\"\n",
    "    The keras tokenization method that will transform a sentence of text to a sequence of tokens, mapping each token to an index. An Out-of-Vocabulary token is also created, to map the word not having an integer index.\n",
    "    \"\"\"\n",
    "    #The tokenizer class has some main buggs when the word index mapping is created. So the function is assembled based on this GitHub post https://github.com/keras-team/keras/issues/8092#issuecomment-372833486\n",
    "    \n",
    "    if variable == \"actors\":\n",
    "        \n",
    "        actors_tokenizer = Tokenizer(num_words=maximum_words + 1,  filters=\",\", lower=True, split=',', oov_token = '<OOV>')\n",
    "\n",
    "        actors_tokenizer.fit_on_texts(list(dataset.loc[:, 'clean_actors']))\n",
    "        \n",
    "        print(\"Maximum length of unique tokens is: {0}\".format(len(actors_tokenizer.word_index)))\n",
    "        \n",
    "        words_to_tokenize=int(np.ceil(len(actors_tokenizer.word_index)*0.95))\n",
    "        \n",
    "        print(\"Number of words to be tokenized is the 95% of those unique tokens, equal to: {0}\\nThe rest 5% or {1} is not tokenized\".format(words_to_tokenize, (len(actors_tokenizer.word_index)-words_to_tokenize)))\n",
    "\n",
    "        actors_tokenizer.word_index = {e:i for e,i in actors_tokenizer.word_index.items() if i <= maximum_words}\n",
    "        \n",
    "        actors_tokenizer.word_index = {x.strip(): v for x, v in actors_tokenizer.word_index.items()}\n",
    "        \n",
    "        actors_tokenizer.word_index[actors_tokenizer.oov_token] = maximum_words + 1\n",
    "\n",
    "        print(\"Number of words mapped: {0}\".format(len(actors_tokenizer.word_index)))\n",
    "\n",
    "        x_train.loc[:, 'actors_seqs'] = actors_tokenizer.texts_to_sequences(x_train.loc[:, 'clean_actors'])\n",
    "    \n",
    "        x_test.loc[:, 'actors_seqs'] = actors_tokenizer.texts_to_sequences(x_test.loc[:, 'clean_actors'])\n",
    "\n",
    "        vocabulary_size_frequent_words = len(actors_tokenizer.word_index)\n",
    "        \n",
    "        try:\n",
    "            assert len(actors_tokenizer.word_index)==maximum_words\n",
    "        except AssertionError:\n",
    "            print(\"ERROR: The length of the vocabulary is not equal to the number of word_index dictionary, but rather is equal to: {0}\\n\\nCorrect length: {1}\".format(len(actors_tokenizer.word_index), maximum_words))\n",
    "        \n",
    "        tokenizer = actors_tokenizer\n",
    "            \n",
    "    elif variable == \"plot\":\n",
    "        \n",
    "        plot_tokenizer = Tokenizer(num_words=maximum_words + 1, filters=\" \", lower=True, split=' ', oov_token = '<OOV>')\n",
    "        \n",
    "        plot_tokenizer.fit_on_texts(list(dataset.loc[:, 'clean_plot_summary']))\n",
    "        \n",
    "        print(\"Maximum length of unique tokens is: {0}\".format(len(plot_tokenizer.word_index)))\n",
    "        \n",
    "        words_to_tokenize=int(np.ceil(len(plot_tokenizer.word_index)*0.95))\n",
    "        \n",
    "        print(\"Number of words to be tokenized is the 95% of those unique tokens, equal to: {0}\\nThe rest 5% or {1} tokens are not tokenized\".format(words_to_tokenize, (len(plot_tokenizer.word_index)-words_to_tokenize)))\n",
    "        \n",
    "        plot_tokenizer.word_index = {e:i for e,i in plot_tokenizer.word_index.items() if i <= maximum_words}\n",
    "        \n",
    "        plot_tokenizer.word_index = {x.strip(): v for x, v in plot_tokenizer.word_index.items()}\n",
    "        \n",
    "        plot_tokenizer.word_index[plot_tokenizer.oov_token] = maximum_words + 1\n",
    "        \n",
    "        print(\"Number of words mapped: {0}\".format(len(plot_tokenizer.word_index)))\n",
    "\n",
    "        x_train.loc[:, 'plot_summary_seqs'] = plot_tokenizer.texts_to_sequences(x_train.loc[:, 'clean_plot_summary'])\n",
    "        \n",
    "        x_test.loc[:, 'plot_summary_seqs'] = plot_tokenizer.texts_to_sequences(x_test.loc[:, 'clean_plot_summary'])\n",
    "\n",
    "        vocabulary_size_frequent_words = len(plot_tokenizer.word_index)\n",
    "        \n",
    "        try:\n",
    "            assert len(plot_tokenizer.word_index)==maximum_words\n",
    "        except AssertionError:\n",
    "            print(\"ERROR: The length of the vocabulary is not equal to the number of word_index dictionary, but rather equal to: {0}\\n\\nCorrect length: {1}\".format(len(plot_tokenizer.word_index), maximum_words))\n",
    "\n",
    "        tokenizer = plot_tokenizer\n",
    "        \n",
    "    elif variable == \"features\":\n",
    "        \n",
    "        combined_features_tokenizer = Tokenizer(num_words=maximum_words + 1, filters=\" \", lower=True, split=' ', oov_token = '<OOV>')\n",
    "        \n",
    "        combined_features_tokenizer.fit_on_texts(list(dataset.loc[:, 'clean_combined_features']))\n",
    "        \n",
    "        print(\"Maximum length of unique tokens is: {0}\".format(len(combined_features_tokenizer.word_index)))\n",
    "        \n",
    "        words_to_tokenize=int(np.ceil(len(combined_features_tokenizer.word_index)*0.95))\n",
    "        \n",
    "        print(\"Number of words to be tokenized is the 95% of those unique tokens, equal to: {0}\\nThe rest 5% or {1} is not tokenized\".format(words_to_tokenize, (len(combined_features_tokenizer.word_index)-words_to_tokenize)))\n",
    "\n",
    "        combined_features_tokenizer.word_index = {e:i for e,i in combined_features_tokenizer.word_index.items() if i <= maximum_words}\n",
    "        \n",
    "        combined_features_tokenizer.word_index = {x.strip(): v for x, v in combined_features_tokenizer.word_index.items()}\n",
    "        \n",
    "        combined_features_tokenizer.word_index[combined_features_tokenizer.oov_token] = maximum_words + 1\n",
    "        \n",
    "        print(\"Number of words mapped: {0}\".format(len(combined_features_tokenizer.word_index)))\n",
    "\n",
    "        x_train.loc[:, 'combined_features_seqs'] = combined_features_tokenizer.texts_to_sequences(x_train.loc[:, 'clean_combined_features'])\n",
    "        \n",
    "        x_test.loc[:, 'combined_features_seqs'] = combined_features_tokenizer.texts_to_sequences(x_test.loc[:, 'clean_combined_features'])\n",
    "\n",
    "        vocabulary_size_frequent_words = len(combined_features_tokenizer.word_index)\n",
    "        \n",
    "        try:\n",
    "            assert len(combined_features_tokenizer.word_index)==maximum_words\n",
    "        except AssertionError:\n",
    "            print(\"ERROR: The length of the vocabulary is not equal to the number of word_index dictionary, but rather equal to: {0}\\n\\nCorrect length: {1}\".format(len(combined_features_tokenizer.word_index), maximum_words))\n",
    "\n",
    "        tokenizer = combined_features_tokenizer\n",
    "        \n",
    "    elif variable == \"reviews\":\n",
    "        \n",
    "        reviews_tokenizer = Tokenizer(num_words=maximum_words + 1, lower=True, filters=\" \", split=' ', oov_token = '<OOV>')\n",
    "        \n",
    "        reviews_tokenizer.fit_on_texts(dataset.loc[:, 'clean_reviews'])\n",
    "        \n",
    "        print(\"Maximum length of unique tokens is: {0}\".format(len(reviews_tokenizer.word_index)))\n",
    "        \n",
    "        words_to_tokenize=int(np.ceil(len(reviews_tokenizer.word_index)*0.95))\n",
    "        \n",
    "        print(\"Number of words to be tokenized is the 95% of those unique tokens, equal to: {0}\\nThe rest 5% or {1} is not tokenized\".format(maximum_words, (len(reviews_tokenizer.word_index)-words_to_tokenize)))\n",
    "\n",
    "        reviews_tokenizer.word_index = {e:i for e,i in reviews_tokenizer.word_index.items() if i <= maximum_words}\n",
    "        \n",
    "        reviews_tokenizer.word_index = {x.strip(): v for x, v in reviews_tokenizer.word_index.items()}\n",
    "\n",
    "        reviews_tokenizer.word_index[reviews_tokenizer.oov_token] = maximum_words + 1\n",
    "        \n",
    "        print(\"Number of words mapped: {0}\".format(len(reviews_tokenizer.word_index)))\n",
    "\n",
    "        x_train.loc[:, 'reviews_seqs'] = reviews_tokenizer.texts_to_sequences(x_train.loc[:, 'clean_reviews'])\n",
    "        \n",
    "        x_test.loc[:, 'reviews_seqs'] = reviews_tokenizer.texts_to_sequences(x_test.loc[:, 'clean_reviews'])\n",
    "\n",
    "        vocabulary_size_frequent_words = len(reviews_tokenizer.word_index)\n",
    "        \n",
    "        try:\n",
    "            assert len(reviews_tokenizer.word_index)==maximum_words\n",
    "        except AssertionError:\n",
    "            print(\"ERROR: The length of the vocabulary is not equal to the number of word_index dictionary, but rather equal to: {0}\\n\\nCorrect length: {1}\".format(len(reviews_tokenizer.word_index), maximum_words))\n",
    "\n",
    "        tokenizer = reviews_tokenizer\n",
    "        \n",
    "    elif  variable == \"movie title\":\n",
    "        \n",
    "        movie_title_tokenizer = Tokenizer(num_words=maximum_words + 1, lower=True, filters=\" \", split=' ', oov_token = '<OOV>')\n",
    "\n",
    "        movie_title_tokenizer.fit_on_texts(dataset.loc[:, 'clean_movie_title'])\n",
    "        \n",
    "        print(\"Maximum length of unique tokens is: {0}\".format(len(movie_title_tokenizer.word_index)))\n",
    "        \n",
    "        words_to_tokenize=int(np.ceil(len(movie_title_tokenizer.word_index)*0.95))\n",
    "        \n",
    "        print(\"Number of words to be tokenized is the 95% of those unique tokens, equal to: {0}\\nThe rest 5% or {1} is not tokenized\".format(maximum_words, (len(movie_title_tokenizer.word_index)-words_to_tokenize)))\n",
    "\n",
    "        movie_title_tokenizer.word_index = {e:i for e,i in movie_title_tokenizer.word_index.items() if i <= maximum_words}\n",
    "\n",
    "        movie_title_tokenizer.word_index = {x.strip(): v for x, v in movie_title_tokenizer.word_index.items()}\n",
    "\n",
    "        movie_title_tokenizer.word_index[movie_title_tokenizer.oov_token] = maximum_words + 1\n",
    "        \n",
    "        print(\"Number of words mapped: {0}\".format(len(movie_title_tokenizer.word_index)))\n",
    "        \n",
    "        x_train.loc[:, 'movie_title_seqs'] = movie_title_tokenizer.texts_to_sequences(x_train.loc[:, 'clean_movie_title'])\n",
    "        \n",
    "        x_test.loc[:, 'movie_title_seqs'] = movie_title_tokenizer.texts_to_sequences(x_test.loc[:, 'clean_movie_title'])\n",
    "\n",
    "        vocabulary_size_frequent_words = len(movie_title_tokenizer.word_index)\n",
    "        \n",
    "        try:\n",
    "            assert len(movie_title_tokenizer.word_index)==maximum_words\n",
    "        except AssertionError:\n",
    "            print(\"ERROR: The length of the vocabulary is not equal to the number of word_index dictionary, but rather equal to: {0}\\n\\nCorrect length: {1}\".format(len(movie_title_tokenizer.word_index), maximum_words))\n",
    "\n",
    "        tokenizer = movie_title_tokenizer\n",
    "        \n",
    "    return vocabulary_size_frequent_words, tokenizer\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "# Function 4\n",
    "\n",
    "def mean(numbers):\n",
    "    return int(np.ceil(float(sum(numbers)) / max(len(numbers), 1)))\n",
    "\n",
    "def padding_sequnce_length(variable, x_train, x_test):\n",
    "    \"\"\"\n",
    "    Find the maximum length of the sequences belonging to a column. The maximum length of the column's sequence is equal to the 95% length of all the sequences. So for example if the dataset has 10,000 sequences and 9,500\n",
    "    of them have length 20 then all the sequences will either be cropped or extended to 20 integers.\n",
    "    \"\"\"\n",
    "    if variable == \"actors\":\n",
    "    \n",
    "        all_train_lengths =  list(x_train.actors_seqs.apply(len))\n",
    "        all_test_lengths =  list(x_test.actors_seqs.apply(len))\n",
    "\n",
    "        maxlen_train = int(np.percentile(all_train_lengths, q=95))\n",
    "        maxlen_test = int(np.percentile(all_test_lengths, q=95))\n",
    "        \n",
    "        if maxlen_train!=maxlen_test:\n",
    "            maxlen_value=mean([maxlen_train, maxlen_test])\n",
    "        else:\n",
    "            maxlen_value=maxlen_train\n",
    "        \n",
    "        print(\"Max Length of the pad sequence for Actors: {0}\".format(maxlen_value))\n",
    "        \n",
    "    elif variable == \"plot\":\n",
    "        \n",
    "        all_train_lengths =  list(x_train.plot_summary_seqs.apply(len))\n",
    "        all_test_lengths =  list(x_test.plot_summary_seqs.apply(len))\n",
    "\n",
    "        maxlen_train = int(np.percentile(all_train_lengths, q=95))\n",
    "        maxlen_test = int(np.percentile(all_test_lengths, q=95))\n",
    "        \n",
    "        if maxlen_train!=maxlen_test:\n",
    "            maxlen_value=mean([maxlen_train, maxlen_test])\n",
    "        else:\n",
    "            maxlen_value=maxlen_train\n",
    "        \n",
    "        print(\"Max Length of the pad sequence for Plot Summary: {0}\".format(maxlen_value))\n",
    "\n",
    "    elif variable == \"features\":\n",
    "        \n",
    "        all_train_lengths =  list(x_train.combined_features_seqs.apply(len))\n",
    "        all_test_lengths =  list(x_test.combined_features_seqs.apply(len))\n",
    "\n",
    "        maxlen_train = int(np.percentile(all_train_lengths, q=95))\n",
    "        maxlen_test = int(np.percentile(all_test_lengths, q=95))\n",
    "        \n",
    "        if maxlen_train!=maxlen_test:\n",
    "            maxlen_value=mean([maxlen_train, maxlen_test])\n",
    "        else:\n",
    "            maxlen_value=maxlen_train\n",
    "        \n",
    "        print(\"Max Length of the pad sequence for Movie Features: {0}\".format(maxlen_value))\n",
    "        \n",
    "    elif variable == \"reviews\":\n",
    "        \n",
    "        all_train_lengths =  list(x_train.reviews_seqs.apply(len))\n",
    "        all_test_lengths =  list(x_test.reviews_seqs.apply(len))\n",
    "\n",
    "        maxlen_train = int(np.percentile(all_train_lengths, q=95))\n",
    "        maxlen_test = int(np.percentile(all_test_lengths, q=95))\n",
    "        \n",
    "        if maxlen_train!=maxlen_test:\n",
    "            maxlen_value=mean([maxlen_train, maxlen_test])\n",
    "        else:\n",
    "            maxlen_value=maxlen_train\n",
    "        \n",
    "        print(\"Max Length of the pad sequence for Movie Reviews: {0}\".format(maxlen_value))\n",
    "        \n",
    "    elif variable == \"movie title\":\n",
    "\n",
    "        all_train_lengths =  list(x_train.movie_title_seqs.apply(len))\n",
    "        all_test_lengths =  list(x_test.movie_title_seqs.apply(len))\n",
    "\n",
    "        maxlen_train = int(np.percentile(all_train_lengths, q=95))\n",
    "        maxlen_test = int(np.percentile(all_test_lengths, q=95))\n",
    "        \n",
    "        if maxlen_train!=maxlen_test:\n",
    "            maxlen_value=mean([maxlen_train, maxlen_test])\n",
    "        else:\n",
    "            maxlen_value=maxlen_train\n",
    "        \n",
    "        print(\"Max Length of the pad sequence for Movie Title: {0}\".format(maxlen_value))\n",
    "        \n",
    "    return maxlen_value\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "# Function 5\n",
    "\n",
    "# the input data for a deep learning model must be a single tensor (of shape e.g. (batch_size, 6, vocab_size), \n",
    "# samples that are shorter than the longest item need to be padded with some placeholder value.\n",
    "\n",
    "#url https://www.tensorflow.org/guide/keras/masking_and_padding\n",
    "\n",
    "def padding_sequence(variable, x_train, x_test, y_train, y_test, maxlen):\n",
    "    \"\"\"\n",
    "    Apply the padding based on the maximum length retrieved from the function 4.1\n",
    "    \"\"\"\n",
    "    if variable == \"actors\":\n",
    "        \n",
    "        x_train_seq = pad_sequences(x_train.loc[:, 'actors_seqs'], padding='post', maxlen=maxlen)\n",
    "    \n",
    "        x_test_seq = pad_sequences(x_test.loc[:, 'actors_seqs'], padding='post', maxlen=maxlen)\n",
    "        \n",
    "        assert len(x_train_seq) == len(y_train) # x_train_seq\n",
    "\n",
    "        assert len(x_test_seq) == len(y_test) # x_test_seq\n",
    "        \n",
    "    elif variable == \"plot\":\n",
    "        \n",
    "        x_train_seq = pad_sequences(x_train.loc[:, 'plot_summary_seqs'], padding='post', maxlen=maxlen)\n",
    "    \n",
    "        x_test_seq = pad_sequences(x_test.loc[:, 'plot_summary_seqs'], padding='post', maxlen=maxlen)\n",
    "\n",
    "        assert len(x_train_seq) == len(y_train)\n",
    "\n",
    "        assert len(x_test_seq) == len(y_test)\n",
    "        \n",
    "    elif variable == \"features\":\n",
    "        \n",
    "        x_train_seq = pad_sequences(x_train.loc[:, 'combined_features_seqs'], padding='post', maxlen=maxlen)\n",
    "    \n",
    "        x_test_seq = pad_sequences(x_test.loc[:, 'combined_features_seqs'], padding='post', maxlen=maxlen)\n",
    "\n",
    "        assert len(x_train_seq) == len(y_train)\n",
    "\n",
    "        assert len(x_test_seq) == len(y_test)\n",
    "        \n",
    "    elif variable == \"reviews\":\n",
    "        \n",
    "        x_train_seq = pad_sequences(x_train.loc[:, 'reviews_seqs'], padding='post', maxlen=maxlen)\n",
    "    \n",
    "        x_test_seq = pad_sequences(x_test.loc[:, 'reviews_seqs'], padding='post', maxlen=maxlen)\n",
    "\n",
    "        assert len(x_train_seq) == len(y_train)\n",
    "\n",
    "        assert len(x_test_seq) == len(y_test)\n",
    "        \n",
    "    elif variable == \"movie title\":\n",
    "        \n",
    "        x_train_seq = pad_sequences(x_train.loc[:, 'movie_title_seqs'], padding='post', maxlen=maxlen)\n",
    "    \n",
    "        x_test_seq = pad_sequences(x_test.loc[:, 'movie_title_seqs'], padding='post', maxlen=maxlen)\n",
    "\n",
    "        assert len(x_train_seq) == len(y_train)\n",
    "\n",
    "        assert len(x_test_seq) == len(y_test)\n",
    "        \n",
    "    return x_train_seq, x_test_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Previously we experinced an error using the stratified sampling. Below we printed the number of genre sequences that are assigned to only one movie.\n",
    "For those 131 movies the stratified sampling is failing to complete.\n",
    "Thus, we should find their indexes and remove them. The final dataset should contain 49122-131=48991\n",
    "\"\"\"\n",
    "list_of_movies_to_remove=None\n",
    "print(\"Number of movies that are assigned to only 1 sequence of genres: \", len(dataset_frequent_genres[\"reduced_genres\"].apply(tuple).value_counts()[dataset_frequent_genres[\"reduced_genres\"].apply(tuple).value_counts()==1]), '\\n')\n",
    "list_of_movies_to_remove=dataset_frequent_genres[\"reduced_genres\"].apply(tuple).value_counts()[dataset_frequent_genres[\"reduced_genres\"].apply(tuple).value_counts()==1].index.tolist()\n",
    "list_of_movies_to_remove=[list(x) for x in list_of_movies_to_remove]\n",
    "assert type(list_of_movies_to_remove[0]) is list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Below are the indexes of rows that should be removed from the dataset. In total 131 indexes.\n",
    "With those final 48991 rows of the dataset, the stratified sampling will be successfully completed.\n",
    "\"\"\"\n",
    "indexes_to_remove=dataset_frequent_genres['reduced_genres'].map(lambda x: 1 if list(x) in list_of_movies_to_remove else 0)[dataset_frequent_genres['reduced_genres'].map(lambda x: 1 if x in list_of_movies_to_remove else 0)==1].index.tolist()\n",
    "dataset_frequent_genres=dataset_frequent_genres[~dataset_frequent_genres.index.isin(indexes_to_remove)]\n",
    "dataset_frequent_genres=dataset_frequent_genres.reset_index(drop=True)\n",
    "dataset_frequent_genres.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "year_list=dataset_frequent_genres['year'].values.tolist()\n",
    "\n",
    "with open('pickled_data_per_part\\\\year_list_{0}.pkl'.format(version_data_control), 'wb') as f:\n",
    "    pickle.dump(year_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_title_list=dataset_frequent_genres['title'].values.tolist()\n",
    "\n",
    "with open('pickled_data_per_part\\\\movie_title_list_{0}.pkl'.format(version_data_control), 'wb') as f:\n",
    "    pickle.dump(movie_title_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code shell may not be executed since it's already pickled after the transformation functions have been applied to each\n",
    "column, which will be later used as model input.\n",
    "\"\"\"\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "# Transfrom the columns:\n",
    "# -> Actors\n",
    "# -> Plot summary\n",
    "# -> Movie Features\n",
    "# -> Reviews\n",
    "# -> Movie Title\n",
    "\n",
    "print(\"Start Execution\")\n",
    "begin_time=time.time()\n",
    "print(\"---------------------------------------------------------------------------------\\n\")\n",
    "print(\"Transfrom the column of the actors\")\n",
    "start_time_one=time.time()\n",
    "transform_actors(\"actors_unified\", dataset_frequent_genres) # function 3: transform_actors\n",
    "print(\"Finished the actors transformation after: {0}\\n\".format(format_timespan(time.time()-start_time_one)))\n",
    "\n",
    "print(\"Transfrom the column of the plot summary\")\n",
    "start_time_two=time.time()\n",
    "transform_plot(\"plot\", dataset_frequent_genres) # function 3: transform_plot\n",
    "print(\"Finished the plot transformation after: {0}\\n\".format(format_timespan(time.time()-start_time_two)))\n",
    "\n",
    "print(\"Transfrom the column of the movie features\")\n",
    "start_time_three=time.time()\n",
    "transform_features(\"movie_features\", dataset_frequent_genres) # function 3: transform_features\n",
    "print(\"Finished the movie_features transformation after: {0}\\n\".format(format_timespan(time.time()-start_time_three)))\n",
    "\n",
    "print(\"Transfrom the column of the movie reviews\")\n",
    "start_time_four=time.time()\n",
    "transform_reviews(\"reviews_pruned\", dataset_frequent_genres) # function 3: transform_reviews\n",
    "print(\"Finished the reviews_pruned transformation after: {0}\".format(format_timespan(time.time()-start_time_four)))\n",
    "\n",
    "print(\"Transfrom the column of the movie title\")\n",
    "start_time_five=time.time()\n",
    "transform_movie_title(\"title\", dataset_frequent_genres)\n",
    "print(\"Finished the movie title transformation after: {0}\".format(format_timespan(time.time()-start_time_five)))\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\\n\")\n",
    "print(\"Finished Execution after: {0}\".format(format_timespan(time.time()-begin_time)))\n",
    "\n",
    "# Total time to transform the columns: 30 minutes and 35.95 seconds on CPU: i7 9th Generation and GPU: NVidia 1660Ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the movies do not contain actors with names of single, 2, or 3 letters\n",
    "mask=dataset_frequent_genres.clean_actors.str.split(\",\").explode().str.len().eq(4)\n",
    "res=dataset_frequent_genres[['title', 'clean_actors']].loc[np.unique(mask.loc[mask].index)]\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Before pre-processing the raw text of the first review about Toy Story. Text has been many stop words, punctuations and words in many different tense!\n",
    "\"\"\"\n",
    "dataset_frequent_genres['reviews'].iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "After pre-processing the raw text of the first review about Toy Story. Text has been lemmatized and cleaned off most of the noise!\n",
    "\"\"\"\n",
    "dataset_frequent_genres['clean_reviews'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: Stratified Shuffle Split using the train_test_split function (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the first way to split the dataset, by using the random shuffle split of the Train_test_split function offered by sklearn module\n",
    "This version was the first to be developed and followed, however we decided to try a second more robust option.\n",
    "The second option refers to the data separation into train, validation and test set using the StratifiedShuffleSPlit function developed and mainted by sklearn module.\n",
    "\n",
    "In cases of imbalanced datasets and specifically for classification models, the stratification comes in handy because it ensures that the data will be splitted uniformly and both the train and test sets, will enclude all the categorical variables.\n",
    "\"\"\"\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "# Split the dataset into train & set sets\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(\"\\nSplit the dataset into train & test sets (stratified shuffle split)\\n\")\n",
    "start_time=time.time()\n",
    "X_train, X_test, y_train, y_test = split_dataset(\"stratify\", dataset_frequent_genres.iloc[:, 13:30], dataset_frequent_genres, 0.2)\n",
    "print(\"Finished the plot transformation after: {0}\".format(format_timespan(time.time()-start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The shape of the X_train, X_test, y_train, y_test splitted and shuffled randomly\n",
    "\"\"\"\n",
    "print(\"X_train shape:{}\".format(X_train.shape))\n",
    "print(\"X_test shape:{}\".format(X_test.shape))\n",
    "print(\"y_train shape:{}\".format(y_train.shape))\n",
    "print(\"y_test shape:{}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(dataset_frequent_genres.reduced_genres.explode().value_counts(normalize=True)*100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The stratification worked!\n",
    "\"\"\"\n",
    "round(X_train.reduced_genres.explode().value_counts(normalize=True)*100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The stratification worked!\n",
    "\"\"\"\n",
    "round(X_test.reduced_genres.explode().value_counts(normalize=True)*100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The below cell serialises the X_train, X_test, y_train, y_test inputs created by the stratified split.\n",
    "\"\"\"\n",
    "X_train.to_pickle(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\X_train_all_inputs_{0}.pkl'.format(version_data_control)))\n",
    "X_test.to_pickle(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\X_test_all_inputs_{0}.pkl'.format(version_data_control)))\n",
    "y_train.to_pickle(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\y_train_all_inputs_{0}.pkl'.format(version_data_control)))\n",
    "y_test.to_pickle(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\y_test_all_inputs_{0}.pkl'.format(version_data_control)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.read_pickle(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\X_train_all_inputs_{0}.pkl'.format(version_data_control)))\n",
    "X_test=pd.read_pickle(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\X_test_all_inputs_{0}.pkl'.format(version_data_control)))\n",
    "y_train=pd.read_pickle(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\y_train_all_inputs_{0}.pkl'.format(version_data_control)))\n",
    "y_test=pd.read_pickle(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\y_test_all_inputs_{0}.pkl'.format(version_data_control)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now that the data is splitted, we separated each column of interest to a different X_train and X_test\n",
    "Those train and text X sets will be later used for tokenization and padding\n",
    "\"\"\"\n",
    "# Separate each different input column (actors, plot, features, reviews, title)\n",
    "\n",
    "X_train_actors = X_train[[\"title\", \"clean_actors\", \"reduced_genres\"]]\n",
    "X_train_plot = X_train[[\"title\", \"clean_plot_summary\", \"reduced_genres\"]]\n",
    "X_train_features = X_train[[\"title\", \"clean_combined_features\", \"reduced_genres\"]]\n",
    "X_train_reviews = X_train[[\"title\", \"clean_reviews\", \"reduced_genres\"]]\n",
    "X_train_title = X_train[[\"title\", \"clean_movie_title\", \"reduced_genres\"]]\n",
    "\n",
    "# In X_train and X_test I also use columns \"title\" and \"genres\" since they will be both used later for making inference with predictions\n",
    "assert X_train_actors.shape==X_train_plot.shape==X_train_features.shape==X_train_reviews.shape==X_train_title.shape\n",
    "\n",
    "X_test_actors = X_test[[\"title\", \"clean_actors\", \"reduced_genres\"]]\n",
    "X_test_plot = X_test[[\"title\", \"clean_plot_summary\", \"reduced_genres\"]]\n",
    "X_test_features = X_test[[\"title\", \"clean_combined_features\", \"reduced_genres\"]]\n",
    "X_test_reviews = X_test[[\"title\", \"clean_reviews\", \"reduced_genres\"]]\n",
    "X_test_title = X_test[[\"title\", \"clean_movie_title\", \"reduced_genres\"]]\n",
    "\n",
    "assert X_test_actors.shape==X_test_plot.shape==X_test_features.shape==X_test_reviews.shape==X_test_title.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the movies do not contain actors with names of single, 2, or 3 letters\n",
    "mask=X_test_actors.clean_actors.str.split(\",\").explode().str.len().eq(4)\n",
    "res=X_test_actors[['title', 'clean_actors']].loc[np.unique(mask.loc[mask].index)]\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "Token Frequency to determine best value for MAX_FREQUENCY_WORDS used later in word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Find the most frequent words among the actor names\n",
    "In the end the number of rows will be equal to the number of maximum features tokenized by the Tokenizer.\n",
    "-> Probably the infrequent tokens will make a better classification\n",
    "\"\"\"\n",
    "def actors_split(s):\n",
    "    return s.split(',')\n",
    "\n",
    "corpus_actors=dataset_frequent_genres['clean_actors'].values.tolist()\n",
    "c_vectorizer=CountVectorizer(tokenizer=actors_split, min_df=1) #keep this to 1 to include all the words/tokens\n",
    "\n",
    "c_vectorizer.fit(corpus_actors)\n",
    "print(\"Vocabulary length of CountVectorizer of the actors corpus: {0}\".format(len(c_vectorizer.vocabulary_)))\n",
    "\n",
    "X=c_vectorizer.fit_transform(corpus_actors)\n",
    "X_words=c_vectorizer.inverse_transform(X)\n",
    "\n",
    "tokens_list=c_vectorizer.get_feature_names()\n",
    "count_list = np.asarray(X.sum(axis=0)).ravel().tolist()\n",
    "\n",
    "token_frequency_df=pd.DataFrame({'term': c_vectorizer.get_feature_names(), 'token_frequency': count_list})\n",
    "token_frequency_df=token_frequency_df.sort_values(by='token_frequency', ascending=False)\n",
    "print(token_frequency_df.shape)\n",
    "#token_frequency_df_pruned_actors=token_frequency_df[token_frequency_df['token_frequency']>=3]\n",
    "#print(token_frequency_df_pruned_actors.shape)\n",
    "\n",
    "# The below code sample creates a dictionary that shows only the n-frequent actors\n",
    "actors_frequency_dictionary=dict(zip(tokens_list, count_list))\n",
    "d_actors = dict((k, v) for k, v in actors_frequency_dictionary.items() if v >= 80) # v = popularity (frequency) or number of movies played\n",
    "\n",
    "# sorted(d_actors.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of actors that exist in the dataset is: {}\".format(len(c_vectorizer.vocabulary_)))\n",
    "actors_tokenized=int(np.ceil(len(c_vectorizer.vocabulary_)*0.95))\n",
    "print(\"The 95% ({0}) of the actors will be tokenized and the rest 5% ({1}) of the actors will be removed due to sparsity\".format(actors_tokenized, (len(c_vectorizer.vocabulary_)-actors_tokenized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Find the most frequent words among the movie plots\n",
    "In the end the number of rows will be equal to the number of maximum features tokenized by the Tokenizer.\n",
    "\"\"\"\n",
    "def plot_split(s):\n",
    "    return s.split(' ')\n",
    "\n",
    "corpus_plot=dataset_frequent_genres['clean_plot_summary'].values.tolist()\n",
    "c_vectorizer=CountVectorizer(tokenizer=plot_split, min_df=1)\n",
    "\n",
    "c_vectorizer.fit(corpus_plot)\n",
    "print(\"Vocabulary length of CountVectorizer of the plot corpus: {0}\".format(len(c_vectorizer.vocabulary_)))\n",
    "\n",
    "X=c_vectorizer.fit_transform(corpus_plot)\n",
    "X_words=c_vectorizer.inverse_transform(X)\n",
    "\n",
    "tokens_list=c_vectorizer.get_feature_names()\n",
    "count_list = np.asarray(X.sum(axis=0)).ravel().tolist()\n",
    "\n",
    "token_frequency_df_plot=pd.DataFrame({'term': c_vectorizer.get_feature_names(), 'token_frequency': count_list})\n",
    "token_frequency_df_plot.reset_index(drop=True)\n",
    "token_frequency_df_plot=token_frequency_df_plot.sort_values(by='token_frequency', ascending=False)\n",
    "print(token_frequency_df_plot.shape)\n",
    "#token_frequency_df_pruned_plot=token_frequency_df[token_frequency_df['token_frequency']>=3]\n",
    "#print(token_frequency_df_pruned_plot.shape)\n",
    "\n",
    "# The below code sample creates a dictionary that shows only the n-frequent actors\n",
    "plot_frequency_dictionary=dict(zip(tokens_list, count_list))\n",
    "d_plot = dict((k, v) for k, v in plot_frequency_dictionary.items() if v >= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of plot tokens that exist in the dataset is: {}\".format(len(c_vectorizer.vocabulary_)))\n",
    "plot_words_tokenized=int(np.ceil(len(c_vectorizer.vocabulary_)*0.95))\n",
    "print(\"The 95% ({0}) of the plot summary tokens will be tokenized and the rest 5% ({1}) of the actors will be removed due to sparsity\".format(plot_words_tokenized, (len(c_vectorizer.vocabulary_)-plot_words_tokenized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Find the most frequent words among the movie features\n",
    "In the end the number of rows will be equal to the number of maximum features tokenized by the Tokenizer.\n",
    "\"\"\"\n",
    "def movie_features_split(s):\n",
    "    return s.split(' ')\n",
    "\n",
    "corpus_features=dataset_frequent_genres['clean_combined_features'].values.tolist()\n",
    "c_vectorizer=CountVectorizer(tokenizer=movie_features_split, min_df=2)\n",
    "\n",
    "c_vectorizer.fit(corpus_features)\n",
    "print(\"Vocabulary length of CountVectorizer of the features corpus: {0}\".format(len(c_vectorizer.vocabulary_)))\n",
    "\n",
    "X=c_vectorizer.fit_transform(corpus_features)\n",
    "X_words=c_vectorizer.inverse_transform(X)\n",
    "\n",
    "tokens_list=c_vectorizer.get_feature_names()\n",
    "count_list = np.asarray(X.sum(axis=0)).ravel().tolist()\n",
    "\n",
    "token_frequency_df=pd.DataFrame({'term': c_vectorizer.get_feature_names(), 'token_frequency': count_list})\n",
    "token_frequency_df=token_frequency_df.sort_values(by='token_frequency', ascending=False)\n",
    "print(token_frequency_df.shape)\n",
    "#token_frequency_df_pruned_features=token_frequency_df[token_frequency_df['token_frequency']>=3]\n",
    "#print(token_frequency_df_pruned_features.shape)\n",
    "\n",
    "# The below code sample creates a dictionary that shows only the n-frequent actors\n",
    "features_frequency_dictionary=dict(zip(tokens_list, count_list))\n",
    "d_features = dict((k, v) for k, v in features_frequency_dictionary.items() if v >= 90)\n",
    "\n",
    "# sorted(d_features.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_frequency_dictionary['drama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of feature tokens that exist in the dataset is: {}\".format(len(c_vectorizer.vocabulary_)))\n",
    "features_words_tokenized=int(np.ceil(len(c_vectorizer.vocabulary_)*0.95))\n",
    "print(\"The 95% ({0}) of the feature tokens will be tokenized and the rest 5% ({1}) of the actors will be removed due to sparsity\".format(features_words_tokenized, (len(c_vectorizer.vocabulary_)-features_words_tokenized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Find the most frequent words among the movie reviews\n",
    "In the end the number of rows will be equal to the number of maximum features tokenized by the Tokenizer.\n",
    "\"\"\"\n",
    "def reviews_split(s):\n",
    "    return s.split(' ')\n",
    "\n",
    "corpus_reviews=dataset_frequent_genres['clean_reviews'].values.tolist()\n",
    "c_vectorizer=CountVectorizer(tokenizer=reviews_split, min_df=1)\n",
    "\n",
    "c_vectorizer.fit(corpus_reviews)\n",
    "print(\"Vocabulary length of CountVectorizer of the reviews corpus: {0}\".format(len(c_vectorizer.vocabulary_)))\n",
    "\n",
    "X=c_vectorizer.fit_transform(corpus_reviews)\n",
    "X_words=c_vectorizer.inverse_transform(X)\n",
    "\n",
    "tokens_list=c_vectorizer.get_feature_names()\n",
    "count_list = np.asarray(X.sum(axis=0)).ravel().tolist()\n",
    "\n",
    "token_frequency_df=pd.DataFrame({'term': c_vectorizer.get_feature_names(), 'token_frequency': count_list})\n",
    "token_frequency_df=token_frequency_df.sort_values(by='token_frequency', ascending=False)\n",
    "print(token_frequency_df.shape)\n",
    "#token_frequency_df_pruned_reviews=token_frequency_df[token_frequency_df['token_frequency']>=3]\n",
    "#print(token_frequency_df_pruned_reviews.shape)\n",
    "\n",
    "# The below code sample creates a dictionary that shows only the n-frequent actors\n",
    "reviews_frequency_dictionary=dict(zip(tokens_list, count_list))\n",
    "d_reviews = dict((k, v) for k, v in reviews_frequency_dictionary.items() if v >= 100)\n",
    "\n",
    "# sorted(d_reviews.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_frequency_dictionary['film']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of review tokens that exist in the dataset is: {}\".format(len(c_vectorizer.vocabulary_)))\n",
    "reviews_words_tokenized=int(np.ceil(len(c_vectorizer.vocabulary_)*0.95))\n",
    "print(\"The 95% ({0}) of the review tokens will be tokenized and the rest 5% ({1}) of the actors will be removed due to sparsity\".format(reviews_words_tokenized, (len(c_vectorizer.vocabulary_)-reviews_words_tokenized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Find the most frequent words among the movie titles\n",
    "In the end the number of rows will be equal to the number of maximum features tokenized by the Tokenizer.\n",
    "\"\"\"\n",
    "def movie_title_split(s):\n",
    "    return s.split(' ')\n",
    "\n",
    "corpus_title=dataset_frequent_genres['clean_movie_title'].values.tolist()\n",
    "c_vectorizer=CountVectorizer(tokenizer=movie_title_split, min_df=2)\n",
    "\n",
    "c_vectorizer.fit(corpus_title)\n",
    "print(\"Vocabulary length of CountVectorizer of the title corpus: {0}\".format(len(c_vectorizer.vocabulary_)))\n",
    "\n",
    "X=c_vectorizer.fit_transform(corpus_title)\n",
    "X_words=c_vectorizer.inverse_transform(X)\n",
    "\n",
    "tokens_list=c_vectorizer.get_feature_names()\n",
    "count_list = np.asarray(X.sum(axis=0)).ravel().tolist()\n",
    "\n",
    "token_frequency_df=pd.DataFrame({'term': c_vectorizer.get_feature_names(), 'token_frequency': count_list})\n",
    "token_frequency_df=token_frequency_df.sort_values(by='token_frequency', ascending=False)\n",
    "print(token_frequency_df.shape)\n",
    "#token_frequency_df_pruned_title=token_frequency_df[token_frequency_df['token_frequency']>=3]\n",
    "#print(token_frequency_df_pruned_title.shape)\n",
    "\n",
    "# The below code sample creates a dictionary that shows only the n-frequent actors\n",
    "title_frequency_dictionary=dict(zip(tokens_list, count_list))\n",
    "d_title = dict((k, v) for k, v in title_frequency_dictionary.items() if v >= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of movie title tokens that exist in the dataset is: {}\".format(len(c_vectorizer.vocabulary_)))\n",
    "title_words_tokenized=int(np.ceil(len(c_vectorizer.vocabulary_)*0.95))\n",
    "print(\"The 95% ({0}) of the movie title tokens will be tokenized and the rest 5% ({1}) of the actors will be removed due to sparsity\".format(title_words_tokenized, (len(c_vectorizer.vocabulary_)-title_words_tokenized)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\text_tokenization_padded_sequences_{0}\".format(version_data_control))) is True:\n",
    "    print(\"Folder already exists!\\n\")\n",
    "else:\n",
    "    print(\"Folder not found!\\n\")\n",
    "    os.mkdir(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\text_tokenization_padded_sequences_{0}\".format(version_data_control)))\n",
    "    print(\"Folder is created!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data tokenization is one of the most important parts when dealing with text data.\n",
    "Since I am going to deploy keras models, I use the python api of Keras Tokenizer,\n",
    "more details about its use on: https://keras.io/preprocessing/text/\n",
    "\"\"\"\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "# Tokenize the dataset (using the keras tokenizer)\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(\"\\nTokenize the dataset (using the keras tokenizer class)\\n\")\n",
    "begin_time=time.time()\n",
    "\n",
    "start_time_one=time.time()\n",
    "print(\"------------\\nActors corpus\\n------------\")\n",
    "vocabulary_size_frequent_words_actors, tokenizer_actors = keras_tokenization(\"actors\", actors_tokenized, dataset_frequent_genres, X_train_actors, X_test_actors) # function 5: keras_tokenization\n",
    "print(\"Actors tokenized with maximum number of words: {}\".format(vocabulary_size_frequent_words_actors))\n",
    "\n",
    "# Pickle the Actors Tokenizer\n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\text_tokenization_padded_sequences_{0}\\\\actors_tokenizer_{1}_{2}.pkl'.format(version_data_control, str(actors_tokenized), version_data_control)), 'wb') as handle:\n",
    "    pickle.dump(tokenizer_actors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"Finished the actors corpus tokenization after: {0}\\n\".format(format_timespan(time.time()-start_time_one)))\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "start_time_two=time.time()\n",
    "print(\"------------\\nPlot Summary corpus\\n------------\")\n",
    "vocabulary_size_frequent_words_plot, tokenizer_plot = keras_tokenization(\"plot\", plot_words_tokenized, dataset_frequent_genres, X_train_plot, X_test_plot) # function 5: keras_tokenization\n",
    "print(\"Plot Summary tokenized with maximum number of words: {}\".format(vocabulary_size_frequent_words_plot))\n",
    "\n",
    "# Pickle the Plot Tokenizer\n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\text_tokenization_padded_sequences_{0}\\\\plot_tokenizer_{1}_{2}.pkl'.format(version_data_control, str(plot_words_tokenized), version_data_control)), 'wb') as handle:\n",
    "    pickle.dump(tokenizer_plot, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"Finished the plot corpus tokenization after: {0}\\n\".format(format_timespan(time.time()-start_time_two)))\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "start_time_three=time.time()\n",
    "print(\"------------\\nMovie Features corpus\\n------------\")\n",
    "vocabulary_size_frequent_words_features, tokenizer_features = keras_tokenization(\"features\", features_words_tokenized, dataset_frequent_genres, X_train_features, X_test_features) # function 5: keras_tokenization\n",
    "print(\"Movie Features tokenized with maximum number of words: {}\".format(vocabulary_size_frequent_words_features))\n",
    "\n",
    "# Pickle the Movie Features Tokenizer\n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\text_tokenization_padded_sequences_{0}\\\\features_tokenizer_{1}_{2}.pkl'.format(version_data_control, str(features_words_tokenized), version_data_control)), 'wb') as handle:\n",
    "    pickle.dump(tokenizer_features, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"Finished the movie features corpus tokenization after: {0}\\n\".format(format_timespan(time.time()-start_time_three)))\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "start_time_four=time.time()\n",
    "print(\"------------\\nMovie Reviews corpus\\n------------\")\n",
    "vocabulary_size_frequent_words_reviews, tokenizer_reviews = keras_tokenization(\"reviews\", reviews_words_tokenized, dataset_frequent_genres, X_train_reviews, X_test_reviews) # function 5: keras_tokenization\n",
    "print(\"Movie Reviews tokenized with maximum number of words: {}\".format(vocabulary_size_frequent_words_reviews))\n",
    "\n",
    "# Pickle the Reviews Tokenizer\n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\text_tokenization_padded_sequences_{0}\\\\reviews_tokenizer_{1}_{2}.pkl'.format(version_data_control, str(reviews_words_tokenized), version_data_control)), 'wb') as handle:\n",
    "    pickle.dump(tokenizer_reviews, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"Finished the movie reviews corpus tokenization after: {0}\\n\".format(format_timespan(time.time()-start_time_four)))\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "start_time_five=time.time()\n",
    "print(\"------------\\nMovie Title corpus\\n------------\")\n",
    "vocabulary_size_frequent_words_title, tokenizer_title = keras_tokenization(\"movie title\", title_words_tokenized, dataset_frequent_genres, X_train_title, X_test_title) # function 5: keras_tokenization\n",
    "print(\"Movie Title tokenized with maximum number of words: {}\".format(vocabulary_size_frequent_words_title))\n",
    "\n",
    "# Pickle the Reviews Tokenizer\n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\text_tokenization_padded_sequences_{0}\\\\title_tokenizer_{1}_{2}.pkl'.format(version_data_control, str(title_words_tokenized), version_data_control)), 'wb') as handle:\n",
    "    pickle.dump(tokenizer_title, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"Finished the movie title corpus tokenization after: {0}\\n\".format(format_timespan(time.time()-start_time_five)))\n",
    "\n",
    "print(\"Finished tokenization of all 5 trainable columns after: {0}\".format(format_timespan(time.time()-begin_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_one=time.time()\n",
    "print(\"------------\\nActors corpus\\n------------\")\n",
    "vocabulary_size_frequent_words_actors, tokenizer_actors = keras_tokenization(\"actors\", actors_tokenized, dataset_frequent_genres, X_train_actors, X_test_actors) # function 5: keras_tokenization\n",
    "print(\"Actors tokenized with maximum number of words: {}\".format(vocabulary_size_frequent_words_actors))\n",
    "\n",
    "# Pickle the Actors Tokenizer\n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\text_tokenization_padded_sequences_{0}\\\\actors_tokenizer_{1}_{2}.pkl'.format(version_data_control, str(actors_tokenized), version_data_control)), 'wb') as handle:\n",
    "    pickle.dump(tokenizer_actors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"Finished the actors corpus tokenization after: {0}\\n\".format(format_timespan(time.time()-start_time_one)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_tokenized_per_trainable_feature={}\n",
    "words_tokenized_per_trainable_feature['actors_tokenized']=actors_tokenized\n",
    "words_tokenized_per_trainable_feature['plot_words_tokenized']=plot_words_tokenized\n",
    "words_tokenized_per_trainable_feature['features_words_tokenized']=features_words_tokenized\n",
    "words_tokenized_per_trainable_feature['reviews_words_tokenized']=reviews_words_tokenized\n",
    "words_tokenized_per_trainable_feature['title_words_tokenized']=title_words_tokenized\n",
    "words_tokenized_per_trainable_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\text_tokenization_padded_sequences_{0}\\\\words_tokenized_{0}.pkl'.format(version_data_control)), 'wb') as handle:\n",
    "    pickle.dump(words_tokenized_per_trainable_feature, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reviews.loc[:, 'reviews_seqs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment: The three below blocks of code where executed once and then were pickled!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "# Specify the length of the maxlen variable\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(\"\\nSpecify the length of the maxlen variable (length is a parameter for the optimal padding execution)\\n\")\n",
    "\n",
    "maxlen_actors = padding_sequnce_length(\"actors\", X_train_actors, X_test_actors)\n",
    "maxlen_plot = padding_sequnce_length(\"plot\", X_train_plot, X_test_plot)\n",
    "maxlen_features = padding_sequnce_length(\"features\", X_train_features, X_test_features)\n",
    "maxlen_reviews = padding_sequnce_length(\"reviews\", X_train_reviews, X_test_reviews)\n",
    "maxlen_title = padding_sequnce_length(\"movie title\", X_train_title, X_test_title)\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "# Create the padding sequence of texts\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(\"\\nCreate the padding sequence of texts\\n\")\n",
    "\n",
    "X_train_seq_actors, X_test_seq_actors = padding_sequence(\"actors\", X_train_actors, X_test_actors, y_train, y_test, maxlen_actors)\n",
    "print(\"\\nActors padded sequences created\\n\")\n",
    "\n",
    "X_train_seq_plot, X_test_seq_plot = padding_sequence(\"plot\", X_train_plot, X_test_plot, y_train, y_test, maxlen_plot)\n",
    "print(\"Plot padded sequences created\\n\")\n",
    "\n",
    "X_train_seq_features, X_test_seq_features = padding_sequence(\"features\", X_train_features, X_test_features, y_train, y_test, maxlen_features)\n",
    "print(\"Movie Features padded sequences created\\n\")\n",
    "\n",
    "X_train_seq_reviews, X_test_seq_reviews = padding_sequence(\"reviews\", X_train_reviews, X_test_reviews, y_train, y_test, maxlen_reviews)\n",
    "print(\"Movie Reviews padded sequences created\\n\")\n",
    "\n",
    "X_train_seq_title, X_test_seq_title = padding_sequence(\"movie title\", X_train_title, X_test_title, y_train, y_test, maxlen_title)\n",
    "print(\"Movie Title padded sequences created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #1st case of data: 80-20 split and non-balanced dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* X_train & X_test with <b>80-20</b> split and <b>non-balanced genre</b> tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train_seq_actors shape:{}\".format(X_train_seq_actors.shape)) #80-20 split\n",
    "print(\"X_train_seq_plot shape:{}\".format(X_train_seq_plot.shape)) #80-20 split\n",
    "print(\"X_train_seq_features shape:{}\".format(X_train_seq_features.shape)) #80-20 split\n",
    "print(\"X_train_seq_reviews shape:{}\".format(X_train_seq_reviews.shape)) #80-20 split\n",
    "print(\"X_train_seq_title shape:{}\\n\".format(X_train_seq_title.shape)) #80-20 split\n",
    "\n",
    "\n",
    "print(\"X_test_seq_actors shape:{}\".format(X_test_seq_actors.shape)) #80-20 split\n",
    "print(\"X_test_seq_plot shape:{}\".format(X_test_seq_plot.shape)) #80-20 split\n",
    "print(\"X_test_seq_features shape:{}\".format(X_test_seq_features.shape)) #80-20 split\n",
    "print(\"X_test_seq_reviews shape:{}\".format(X_test_seq_reviews.shape)) #80-20 split\n",
    "print(\"X_test_seq_title shape:{}\".format(X_test_seq_title.shape)) #80-20 split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* y_train & y_test with <b>80-20</b> split and <b>non-balanced genre</b> tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"y_train shape:{}\".format(y_train.shape)) #80-20 split\n",
    "print(\"y_test shape:{}\".format(y_test.shape)) #80-20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\text_tokenization_padded_sequences_{0}\\\\x_train_seq_actors_80-20_non-balanced_{1}_{0}\".format(version_data_control, str(actors_tokenized))), X_train_seq_actors)\n",
    "np.save(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\text_tokenization_padded_sequences_{0}\\\\x_train_seq_plot_80-20_non-balanced_{1}_{0}\".format(version_data_control, str(plot_words_tokenized))), X_train_seq_plot)\n",
    "np.save(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\text_tokenization_padded_sequences_{0}\\\\x_train_seq_features_80-20_non-balanced_{1}_{0}\".format(version_data_control, str(features_words_tokenized))), X_train_seq_features)\n",
    "np.save(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\text_tokenization_padded_sequences_{0}\\\\x_train_seq_reviews_80-20_non-balanced_{1}_{0}\".format(version_data_control, str(reviews_words_tokenized))), X_train_seq_reviews)\n",
    "np.save(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\text_tokenization_padded_sequences_{0}\\\\x_train_seq_title_80-20_non-balanced_{1}_{0}\".format(version_data_control, str(title_words_tokenized))), X_train_seq_title)\n",
    "\n",
    "np.save(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\text_tokenization_padded_sequences_{0}\\\\x_test_seq_actors_80-20_non-balanced_{1}_{0}\".format(version_data_control, str(actors_tokenized))), X_test_seq_actors)\n",
    "np.save(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\text_tokenization_padded_sequences_{0}\\\\x_test_seq_plot_80-20_non-balanced_{1}_{0}\".format(version_data_control, str(plot_words_tokenized))), X_test_seq_plot)\n",
    "np.save(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\text_tokenization_padded_sequences_{0}\\\\x_test_seq_features_80-20_non-balanced_{1}_{0}\".format(version_data_control, str(features_words_tokenized))), X_test_seq_features)\n",
    "np.save(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\text_tokenization_padded_sequences_{0}\\\\x_test_seq_reviews_80-20_non-balanced_{1}_{0}\".format(version_data_control, str(reviews_words_tokenized))), X_test_seq_reviews)\n",
    "np.save(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\text_tokenization_padded_sequences_{0}\\\\x_test_seq_title_80-20_non-balanced_{1}_{0}\".format(version_data_control, str(title_words_tokenized))), X_test_seq_title)\n",
    "\n",
    "np.save(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\text_tokenization_padded_sequences_{0}\\\\y_train_80-20_non-balanced_{0}\".format(version_data_control)), y_train)\n",
    "np.save(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\text_tokenization_padded_sequences_{0}\\\\y_test_80-20_non-balanced_{0}\".format(version_data_control)), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle the \"dataset_frequent_genres\" with the added cleaned columns of actors, plot, features and reviews\n",
    "\n",
    "#### Pickle the X_test dataset for use in part 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_frequent_genres.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_frequent_genres.to_pickle(os.path.join(os.getcwd(), \"pickled_data_per_part\\\\dataset_part_3.1_{0}.pkl\".format(version_data_control)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_pickle(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\text_tokenization_padded_sequences_{0}\\\\x_test_{0}.pkl\".format(version_data_control)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### THIS IS THE END OF PART 3.1 - Where tokenization, cleaning and balancing of the data took place."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
